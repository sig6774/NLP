# -*- coding: utf-8 -*-
"""1_seq2seq ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IMNmcicTLpDAmHMgi7NwuTlyOF1Ahgju
"""

import os
from google.colab import drive
drive.mount('/content/gdrive/')

# 구글드라이브 마운트

os.listdir('gdrive/My Drive/Graduate')
# 지정된 경로에 파일을 출력

import torch
import torch.nn as nn
import torch.optim as optim

from torchtext.legacy.datasets import Multi30k
from torchtext.legacy.data import Field, BucketIterator

import spacy
import numpy as np

import random
import math
import time

pip install spacy

!python -m spacy download en_core_web_sm
!python -m spacy download de_core_news_sm
# 코랩에서 터미널 설치법 : ! 사용

SEED = 1234

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True

spacy_de = spacy.load('de_core_news_sm')
spacy_en = spacy.load('en_core_web_sm')

def tokenize_de(text):
    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]
# 거꾸로 데이터값을 주면 성능이 좋아진다고 했으므로 데이터를 거꾸로 입력

def tokenize_en(text):
    return [tok.text for tok in spacy_en.tokenizer(text)]

SRC = Field(tokenize = tokenize_de,
               init_token = '<sos>',
               eos_token = '<eos>',
               lower = True)

TRC = Field(tokenize = tokenize_en,
                init_token = '<sos>',
                eos_token= '<eos>',
                lower = True)

train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),
                                                    fields = (SRC, TRC),
                                                    root = 'data')

SRC.build_vocab(train_data, min_freq = 2)
TRC.build_vocab(train_data, min_freq = 2)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

Batch_size = 128

train_iterator, valid_iterator, test_iterator = BucketIterator.splits(
    (train_data, valid_data, test_data),
    batch_size = Batch_size,
    device = device)

class Encoder(nn.Module):

    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()

        self.hid_dim = hid_dim
        self.n_layers = n_layers
        self.embedding = nn.Embedding(input_dim, emb_dim)

        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)

        self.dropout = nn.Dropout(dropout)

        # Encoder에서 사용할 변수들을 새롭게 정의

    def forward(self, src):

        # src = [sentence_len, batch_size]

        embedded = self.dropout(self.embedding(src))

        # embedded = [sentence_len, batch_size, emb_dim]

        outputs, (h, c) = self.rnn(embedded)

        # emb_dim -> hid_dim

        # outputs = [sentence_len, batch_size, hid_dim]

        # h = [n_layers, batch_size, hid_dim]

        # c = [n_layers, batch_size, hid_dim]

        return h, c

    # h와 c가 decoder에 들어가는 context vector가 된다.

class Decoder(nn.Module):

    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()

        self.output_dim = output_dim
        self.hid_dim = hid_dim
        self.n_layers = n_layers

        self.embedding = nn.Embedding(output_dim, emb_dim)
        # decoder input을 embedding

        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)
        # rnn 모델 정의

        self.fc_out = nn.Linear(hid_dim, output_dim)

        self.dropout = nn.Dropout(dropout)

    def forward(self, input, hidden, cell):

        input = input.unsqueeze(0)
        # 차원을 맞춰주기 위해 1번째 차원을 추가

        embedded = self.dropout(self.embedding(input))
        # embedding

        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))
        # 실제 context vector가 Decoder에 들어가는 부분

        pred = self.fc_out(output.squeeze(0))
        # 최종적인 예측값 도출

        # pred = [batch_size, output_dim]

        return pred, hidden, cell

class seq2seq(nn.Module):

    def __init__(self, encoder, decoder, device):
        super().__init__()

        self.encoder = encoder
        self.decoder = decoder
        self.device = device

        assert encoder.hid_dim == decoder.hid_dim, \
            "Hidden dimensions of encoder and decoder must be equal!"
        assert encoder.n_layers == decoder.n_layers, \
            "Encoder and decoder must have equal number of layers!"

    def forward(self, src, trg, ratio = 0.5):

        batch_size = trg.shape[1]
        trg_len = trg.shape[0]
        trg_vocab_size = self.decoder.output_dim

        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)
        # 들어갈 원핫 벡터의 틀을 만듬 

        hidden, cell = self.encoder(src)
        # context vector 생성

        input = trg[0, :]

        for t in range(1, trg_len):
          # 첫번째는 <sos>라서 1부터 시작 

            output, hidden, cell = self.decoder(input, hidden, cell)
            # input과 encoder의 output인 hidden과 cell이 decoder의 input으로 들어감 
            # output : [batch_size, vocab_size]
            

            outputs[t] = output
            # 출력된 output을 만들어놓은 outputs(zero tensor)에 넣어줌 

            force = random.random() < ratio

            top1 = output.argmax(1)
            # force ~ top1 부분 이해가 안감
            # output으로 나온 값들에 대해서 최대 값을 도출하는건가?

            input = trg[t] if force else top1

        return outputs

INPUT_DIM = len(SRC.vocab)
OUTPUT_DIM = len(TRC.vocab)
ENC_EMB_DIM = 256
DEC_EMB_DIM = 256
HID_DIM = 512
N_LAYERS = 2
ENC_DROPOUT = 0.5
DEC_DROPOUT = 0.5

enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)
dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)

model = seq2seq(enc, dec, device).to(device)

def init_weights(m):
    for name, param in m.named_parameters():
        nn.init.uniform_(param.data, -0.08, 0.08)


print(model.apply(init_weights))
# 모델 구조

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f'The model has {count_parameters(model):,} trainable parameters')


optimizer = optim.Adam(model.parameters())

TRG_PAD_IDX = TRC.vocab.stoi[TRC.pad_token]

criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)
# 원핫으로 되어 있어도 ignore

def train(model, iterator, optimizer, criterion, clip):

    model.train()
    epoch_loss = 0

    for i, batch in enumerate(iterator):

        src = batch.src
        trg = batch.trg
        # print(trg)

        optimizer.zero_grad()

        output = model(src, trg)

        output_dim = output.shape[-1]

        output = output[1:].view(-1, output_dim)
        # output의 형태를 [max_len, batch_size, vocab_size] -> [(max_len-1) * batch_size, vocab_size]로 변경 
        # 


        trg = trg[1:].view(-1)
        # 해당 부분 이해가 안됨... 왜 두번째부터 하는거지?
          # <sos>가 처음에 있으니깐 두번째부터 시작 
          # trg = [(max_len-1)*batch_size]

        loss = criterion(output, trg)
        # 모델의 output값과 실제 trg값을 비교하여 loss도출 

        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)

        optimizer.step()

        epoch_loss += loss.item()

    return epoch_loss / len(iterator)


def evaluate(model, iterator, criterion):
    model.eval()

    epoch_loss = 0

    with torch.no_grad():
        for i, batch in enumerate(iterator):
            src = batch.src
            trg = batch.trg

            output = model(src, trg, 0)

            # trg = [trg len, batch size]
            # output = [trg len, batch size, output dim]

            output_dim = output.shape[-1]

            output = output[1:].view(-1, output_dim)
            trg = trg[1:].view(-1)

            # trg = [(trg len - 1) * batch size]
            # output = [(trg len - 1) * batch size, output dim]
            # output_dim으로 값이 구성되어 있는데 crossentropy를 통해서 해당 인덱스의 값으로 변환? 
            # 

            loss = criterion(output, trg)

            epoch_loss += loss.item()

    return epoch_loss / len(iterator)

def epoch_time(start_time, end_time):
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs

N_EPOCHS = 10
CLIP = 1

best_valid_loss = float('inf')

for epoch in range(N_EPOCHS):

    start_time = time.time()

    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)
    valid_loss = evaluate(model, valid_iterator, criterion)

    end_time = time.time()

    epoch_mins, epoch_secs = epoch_time(start_time, end_time)

    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(model.state_dict(), 'tut1-model.pt')

    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')
    print(f'\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')
    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')

''' 정리 
1. 소스문장을 Encoder에 넣어서 해당 문장의 정보를 가지고 있는 context vector생성 
2. 소스문장의 정보를 가진 context vector를 Decoder에 넣어줌 
3. Decoder 계산
  3-1. 실제 출력되는 output과 실제 값의 entropy값을 출력 
  3-2. loss값을 줄이며 학습 
'''
