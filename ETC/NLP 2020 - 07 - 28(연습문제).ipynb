{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filepath:  /Users/Moon/opt/anaconda3/lib/python3.7/site-packages\n",
      "classpath:  /Users/Moon/opt/anaconda3/lib/python3.7/site-packages/rhinoMorph/lib/rhino.jar\n",
      "RHINO started!\n"
     ]
    }
   ],
   "source": [
    "import rhinoMorph\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "rn = rhinoMorph.startRhino()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "naver_dir = '/Users/Moon/Desktop/Summer/NLP/pytest/네이버뉴스-생활문화_다중/'\n",
    "train_dir = os.path.join(naver_dir, 'train')\n",
    "test_dir = os.path.join(naver_dir, 'test')\n",
    "\n",
    "label_types = ['건강정보', '공연전시', '도로교통', '여행레저', '음식맛집', '자동차시승기', '종교', '책', '패션뷰티']\n",
    "#os패키지를 이용하여 디렉토리를 지정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_texts_labels(dir, label_types, encode = 'utf-8'):\n",
    "    texts =[]\n",
    "    labels = []\n",
    "    #각각 빈리스트를 만든다\n",
    "    \n",
    "    for label_type in label_types:\n",
    "        dir_name = os.path.join(dir, label_type) \n",
    "        #각각의 라벨폴더에 접근\n",
    "        for fname in os.listdir(dir_name):\n",
    "            if fname[-4:] == '.txt': #끝에서부터 4번째까지가 .txt일때 \n",
    "                f = open(os.path.join(dir_name,fname), encoding = encode)\n",
    "                texts.append(f.read())\n",
    "                f.close()\n",
    "                \n",
    "                if label_type == label_types[0]: #label_types[0]이 저장되어있는 것과 같을때 \n",
    "                    labels.append(0)\n",
    "                elif label_type == label_types[1]:\n",
    "                    labels.append(1)\n",
    "                elif label_type == label_types[2]:\n",
    "                    labels.append(2)\n",
    "                elif label_type == label_types[3]:\n",
    "                    labels.append(3)\n",
    "                elif label_type == label_types[4]:\n",
    "                    labels.append(4)\n",
    "                elif label_type == label_types[5]:\n",
    "                    labels.append(5)\n",
    "                elif label_type == label_types[6]:\n",
    "                    labels.append(6)\n",
    "                elif label_type == label_types[7]:\n",
    "                    labels.append(7)\n",
    "                elif label_type == label_types[8]:\n",
    "                    labels.append(8)\n",
    "    return texts,labels\n",
    "#os패키지에 대해 이해가 부족함\n",
    "#특정 폴더에 있는 데이터를 불러와서 빈리스트에 저장하고 해당 폴더의 이름을 확인하여 labels리스트에 분류를 적는 함수를 만듬 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SK케미칼, 백신 사업 분사 결정 : 네이버 뉴스\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "글로벌화 단계 진입…올해 백신사업 분사로 별도법인 설립SK케미칼이 주력 사업부문인 백신 사업을 별도 법인으로 분리한다.SK케미칼은 글로벌 프리미엄 백신 회사로 도약을 위해 올해 안에 백신 사업을 분사할 예정이라고 12일 공시했다.SK케미칼은 2006년부터 백신을 미래 성장 동력으로 육성하기 위해 인프라구축에 약 4000억원을 투자했다. 지난 2012년에는 경북 안동에 세포배양 백신 등 차세대 백신을 생산할 수 있는 백신공장 'L하우스'를 짓고 독감백신, 대상포진백신 등을 생산했다. SK케미칼의 사업 부문별 비중을 보면 합성의약품은 약 60%, 백신은 40% 규모다.SK케미칼은 앞으로 분사한 별도법인에 대한 SI(전략적투자자)를 본격적으로 유치하고 기업공개(IPO)를 통해 주주가치 제고에 기여하겠다는 계획이다.SK케미칼 측은 \"지속적인 노력의 성과로 사노피 파스퇴르와 차세대폐렴구균 공동개발 계약 체결, 세포배양방식의 독감백신 3·4가 및 대상포진 백신 출시 등 다양한 성과를 창출해 왔다\"며 \"자체 기술 경쟁력을 바탕으로 글로벌화의 본격화 단계에 진입했다\"고 밝혔다.김지섭기자 cloud50@dt.co.kr 디지털타임스 홈페이지 바로가기 / SNS 바로가기디지털타임스 영문뉴스 바로가기 / 카드뉴스 바로가기김지섭\n",
      "\t\n",
      "\n",
      "90\n",
      "0\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "texts, labels = make_texts_labels(train_dir, label_types)\n",
    "\n",
    "print(texts[0])\n",
    "print(len(texts))\n",
    "print(labels[0])\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SK', '케미칼', '백신', '사업', '분사', '결정', '네이버', '뉴스', '글로벌', '단계', '진입', '올해', '백신', '사업', '분사', '별도', '법인', '설립', 'SK', '케미칼', '주력', '사업', '부문', '백신', '사업', '별도', '법인', '분리', 'SK', '케미칼', '글로벌', '프리미엄', '백신', '회사', '도약', '위하', '올해', '안', '백신', '사업', '분사', '예정', '공시', 'SK', '케미칼', '백신', '미래', '성장', '동력', '육성', '위하', '인프라', '구축', '약', '투자', '지나', '경북', '안동', '세포', '배양', '백신', '차세대', '백신', '생산', '백신', '공장', '하우스', '짓', '독감', '백신', '대상포진', '백신', '등', '생산', 'SK', '케미칼', '사업', '부문', '비중', '합성', '의약품', '약', '백신', '규모', 'SK', '케미칼', '앞', '분사', '하', '별도', '법인', '대하', '전략적', '투자자', '본격적', '유치', '기업공개', '통하', '주주', '가치', '제고', '기여', '계획', 'SK', '케미칼', '지속적', '노력', '성과', '사노', '피', '파스퇴르', '차세대', '폐렴', '구균', '공동', '개발', '계약', '체결', '세포', '배양', '방식', '독감', '백신', '가', '및', '대상포진', '백신', '출시', '다양', '하', '성과', '창출', '자체', '기술', '경쟁력', '바탕', '글로벌', '본격화', '단계', '진입', '밝히', '김지섭', '디지털', '타임스', '홈페이지', '바로', '가', 'SNS', '바로', '가', '디지털', '타임스', '영문', '뉴스', '바로', '가', '카드', '뉴스', '바로', '가', '김', '지섭']\n"
     ]
    }
   ],
   "source": [
    "texts = [rhinoMorph.onlyMorph_list(rn, sentence, pos= ['NNG', 'NNP','NP', 'VV', 'VA', 'XR', 'IC', 'MM', 'MAG', 'MAJ'], eomi = False) for sentence in texts]\n",
    "#texts를 sentence라는 반복문으로돌리고 해당 반복문에서 생성된 객체를 형태소분석을 진행해서 다시 texts로 저장 \n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_ratio = math.floor(len(texts) * 0.3)\n",
    "#validation비율을 나타냄 \n",
    "max_words = 10000\n",
    "#최대 단어를 만개\n",
    "maxlen = 200\n",
    "#문장의 최대 길이를 200\n",
    "class_number = 9\n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "#max_words로 지정한만큼 토큰화 \n",
    "tokenizer.fit_on_texts(texts)\n",
    "#texts를 토큰화해줌 \n",
    "word_index = tokenizer.word_index\n",
    "#word_index는 토큰화한것에 대한 각 단어의 인덱스를 저장 \n",
    "#10000개의 토큰화한 것들에 대해 각각의 고유한 인덱스 번호를 지정 \n",
    "#인덱스 번호 부여 기준은 빈도가 높은 순서대로 부여 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[443, 600, 221, 55, 1081, 256, 5, 4, 257, 865, 1082, 276, 221, 55, 1081, 722, 1083, 866, 443, 600, 1369, 55, 444, 221, 55, 722, 1083, 2785, 443, 600, 257, 1084, 221, 222, 1858, 10, 276, 159, 221, 55, 1081, 344, 2786, 443, 600, 221, 867, 223, 2787, 2788, 10, 1370, 345, 59, 314, 107, 1085, 1859, 1860, 1861, 221, 1862, 221, 174, 221, 868, 388, 1086, 1863, 221, 1864, 221, 76, 174, 443, 600, 55, 444, 869, 2789, 510, 59, 221, 346, 443, 600, 195, 1081, 1, 722, 1083, 18, 1865, 1866, 1371, 1087, 2790, 26, 1088, 1089, 2791, 1090, 47, 443, 600, 2792, 347, 1091, 2793, 175, 2794, 1862, 2795, 2796, 348, 128, 511, 1867, 1860, 1861, 601, 1863, 221, 15, 6, 1864, 221, 196, 39, 1, 1091, 1372, 602, 512, 603, 723, 257, 1092, 865, 1082, 38, 1093, 197, 389, 129, 30, 15, 198, 30, 15, 197, 389, 870, 4, 30, 15, 315, 4, 30, 15, 22, 1094], [42, 724, 277, 94, 1095, 224, 5, 4, 130, 108, 144, 2, 1373, 95, 96, 42, 131, 16, 1374, 725, 199, 1375, 2, 1868, 109, 14, 2, 2797, 513, 1096, 42, 131, 85, 1376, 2798, 445, 1377, 130, 514, 2799, 16, 145, 1097, 121, 97, 43, 446, 515, 390, 34, 2800, 42, 604, 447, 2801, 1378, 200, 726, 34, 1379, 44, 1869, 726, 42, 724, 277, 605, 11, 86, 42, 131, 2802, 44, 90, 871, 2803, 1870, 54, 2, 60, 42, 131, 1871, 316, 1098, 85, 606, 514, 1870, 1872, 2, 872, 516, 2804, 2805, 2806, 2807, 2808, 1380, 76, 1381, 3, 1382, 2809, 224, 23, 1873, 873, 514, 1383, 391, 1099, 1100, 874, 1384, 47, 34, 94, 514, 42, 727, 1874, 1385, 517, 60, 42, 131, 1875, 1098, 518, 61, 56, 606, 42, 131, 42, 519, 42, 1876, 1101, 1384, 2810, 176, 1096, 42, 607, 1, 176, 34, 1877, 132, 2811, 42, 724, 277, 1386, 1098, 1102, 42, 34, 2812, 60, 34, 277, 87, 448, 875, 2813, 1878, 66, 85, 34, 1879, 130, 1103, 512, 2814, 42, 2815, 225, 278, 177, 128, 2, 2816, 7, 726, 608, 1104, 34, 2817, 1880, 133, 277, 160, 1387, 109, 2818, 517, 3, 2819, 256, 1098, 2820, 1388, 2821, 200, 349, 85, 449, 3, 1099, 1100, 2822, 42, 131, 94, 609, 1105, 450, 2823, 277, 160, 728, 1374, 725, 199, 226, 130, 108, 144, 2, 1373, 95, 96, 42, 131, 16, 1374, 725, 199, 1375, 2, 1868, 109, 14, 2, 7, 2824, 451, 1389, 67, 2825, 1390, 108, 2826, 2827, 178, 876, 452, 2828, 1881, 95, 2829, 26, 517, 95, 179, 448, 519, 42, 131, 160, 725, 6, 1391, 199, 610, 1373, 1106, 53, 7, 95, 514, 727, 1874, 1385, 42, 1392, 199, 42, 131, 16, 453, 392, 98, 62, 2830, 42, 131, 16, 1882, 1, 61, 393, 392, 122, 350, 95, 179, 7, 317, 1883, 2831, 2832, 1393, 199, 2833, 2834, 1884, 1885, 199, 6, 725, 199, 54, 2, 729, 96, 36, 730, 95, 96, 1886, 1878, 731, 448, 2835, 42, 131, 2836, 1885, 199, 6, 2837, 199, 36, 227, 318, 2, 1883, 60, 42, 131, 16, 453, 392, 1394, 226, 1393, 199, 60, 68, 109, 351, 1887, 728, 453, 2838, 732, 453, 1107, 318, 1, 96, 1393, 199, 60, 1395, 454, 611, 1884, 1395, 454, 68, 1394, 226, 42, 131, 160, 2839, 161, 1108, 2840, 94, 42, 131, 725, 199, 1381, 451, 1108, 3, 108, 1888, 95, 96, 448, 519, 1, 42, 724, 277, 16, 728, 94, 277, 16, 94, 1109, 451, 2841, 454, 161, 733, 455, 2842, 734, 394, 228, 1888, 1396, 1103, 612, 1881, 94, 59, 122, 729, 201, 352, 1889, 448, 519, 1, 42, 131, 16, 1397, 277, 16, 277, 16, 94, 59, 95, 91, 451, 877, 2843, 94, 451, 451, 2844, 1890, 448, 1891, 1, 94, 44, 109, 2845, 448, 875, 94, 2846, 351, 60, 1892, 728, 1398, 1893, 1399, 520, 2, 1110, 1111, 1400, 1401, 456, 451, 1894, 94, 451, 123, 448, 519, 1, 42, 724, 1895, 16, 94, 1111, 1400, 730, 95, 1, 96, 277, 16, 94, 456, 451, 392, 1109, 1111, 1400, 66, 1401, 95, 3, 1896, 450, 520, 457, 42, 131, 1386, 1896, 450, 395, 94, 3, 2847, 1897, 176, 85, 449, 42, 2848, 878, 604, 2, 395, 1898, 450, 77, 879, 1402, 277, 160, 2849, 2850, 2851, 1899, 1380, 2852, 1401, 2853, 76, 520, 61, 3, 2854, 85, 42, 724, 277, 160, 2855, 2856, 2857, 258, 1899, 1380, 1381, 1, 2858, 448, 604, 2, 60, 1112, 2859, 1403, 1112, 1403, 1385, 395, 3, 1886, 225, 316, 1099, 1100, 42, 131, 94, 609, 90, 450, 880, 3, 735, 1384, 47, 34, 1392, 199, 225, 736, 42, 131, 256, 514, 1876, 1404, 2860, 225, 42, 2861, 2862, 1405, 42, 519, 200, 726, 881, 1900, 1901, 514, 225, 77, 2863, 1406, 2864, 2, 39, 1, 726, 1902, 1, 1099, 1100, 2865, 514, 225, 10, 42, 131, 1903, 60, 200, 873, 1113, 16, 1875, 9, 2866, 353, 453, 319, 229, 458, 1, 726, 737, 738, 225, 279, 320, 1887, 2867, 8, 739, 198, 740, 124, 230, 613, 882, 14, 739, 19, 20, 6, 17, 24], [1904, 2868, 521, 883, 2869, 2870, 741, 614, 615, 616, 606, 5, 4, 354, 459, 460, 617, 1905, 2871, 1114, 1904, 1906, 512, 259, 2872, 1407, 521, 354, 459, 460, 1115, 741, 36, 18, 614, 615, 616, 1114, 9, 355, 354, 459, 460, 617, 883, 459, 618, 6, 619, 1408, 2873, 107, 346, 741, 36, 48, 356, 883, 459, 60, 2874, 2875, 619, 2876, 2, 134, 2877, 347, 2878, 40, 1907, 2, 2879, 195, 1905, 614, 1116, 2880, 606, 49, 354, 459, 460, 2881, 2882, 2883, 36, 884, 1117, 1908, 522, 14, 1909, 2884, 883, 459, 1118, 1, 86, 348, 95, 618, 460, 2885, 346, 741, 2886, 2887, 2, 63, 741, 2888, 618, 66, 224, 16, 460, 132, 1879, 280, 354, 346, 741, 36, 48, 1910, 620, 885, 2889, 396, 1408, 1911, 7, 1907, 1119, 397, 2, 3, 886, 523, 521, 130, 459, 2890, 2891, 887, 742, 3, 743, 345, 1, 883, 459, 1912, 281, 2, 1409, 1913, 1120, 40, 21, 1114, 1093, 197, 389, 129, 30, 15, 198, 30, 15, 197, 389, 870, 4, 30, 15, 315, 4, 30, 15, 22, 1094], [320, 146, 282, 1410, 123, 29, 1411, 2892, 144, 5, 4, 461, 744, 745, 260, 888, 202, 231, 2893, 744, 745, 1412, 1914, 1413, 1414, 444, 320, 146, 2894, 1092, 2, 202, 29, 69, 1411, 889, 1121, 1915, 144, 2, 14, 3, 202, 56, 320, 146, 1415, 2895, 2896, 130, 889, 1121, 321, 621, 511, 746, 355, 14, 1916, 2897, 320, 146, 282, 461, 744, 314, 745, 2898, 77, 1917, 1918, 2899, 2900, 524, 1919, 357, 6, 510, 1122, 174, 461, 320, 146, 282, 622, 232, 261, 352, 1416, 525, 461, 60, 280, 1920, 99, 353, 461, 2901, 1123, 6, 1921, 1922, 1124, 747, 283, 1921, 1922, 1124, 282, 231, 1923, 748, 2, 322, 461, 2902, 1924, 524, 2903, 744, 745, 2904, 2905, 2906, 110, 314, 16, 231, 180, 1, 282, 70, 1889, 161, 2907, 1417, 47, 202, 461, 320, 146, 282, 60, 510, 1122, 174, 623, 602, 31, 2908, 398, 2909, 358, 1413, 1418, 1925, 14, 3, 2910, 461, 320, 146, 282, 2911, 2912, 3, 282, 70, 462, 399, 109, 1125, 2, 526, 2913, 2914, 231, 624, 64, 202, 77, 233, 461, 63, 282, 147, 622, 232, 1919, 749, 2915, 284, 1126, 1, 2916, 736, 1926, 3, 14, 1916, 2917, 750, 1419, 1, 100, 61, 34, 49, 202, 2918, 284, 744, 745, 282, 1412, 3, 396, 744, 745, 320, 146, 282, 1, 160, 203, 1089, 227, 1415, 47, 1127, 3, 320, 146, 233, 130, 2919, 2920, 1128, 1420, 2921, 2922, 2923, 359, 1421, 751, 1422, 16, 2924, 2925, 890, 221, 128, 511, 1867, 1420, 1423, 314, 2926, 1927, 3, 203, 223, 232, 2927, 260, 53, 2, 49, 400, 320, 146, 1129, 1928, 622, 202, 65, 1914, 2928, 1130, 1, 280, 1129, 2929, 752, 1928, 226, 1093, 197, 389, 129, 30, 15, 198, 30, 15, 197, 389, 870, 4, 30, 15, 315, 4, 30, 15, 22, 1094], [393, 59, 891, 1929, 111, 527, 148, 452, 45, 1424, 360, 1930, 5, 4, 361, 1131, 1931, 401, 1425, 747, 26, 31, 892, 280, 1426, 893, 1427, 753, 1132, 59, 196, 888, 45, 261, 280, 393, 59, 2930, 360, 361, 891, 1929, 181, 31, 262, 45, 360, 2931, 1428, 517, 202, 56, 2932, 176, 204, 1133, 100, 894, 1429, 393, 59, 361, 361, 111, 527, 148, 452, 135, 2, 31, 262, 360, 1930, 136, 393, 59, 1413, 1132, 59, 128, 48, 2933, 730, 353, 1134, 1430, 101, 360, 107, 1135, 2934, 2935, 1106, 2936, 895, 111, 527, 148, 452, 1133, 100, 894, 1429, 393, 1430, 525, 111, 527, 148, 452, 1131, 361, 1931, 361, 111, 527, 148, 452, 393, 1430, 2, 322, 360, 2937, 2938, 393, 59, 135, 2939, 280, 59, 2940, 361, 528, 2941, 401, 1136, 7, 98, 393, 59, 401, 71, 45, 360, 31, 893, 625, 2942, 510, 45, 201, 878, 2943, 896, 56, 1133, 100, 894, 45, 360, 747, 1425, 1137, 111, 527, 529, 897, 280, 402, 205, 226, 1, 1426, 893, 1427, 112, 753, 182, 898, 360, 895, 1425, 111, 527, 148, 452, 1431, 1932, 2944, 1138, 747, 31, 45, 892, 137, 1, 361, 262, 361, 111, 527, 148, 452, 402, 205, 223, 1, 112, 753, 2945, 360, 361, 753, 888, 2946, 181, 1132, 1933, 2947, 1934, 350, 11, 2948, 622, 263, 72, 402, 205, 223, 1, 112, 65, 182, 2949, 100, 2950, 148, 452, 402, 205, 223, 1, 112, 1935, 622, 100, 1936, 1432, 362, 280, 223, 1, 112, 2951, 510, 2952, 182, 899, 1139, 622, 111, 1140, 1937, 280, 1426, 893, 1427, 402, 205, 2953, 622, 202, 77, 233, 1131, 626, 393, 59, 401, 50, 1938, 86, 1141, 881, 56, 109, 530, 1132, 1933, 2954, 196, 1133, 100, 894, 45, 602, 1433, 1142, 31, 1131, 361, 15, 360, 747, 281, 45, 2955, 137, 1, 136, 49, 1093, 197, 389, 129, 30, 15, 198, 30, 15, 197, 389, 870, 4, 30, 15, 315, 4, 30, 15, 22, 1094], [1143, 1144, 1434, 627, 900, 5, 4, 900, 754, 531, 2956, 2957, 755, 26, 2958, 2959, 230, 624, 1143, 1144, 1434, 1939, 2960, 900, 196, 900, 754, 264, 1143, 1145, 1434, 532, 3, 900, 754, 21, 197, 389, 129, 30, 15, 198, 30, 15, 197, 389, 870, 4, 30, 15, 315, 4, 30, 15], [28, 27, 1940, 285, 178, 363, 901, 1941, 2961, 5, 4, 285, 178, 363, 1435, 890, 403, 10, 27, 1436, 2962, 195, 1942, 628, 2963, 2964, 8, 1146, 1147, 902, 403, 528, 1943, 13, 4, 2965, 752, 8, 285, 178, 363, 28, 27, 1940, 756, 28, 206, 27, 286, 28, 234, 62, 285, 178, 363, 901, 2966, 3, 2967, 1944, 2968, 3, 28, 27, 1148, 265, 56, 183, 353, 285, 178, 363, 1149, 1130, 2, 285, 178, 363, 1149, 107, 629, 112, 1, 145, 23, 364, 1149, 463, 27, 365, 14, 10, 123, 464, 1150, 7, 62, 1945, 3, 1149, 1437, 630, 872, 109, 1946, 2969, 2970, 2971, 73, 285, 178, 363, 1151, 2, 1947, 46, 1941, 1438, 2, 903, 1439, 1387, 1948, 1949, 1, 2972, 225, 16, 132, 1950, 1440, 404, 2973, 56, 285, 178, 363, 1441, 732, 516, 1951, 1442, 1110, 2974, 2975, 2976, 757, 1151, 2, 1443, 1952, 1953, 1954, 76, 901, 1444, 1435, 2, 901, 528, 533, 1444, 2977, 1444, 890, 904, 161, 1438, 284, 285, 178, 363, 904, 12, 2978, 905, 1955, 1956, 2979, 2980, 2981, 2982, 184, 631, 98, 1387, 366, 520, 903, 1955, 2983, 1956, 2984, 631, 901, 184, 2985, 1438, 284, 632, 758, 1, 759, 2986, 2987, 453, 177, 36, 7, 1435, 890, 403, 2988, 160, 633, 906, 1147, 528, 533, 285, 178, 363, 2989, 1445, 465, 43, 1446, 2990, 138, 2991, 2992, 2993, 2994, 1947, 1957, 902, 88, 1895, 2995, 2996, 43, 902, 1443, 1147, 2997, 1958, 1952, 1953, 1447, 185, 533, 145, 760, 1959, 2998, 2999, 1152, 3000, 1396, 1448, 1449, 454, 1960, 1961, 601, 605, 3001, 1448, 1449, 454, 1960, 278, 185, 1448, 1449, 3002, 2, 1443, 3003, 3004, 3005, 53, 1151, 161, 1450, 3006, 618, 3007, 43, 1961, 1946, 73, 285, 178, 363, 904, 3008, 3009, 620, 3010, 403, 631, 56, 3011, 3012, 893, 1, 528, 1943, 3013, 634, 1962, 1116, 3014, 44, 1451, 3015, 279, 28, 54, 4, 149, 14, 5, 207, 4, 81, 162, 150, 33, 235, 33, 4, 110, 19, 20, 6, 17, 24], [761, 1963, 1452, 6, 762, 128, 1369, 5, 4, 35, 466, 763, 764, 1137, 510, 95, 907, 2, 3016, 203, 23, 908, 1153, 148, 51, 1453, 909, 1964, 223, 761, 208, 3017, 3018, 3019, 3020, 762, 128, 765, 3021, 467, 23, 908, 1153, 148, 1453, 51, 1453, 51, 3022, 1103, 323, 1965, 73, 468, 199, 1966, 1967, 1869, 225, 59, 34, 761, 766, 323, 762, 128, 7, 222, 866, 23, 908, 1153, 148, 95, 179, 3023, 3024, 3025, 3026, 3027, 762, 128, 3028, 1968, 321, 1117, 18, 512, 3029, 635, 398, 1969, 95, 53, 133, 39, 1, 909, 910, 264, 26, 909, 910, 115, 115, 324, 3030, 909, 1964, 223, 762, 1970, 911, 345, 761, 18, 1971, 1454, 116, 225, 1972, 1455, 95, 1, 9, 96, 1968, 909, 912, 631, 115, 3031, 3032, 3033, 913, 204, 3034, 1973, 631, 1451, 1456, 3035, 3036, 1456, 115, 3037, 204, 325, 163, 453, 15, 3038, 1974, 510, 3039, 39, 1, 1975, 36, 88, 85, 3040, 2, 3, 3041, 319, 1973, 631, 1456, 1971, 3042, 1976, 128, 358, 1977, 756, 116, 1978, 1457, 1454, 1969, 1457, 912, 631, 1457, 1975, 405, 225, 1117, 128, 886, 53, 3, 3043, 3044, 1407, 767, 1137, 907, 314, 54, 314, 203, 135, 2, 23, 908, 1153, 148, 761, 1420, 321, 1117, 1421, 1082, 736, 95, 3045, 886, 761, 762, 3046, 74, 3047, 3048, 1866, 635, 203, 512, 626, 55, 1416, 1979, 466, 51, 232, 1980, 326, 636, 634, 738, 510, 349, 3049, 1981, 176, 357, 128, 257, 1137, 320, 146, 203, 1858, 1154, 38, 35, 67, 124, 35, 534, 535, 35, 67, 536, 81, 139, 33, 537, 35], [768, 1155, 881, 316, 538, 768, 232, 769, 1090, 5, 4, 35, 466, 763, 764, 3050, 115, 3051, 1458, 326, 2, 1982, 225, 18, 3052, 3053, 109, 3, 351, 1983, 539, 349, 1984, 1090, 367, 914, 3054, 1156, 18, 327, 915, 916, 406, 530, 407, 18, 327, 915, 406, 916, 406, 916, 462, 367, 323, 3055, 3056, 367, 1985, 134, 327, 3057, 3058, 469, 1, 3059, 3060, 3061, 1986, 520, 2, 9, 873, 1987, 225, 1976, 225, 3062, 3063, 3064, 2, 3065, 367, 3066, 1, 160, 1988, 1989, 3067, 2, 7, 1990, 1991, 1992, 327, 3068, 3069, 327, 3070, 1157, 2, 9, 3071, 18, 327, 915, 916, 540, 915, 3072, 113, 1, 3, 394, 18, 327, 1993, 3073, 55, 3074, 1158, 768, 1155, 881, 3075, 10, 625, 347, 3, 768, 1155, 2, 367, 3076, 637, 2, 917, 2, 367, 18, 88, 3077, 3078, 109, 453, 1994, 3079, 40, 21, 3, 163, 367, 6, 367, 1429, 174, 1159, 541, 3080, 95, 128, 76, 731, 327, 26, 1459, 1995, 1996, 3081, 918, 367, 769, 2, 367, 1460, 3082, 1461, 3083, 919, 1462, 3, 7, 50, 327, 3084, 1997, 920, 1463, 180, 2, 3085, 223, 464, 921, 1103, 45, 603, 398, 45, 232, 1372, 86, 140, 769, 3086, 3, 406, 916, 3087, 10, 1421, 1160, 1, 327, 867, 10, 922, 92, 327, 1991, 923, 3088, 638, 1439, 1464, 1465, 113, 1, 768, 232, 769, 316, 18, 539, 3089, 1161, 1466, 3, 9, 367, 1998, 3090, 639, 176, 1, 924, 88, 1, 542, 3091, 26, 453, 1950, 1467, 526, 3092, 76, 458, 3093, 3094, 1, 3095, 3096, 3097, 1999, 1986, 3098, 199, 225, 3, 403, 640, 925, 279, 18, 327, 915, 768, 1155, 3099, 3100, 891, 926, 6, 2000, 408, 541, 615, 204, 3101, 2001, 2002, 35, 67, 124, 35, 534, 535, 35, 67, 536, 81, 139, 33, 537, 35], [287, 392, 770, 1468, 2, 50, 2003, 2004, 10, 927, 5, 4, 1469, 288, 1470, 543, 183, 928, 3102, 2005, 2006, 1470, 3103, 286, 7, 409, 1471, 1, 3104, 3105, 3106, 164, 1162, 3107, 3108, 3, 2005, 2006, 355, 288, 1470, 186, 64, 1, 1, 366, 771, 3109, 3110, 344, 38, 3111, 50, 287, 929, 26, 743, 470, 1157, 1, 266, 3112, 392, 770, 1163, 927, 133, 328, 1164, 544, 2007, 1472, 2008, 2009, 2010, 1473, 259, 76, 325, 50, 183, 50, 3113, 755, 3114, 3115, 3116, 328, 267, 265, 1474, 329, 267, 265, 2011, 471, 328, 267, 1165, 47, 6, 1474, 329, 267, 1165, 47, 930, 265, 2011, 918, 931, 63, 330, 7, 3117, 229, 1166, 265, 267, 3118, 3119, 109, 3120, 265, 1383, 1993, 78, 1, 38, 50, 2003, 2004, 458, 287, 410, 392, 770, 1468, 1163, 927, 1, 462, 287, 929, 26, 743, 470, 1475, 266, 932, 641, 1476, 268, 1477, 60, 392, 1471, 1478, 233, 2012, 3121, 1, 11, 470, 1157, 328, 117, 3122, 2013, 317, 2014, 1479, 3123, 2015, 287, 364, 3, 462, 3124, 60, 11, 470, 1475, 932, 641, 2016, 3, 2017, 2012, 317, 2014, 446, 68, 48, 11, 3125, 470, 2018, 1, 43, 392, 770, 1, 3126, 209, 3127, 2019, 641, 3128, 1480, 3, 287, 392, 770, 1468, 927, 2, 1481, 132, 1478, 3129, 56, 353, 11, 470, 1157, 1, 287, 7, 181, 1471, 1476, 2020, 268, 287, 59, 3130, 1167, 914, 1482, 74, 1945, 3131, 2021, 368, 76, 122, 1, 118, 287, 59, 642, 772, 2, 872, 66, 28, 206, 27, 643, 773, 86, 51, 933, 1, 1168, 934, 287, 289, 3132, 1377, 774, 98, 60, 118, 287, 410, 268, 2, 7, 94, 772, 2022, 932, 641, 608, 2, 7, 1483, 2, 3133, 2, 22, 3134, 328, 3135, 52, 118, 287, 369, 1906, 2023, 765, 1482, 101, 1087, 10, 3136, 60, 90, 7, 932, 641, 1484, 287, 324, 872, 3, 735, 267, 775, 236, 2024, 116, 458, 932, 641, 122, 256, 49, 50, 63, 328, 267, 1165, 47, 1979, 1169, 2025, 261, 3137, 898, 1169, 267, 2025, 261, 6, 2026, 3138, 3139, 1, 545, 1485, 7, 10, 50, 776, 3140, 3141, 76, 1170, 777, 1171, 1164, 775, 914, 1913, 750, 1087, 3142, 3143, 328, 97, 750, 101, 1486, 1487, 200, 400, 3144, 3145, 1486, 2, 101, 778, 3146, 1948, 66, 1488, 3147, 1489, 3148, 615, 2027, 370, 50, 13, 237, 1478, 2028, 1169, 3149, 201, 371, 151, 372, 331, 359, 187, 133, 3150, 198, 26, 1488, 2015, 3151, 3152, 259, 2029, 3153, 2030, 179, 779, 133, 1164, 411, 86, 539, 621, 3154, 780, 781, 259, 1490, 781, 1422, 16, 754, 781, 1422, 2031, 775, 328, 1164, 544, 123, 2032, 1473, 325, 929, 261, 2007, 1472, 2008, 2009, 2010, 1473, 259, 76, 325, 400, 50, 1474, 329, 79, 267, 2033, 1396, 2034, 1172, 3155, 94, 3156, 79, 935, 11, 470, 1479, 2035, 1491, 470, 77, 1162, 2036, 1479, 3157, 79, 1, 743, 539, 3158, 3159, 94, 869, 109, 3160, 2037, 1169, 1492, 2038, 1488, 2037, 3161, 6, 3162, 3163, 1493, 322, 97, 936, 125, 332, 2039, 124, 1494, 1173, 290, 125, 3164, 2039, 2040, 19, 20, 6, 17, 24], [29, 3165, 3166, 75, 3167, 3168, 3169, 238, 77, 2041, 3170, 5, 4, 3171, 2042, 2043, 3172, 3173, 2044, 3174, 903, 1174, 3175, 3176, 3177, 3178, 3179, 227, 329, 78, 291, 75, 644, 13, 140, 1175, 329, 3180, 3181, 291, 645, 3182, 3183, 75, 3184, 469, 2, 2045, 200, 3185, 329, 1174, 3186, 2046, 3187, 238, 1175, 329, 78, 291, 2047, 2048, 1107, 291, 329, 2047, 2043, 41, 21, 13, 2049, 3188], [373, 28, 165, 228, 1176, 82, 32, 464, 1177, 646, 1495, 292, 5, 4, 3189, 82, 3190, 472, 412, 27, 2050, 3191, 1496, 766, 16, 165, 228, 782, 121, 100, 783, 734, 28, 1497, 187, 165, 228, 413, 2051, 369, 130, 3192, 413, 68, 198, 2052, 3193, 3194, 3195, 3196, 937, 938, 939, 3197, 28, 165, 228, 82, 165, 228, 27, 1498, 1411, 11, 3198, 3199, 182, 28, 1178, 48, 100, 783, 734, 1497, 187, 165, 228, 2042, 82, 37, 21, 3200, 266, 3201, 2053, 136, 32, 3202, 27, 3203, 1179, 99, 90, 187, 784, 32, 1487, 782, 3204, 2, 259, 177, 1499, 43, 1, 3205, 92, 3206, 37, 374, 449, 32, 414, 1, 401, 2, 82, 3207, 3208, 293, 9, 99, 401, 2, 2054, 1180, 917, 14, 46, 917, 373, 187, 32, 3209, 14, 3210, 46, 3, 32, 39, 2055, 2, 37, 32, 1500, 3211, 3212, 3213, 1501, 3214, 3215, 34, 1176, 2, 938, 3216, 1, 27, 647, 294, 1, 293, 82, 32, 132, 626, 32, 9, 258, 1181, 32, 144, 295, 165, 228, 782, 121, 375, 82, 546, 48, 144, 1, 165, 785, 28, 1178, 1182, 100, 783, 734, 1497, 32, 1178, 258, 1502, 269, 3217, 3218, 82, 546, 1183, 7, 546, 165, 228, 330, 3219, 32, 467, 82, 21, 1459, 547, 473, 32, 165, 785, 3220, 547, 3221, 98, 2056, 49, 3222, 266, 609, 3223, 2057, 165, 374, 940, 3224, 941, 1184, 266, 415, 3225, 284, 1185, 3226, 1503, 15, 938, 2058, 1, 940, 3227, 2059, 547, 1185, 1504, 3228, 266, 1505, 2060, 165, 785, 1506, 2, 2034, 85, 786, 210, 3, 266, 540, 3229, 416, 115, 270, 938, 1186, 609, 2061, 165, 228, 548, 81, 2062, 37, 21, 82, 165, 228, 1187, 2062, 187, 294, 3230, 266, 1507, 259, 23, 940, 3231, 3232, 942, 897, 296, 1508, 239, 474, 940, 1186, 547, 3233, 3234, 195, 14, 34, 2058, 211, 2063, 3235, 293, 165, 785, 940, 943, 3236, 2056, 549, 3237, 297, 2064, 210, 41, 32, 197, 3238, 82, 37, 21, 82, 1459, 2063, 540, 1509, 39, 3239, 1, 294, 1, 9, 28, 1178, 48, 2065, 362, 944, 924, 212, 1951, 2066, 187, 7, 32, 1510, 23, 3240, 2066, 23, 3241, 1188, 2067, 919, 3242, 3243, 7, 326, 3244, 648, 1, 211, 3245, 1511, 649, 394, 3246, 945, 550, 1902, 1379, 32, 3247, 1189, 187, 650, 1, 211, 2065, 3248, 82, 37, 21, 3249, 3250, 3251, 187, 211, 82, 37, 21, 211, 85, 115, 147, 782, 390, 891, 97, 2068, 624, 474, 551, 93, 946, 947, 2069, 3252, 55, 3253, 651, 32, 944, 187, 32, 787, 3254, 3255, 298, 3256, 1177, 787, 188, 3257, 3258, 787, 948, 1190, 2070, 3259, 3260, 3261, 652, 2071, 3, 7, 652, 528, 3262, 939, 2061, 3263, 2072, 3, 551, 93, 946, 947, 1182, 1, 82, 37, 21, 551, 93, 946, 947, 1182, 1, 82, 37, 21, 646, 55, 651, 3264, 1186, 2073, 210, 32, 82, 646, 55, 945, 1512, 608, 85, 449, 1507, 121, 376, 100, 1, 552, 117, 283, 1, 3265, 3266, 1, 133, 200, 187, 32, 787, 2074, 2069, 787, 3267, 2075, 1177, 213, 37, 270, 1513, 2076, 296, 551, 93, 946, 947, 1182, 1, 787, 2074, 82, 37, 21, 82, 1191, 144, 1, 1514, 1399, 351, 85, 786, 210, 165, 785, 943, 3268, 3269, 785, 2053, 32, 3270, 1512, 473, 100, 2070, 293, 356, 746, 2077, 32, 211, 18, 456, 18, 1192, 18, 653, 2073, 405, 1193, 2, 128, 788, 1, 92, 3271, 891, 9, 71, 3272, 949, 1514, 68, 1512, 115, 3273, 547, 2078, 949, 270, 415, 1514, 1399, 82, 37, 21, 9, 549, 187, 41, 654, 1187, 540, 1509, 281, 1, 1515, 944, 165, 228, 540, 924, 294, 950, 3274, 1965, 1096, 108, 3275, 3276, 3277, 3278, 3279, 3280, 3281, 15, 48, 3282, 1516, 3283, 9, 455, 211, 394, 68, 642, 1517, 214, 3, 1194, 951, 1518, 1, 98, 416, 395, 211, 18, 85, 634, 3284, 115, 3285, 85, 540, 1195, 913, 648, 236, 924, 294, 3, 82, 49, 293, 2079, 475, 372, 914, 751, 16, 130, 546, 3286, 467, 32, 374, 9, 49, 211, 115, 782, 85, 786, 3287, 3, 1495, 292, 82, 37, 21, 145, 899, 3288, 931, 546, 123, 82, 2080, 1187, 187, 294, 1, 41, 3289, 1519, 1520, 2080, 1519, 952, 1515, 82, 37, 21, 3290, 1521, 165, 228, 3291, 34, 123, 1183, 82, 37, 21, 2081, 3292, 123, 1183, 82, 37, 21, 3293, 3294, 187, 175, 1196, 3295, 187, 2082, 2, 48, 3296, 187, 2, 82, 37, 21, 3297, 953, 8, 739, 198, 740, 124, 230, 613, 882, 14, 739, 19, 20, 6, 17, 24], [1522, 655, 2083, 417, 152, 5, 4, 13, 656, 3298, 8, 13, 3299, 3300, 518, 1, 2084, 269, 942, 1522, 121, 655, 78, 417, 655, 2083, 152, 38, 1522, 64, 3301, 954, 3302, 3303, 2085, 3304, 3305, 240, 210, 465, 655, 2086, 417, 288, 75, 184, 2086, 657, 3306, 9, 3307, 290, 317, 3308, 3309, 2085, 2087, 32, 1523, 476, 3310, 3311, 955, 3, 63, 75, 655, 2088, 781, 3312, 97, 476, 782, 39, 1, 121, 78, 375, 655, 1524, 1460, 10, 1525, 789, 3313, 51, 3314, 477, 466, 51, 956, 3315, 3316, 934, 3317, 655, 2089, 113, 139, 33, 6, 41, 102, 1526, 8, 957, 1527, 790], [416, 41, 958, 1528, 37, 28, 2090, 1529, 5, 4, 1530, 2091, 8, 416, 41, 3318, 1528, 37, 28, 27, 647, 293, 959, 28, 2090, 939, 1529, 184, 2092, 3319, 3320, 286, 63, 1529, 1528, 37, 952, 3321, 2093, 41, 650, 1441, 43, 2093, 2094, 3322, 952, 32, 3323, 2046, 3324, 133, 658, 3325, 2095, 3326, 241, 92, 633, 77, 260, 1531, 952, 32, 637, 2091, 3327, 3328, 3329, 3330, 314, 259, 960, 5, 81, 1530, 119, 1197, 553, 777, 140, 259, 961, 1530, 19, 20, 17], [791, 1198, 533, 153, 3331, 189, 3332, 263, 5, 4, 792, 3333, 478, 3334, 3335, 1, 11, 1532, 659, 37, 11, 189, 377, 793, 477, 153, 794, 659, 37, 9, 377, 757, 189, 95, 418, 3336, 532, 11, 1532, 659, 37, 478, 3337, 153, 3338, 62, 2096, 180, 554, 611, 3, 351, 2097, 46, 153, 794, 463, 153, 3339, 3340, 153, 185, 1198, 3341, 1, 153, 479, 1533, 3342, 161, 2098, 13, 3343, 1199, 610, 546, 184, 9, 480, 1388, 1199, 263, 152, 478, 481, 633, 1534, 1532, 2099, 332, 9, 130, 11, 189, 377, 732, 377, 793, 477, 372, 377, 1200, 48, 2100, 11, 189, 1389, 1389, 99, 555, 377, 3344, 3345, 48, 795, 242, 372, 481, 13, 418, 922, 2101, 377, 114, 2102, 3346, 189, 2103, 1460, 1466, 3347, 116, 1535, 1536, 299, 78, 1201, 418, 1, 3348, 3349, 9, 1537, 1398, 660, 243, 918, 530, 962, 2, 97, 43, 3350, 136, 390, 1, 1202, 418, 3351, 372, 1534, 1535, 1536, 299, 415, 375, 936, 299, 1203, 482, 236, 71, 1204, 100, 936, 299, 68, 121, 1097, 30, 481, 64, 1160, 1, 1538, 2104, 2101, 418, 1, 145, 377, 1200, 48, 3352, 1539, 2105, 3353, 195, 2106, 660, 1148, 2107, 2108, 526, 2109, 153, 3354, 294, 2098, 13, 481, 1199, 1540, 546, 1535, 1536, 299, 1201, 418, 130, 11, 189, 377, 793, 477, 481, 2104, 3355, 3356, 3357, 332, 1160, 1097, 2110, 1, 1160, 795, 1205, 3358, 3359, 48, 295, 189, 2111, 3360, 9, 1206, 2112, 760, 3361, 607, 3362, 1405, 1958, 3363, 159, 23, 3364, 1201, 1207, 3365, 3366, 9, 1208, 796, 326, 661, 121, 1405, 1541, 3367, 34, 552, 1202, 468, 2113, 963, 659, 465, 794, 3368, 1201, 418, 1206, 1542, 2, 185, 1202, 418, 3369, 964, 515, 48, 456, 68, 479, 3370, 3371, 513, 797, 153, 330, 239, 48, 3372, 3373, 2114, 185, 1982, 662, 3374, 317, 2114, 3375, 1209, 2, 14, 1209, 3376, 3377, 3378, 350, 791, 2, 3379, 2, 1210, 455, 211, 115, 3380, 1148, 1180, 611, 516, 260, 3381, 798, 799, 3382, 189, 332, 145, 1210, 30, 189, 468, 390, 660, 189, 2103, 912, 3383, 1543, 78, 1211, 3384, 115, 121, 456, 791, 552, 98, 189, 663, 456, 663, 2115, 791, 18, 3385, 97, 449, 794, 791, 1126, 547, 3386, 9, 3387, 3388, 3389, 2116, 556, 1138, 3390, 1212, 664, 2117, 482, 632, 332, 2117, 482, 1, 145, 3391, 3392, 1992, 9, 2105, 770, 2118, 3393, 244, 419, 475, 3394, 800, 801, 3395, 3396, 419, 2118, 475, 482, 244, 3397, 800, 1544, 3398, 1545, 800, 1544, 795, 1546, 2119, 1, 965, 2120, 2121, 419, 2122, 3399, 3400, 245, 1213, 1104, 3401, 1, 9, 455, 7, 2121, 3402, 966, 477, 3403, 78, 263, 2106, 2123, 800, 1544, 2107, 1547, 2124, 634, 2119, 623, 113, 1547, 2108, 526, 949, 1548, 153, 332, 3404, 153, 967, 3405, 153, 3406, 1909, 3407, 1209, 3408, 968, 646, 9, 159, 3409, 3410, 611, 3411, 46, 2109, 3412, 3413, 3414, 963, 659, 465, 1199, 650, 13, 420, 481, 11, 1549, 3415, 286, 43, 263, 153, 44, 967, 665, 2125, 554, 68, 153, 16, 209, 646, 3416, 77, 260, 532, 3417, 2113, 14, 153, 3418, 3419, 802, 3420, 186, 3421, 182, 791, 1, 1214, 123, 14, 2126, 136, 1550, 153, 1209, 44, 3422, 188, 3, 3423, 1215, 2127, 260, 1, 1517, 153, 263, 1518, 16, 48, 1538, 1194, 1, 211, 623, 2128, 1192, 2129, 14, 456, 1543, 2130, 395, 964, 1397, 2129, 1551, 3, 7, 650, 153, 294, 2131, 3424, 1086, 968, 153, 1437, 263, 211, 3425, 802, 646, 659, 465, 377, 793, 3426, 47, 323, 2132, 632, 2132, 65, 261, 370, 189, 243, 1552, 3427, 1475, 969, 189, 1553, 653, 244, 1202, 418, 922, 78, 1, 2110, 1554, 3428, 184, 189, 468, 115, 2133, 3429, 7, 1555, 941, 1166, 1, 99, 1, 189, 326, 3430, 34, 463, 3431, 189, 2134, 2135, 749, 284, 377, 2135, 3432, 2082, 749, 284, 378, 90, 3433, 1556, 8, 41, 659, 37, 21, 792, 1557, 2, 1558, 119, 46, 910, 241, 1559, 1560, 1197, 1561, 19, 20, 6, 17, 24], [1562, 401, 2136, 666, 32, 557, 1216, 5, 4, 663, 666, 141, 667, 2136, 874, 1562, 401, 483, 238, 557, 1216, 484, 2137, 68, 3434, 482, 238, 1563, 2, 3435, 663, 666, 3436, 77, 3437, 124, 668, 26, 238, 2138, 1564, 2139, 557, 1216, 1, 669, 111, 1565, 375, 9, 874, 117, 803, 3438, 16, 378, 333, 3439, 3440, 1988, 395, 378, 1217, 1566, 166, 351, 364, 34, 111, 297, 666, 117, 3441, 2140, 3442, 246, 378, 669, 1218, 485, 462, 1, 75, 147, 246, 378, 2141, 38, 420, 195, 351, 364, 166, 970, 670, 97, 3443, 117, 270, 1409, 347, 1566, 611, 971, 111, 1096, 666, 874, 75, 3444, 1562, 1567, 130, 2142, 961, 198, 208, 238, 1174, 2052, 1563, 2, 186, 3445, 9, 657, 3446, 804, 483, 2138, 1564, 2139, 3447, 3448, 1918, 666, 3449, 414, 805, 3450, 1216, 3451, 2143, 300, 3452, 3453, 2144, 1551, 238, 3454, 2145, 3455, 1, 666, 238, 612, 1174, 2146, 1568, 1569, 130, 3456, 113, 72, 2146, 3457, 3458, 806, 3459, 483, 72, 238, 1523, 3460, 476, 3461, 3462, 1570, 3463, 3464, 247, 121, 2147, 671, 3465, 8, 1494, 290, 553, 124, 972, 2148, 3466, 973, 3467, 19, 20, 6, 17, 24], [1571, 1572, 1981, 807, 238, 808, 286, 5, 4, 807, 238, 808, 1573, 140, 167, 3468, 179, 3469, 8, 807, 238, 808, 286, 3470, 1546, 807, 807, 238, 808, 64, 13, 3471, 3472, 3473, 53, 63, 3474, 3475, 3476, 807, 3477, 3478, 16, 92, 238, 808, 2089, 1574, 445, 529, 2149, 1571, 1572, 610, 473, 728, 558, 1540, 270, 238, 294, 236, 71, 445, 529, 2149, 1571, 1572, 672, 376, 3479, 1575, 32, 152, 344, 2150, 3480, 2151, 3481, 2151, 3482, 3483, 3484, 1219, 3485, 2150, 3486, 617, 32, 75, 539, 3487, 974, 1210, 3488, 246, 2152, 210, 1573, 140, 81, 119, 1494, 1173, 290, 553, 1576, 1573, 140, 19, 20, 6, 17, 24], [975, 673, 2153, 559, 75, 560, 2154, 1577, 5, 4, 28, 27, 756, 167, 976, 483, 561, 16, 2155, 2156, 8, 3489, 326, 975, 673, 2153, 75, 1539, 75, 560, 2154, 1577, 2, 300, 975, 1578, 113, 1409, 3490, 1579, 3491, 233, 673, 3492, 3493, 13, 3494, 3495, 75, 1, 107, 976, 75, 483, 561, 16, 3496, 469, 1, 3497, 1, 12, 405, 3498, 562, 182, 132, 7, 233, 247, 977, 561, 16, 3499, 77, 977, 3500, 902, 30, 563, 797, 975, 355, 3501, 229, 2157, 190, 3502, 673, 2158, 166, 332, 563, 3503, 3504, 210, 977, 976, 3505, 3506, 3507, 472, 412, 27, 50, 559, 3508, 3509, 245, 75, 3510, 229, 94, 190, 266, 1580, 483, 3511, 12, 1923, 1207, 91, 2159, 1220, 1580, 1915, 975, 3512, 3513, 483, 2, 132, 9, 330, 245, 3514, 2159, 91, 85, 978, 1580, 559, 75, 3515, 136, 190, 3516, 978, 348, 562, 1581, 3, 49, 2155, 2156, 8, 57, 215, 30, 15, 301, 125, 57, 119, 142, 302, 19, 20, 6, 17, 24], [554, 1582, 809, 2160, 979, 388, 876, 1221, 980, 5, 4, 140, 269, 3517, 57, 22, 1583, 2161, 8, 1523, 956, 3518, 794, 809, 2160, 979, 388, 3519, 1579, 113, 1, 1162, 3520, 3521, 1212, 1135, 475, 2162, 16, 1102, 1162, 956, 1205, 145, 2163, 2096, 475, 663, 9, 757, 2102, 661, 2164, 3522, 484, 610, 1, 554, 263, 3523, 122, 294, 186, 1, 258, 2164, 484, 32, 1584, 1564, 3524, 1, 3525, 291, 956, 2165, 1521, 2166, 2, 7, 554, 3526, 629, 3527, 3528, 3529, 3530, 876, 1221, 77, 980, 610, 473, 3531, 980, 292, 71, 3532, 3533, 270, 3534, 2, 981, 1545, 3535, 980, 3536, 185, 188, 1175, 876, 1221, 528, 3537, 2064, 3538, 2125, 3539, 3540, 71, 3541, 1222, 9, 77, 980, 9, 953, 1533, 1585, 3542, 175, 3543, 2167, 1138, 3544, 71, 1102, 3545, 3546, 1585, 3547, 2127, 876, 1221, 3548, 956, 2165, 1521, 130, 2137, 3549, 1180, 1487, 7, 32, 3550, 3551, 94, 1897, 3552, 3553, 918, 1223, 895, 3554, 648, 3, 2168, 22, 1583, 2161, 8, 57, 215, 30, 15, 301, 125, 57, 119, 142, 302, 19, 20, 6, 17, 24], [654, 3555, 3556, 289, 810, 564, 1224, 674, 5, 4, 234, 4, 118, 972, 179, 3557, 3558, 8, 303, 2, 234, 810, 2169, 1225, 3559, 1214, 1225, 183, 2, 982, 654, 558, 46, 964, 3560, 1225, 3561, 2170, 474, 1226, 3562, 136, 234, 472, 412, 27, 286, 91, 366, 810, 1224, 674, 1586, 1587, 103, 564, 3, 811, 53, 2, 7, 543, 183, 3563, 2170, 483, 885, 75, 9, 145, 1225, 1496, 982, 654, 517, 810, 1224, 674, 654, 210, 658, 2092, 810, 28, 269, 27, 767, 3564, 1224, 674, 1588, 1586, 1587, 53, 2, 3, 1586, 1587, 674, 1588, 64, 810, 2171, 1227, 183, 271, 164, 885, 53, 2, 4, 3565, 28, 54, 4, 149, 14, 5, 207, 4, 81, 162, 150, 33, 235, 33, 4, 110, 19, 20, 6, 17, 24], [1228, 1589, 197, 3566, 1229, 3567, 5, 4, 334, 335, 365, 1228, 2172, 3568, 13, 2173, 1590, 1589, 197, 62, 241, 3569, 3570, 421, 615, 3571, 1229, 186, 144, 1228, 2172, 2174, 1591, 365, 3572, 2175, 905, 3573, 107, 1589, 197, 62, 983, 154, 7, 62, 1531, 898, 83, 1383, 1230, 216, 92, 984, 344, 57, 215, 30, 15, 301, 125, 57, 119, 142, 302, 19, 20, 6, 17, 24], [151, 104, 985, 272, 2176, 1200, 3574, 272, 1592, 1370, 345, 5, 4, 2177, 812, 35, 3575, 3576, 151, 104, 985, 272, 3577, 813, 40, 3578, 2178, 3579, 732, 1485, 151, 104, 675, 404, 3580, 222, 866, 2177, 3581, 2179, 812, 1556, 38, 1231, 272, 628, 731, 11, 813, 3582, 1232, 3583, 420, 986, 40, 1233, 3584, 76, 283, 1, 777, 985, 272, 628, 742, 470, 987, 747, 2176, 1200, 151, 104, 2180, 122, 986, 444, 3585, 3586, 6, 3587, 40, 3588, 444, 3589, 185, 40, 1233, 272, 40, 2181, 272, 40, 76, 742, 151, 104, 72, 104, 304, 79, 6, 988, 985, 272, 55, 134, 2182, 203, 282, 107, 1229, 3590, 2183, 3591, 628, 186, 151, 104, 26, 151, 104, 3592, 134, 2184, 213, 3593, 272, 40, 134, 988, 1120, 3594, 981, 345, 2, 1949, 1, 3595, 319, 614, 6, 2185, 3596, 3597, 814, 104, 675, 54, 2, 25, 40, 1234, 1, 865, 2186, 422, 3, 163, 1235, 3598, 1593, 101, 358, 813, 985, 272, 989, 743, 2187, 1491, 987, 2181, 272, 6, 1233, 272, 55, 3599, 676, 248, 47, 322, 56, 151, 104, 282, 123, 59, 399, 1115, 61, 1119, 463, 64, 1229, 164, 935, 47, 151, 104, 233, 480, 134, 26, 3600, 151, 104, 738, 272, 675, 40, 1234, 227, 557, 423, 151, 104, 74, 2187, 815, 1, 3601, 2180, 272, 2031, 742, 47, 49, 35, 67, 124, 35, 534, 535, 35, 67, 536, 81, 139, 33, 537, 35], [424, 29, 69, 143, 486, 411, 5, 4, 990, 4, 3602, 8, 3603, 334, 335, 2188, 3604, 50, 990, 2174, 424, 3605, 29, 69, 91, 118, 105, 565, 3606, 1, 3, 63, 29, 69, 91, 246, 143, 138, 84, 486, 411, 2, 44, 991, 3607, 12, 336, 337, 305, 183, 273, 2, 4, 28, 54, 4, 149, 14, 5, 207, 4, 81, 162, 150, 33, 235, 33, 4, 110, 19, 20, 6, 17, 24], [29, 69, 143, 69, 2189, 992, 560, 44, 1236, 5, 4, 41, 656, 63, 29, 69, 91, 336, 188, 69, 2189, 337, 305, 188, 29, 560, 183, 44, 177, 1594, 273, 2, 334, 335, 56, 336, 305, 188, 143, 212, 397, 12, 486, 411, 224, 72, 155, 226, 13, 237, 12, 13, 1595, 23, 12, 237, 214, 12, 331, 214, 12, 816, 397, 2, 3608, 2, 28, 206, 27, 224, 16, 13, 234, 154, 12, 12, 72, 12, 12, 226, 234, 13, 12, 12, 12, 12, 630, 748, 64, 29, 69, 118, 105, 565, 91, 2190, 164, 487, 271, 212, 487, 488, 273, 2, 7, 181, 1119, 993, 3609, 201, 2, 143, 138, 84, 271, 212, 877, 318, 2, 63, 72, 2191, 1537, 143, 486, 411, 2, 317, 1596, 143, 138, 246, 84, 268, 2, 143, 138, 278, 3610, 98, 64, 143, 1597, 2192, 2193, 2194, 1237, 421, 1238, 994, 337, 995, 802, 134, 2, 7, 1598, 677, 337, 183, 421, 1238, 994, 134, 12, 12, 544, 2, 424, 63, 29, 69, 28, 206, 27, 91, 3611, 56, 3612, 358, 83, 84, 1239, 1440, 2027, 370, 1599, 83, 817, 104, 2195, 358, 105, 618, 18, 621, 88, 1240, 379, 88, 2196, 879, 1600, 1601, 319, 36, 205, 1602, 878, 815, 88, 1603, 615, 345, 83, 105, 88, 227, 2197, 2198, 786, 227, 10, 29, 69, 91, 1493, 2199, 1241, 1, 2200, 134, 818, 678, 679, 126, 2201, 76, 1489, 47, 886, 1604, 2202, 2203, 2204, 76, 819, 2205, 2206, 2207, 2208, 820, 2209, 344, 57, 215, 3613, 57, 215, 30, 15, 301, 125, 57, 119, 142, 302, 19, 20, 6, 17, 24], [1605, 2210, 52, 613, 486, 984, 1606, 188, 733, 5, 4, 334, 335, 21, 1607, 988, 3614, 566, 83, 818, 126, 379, 990, 4, 1608, 8, 29, 69, 366, 143, 123, 154, 486, 411, 2, 105, 3615, 2211, 10, 566, 83, 818, 126, 261, 2, 334, 335, 64, 29, 118, 105, 565, 91, 162, 7, 98, 240, 283, 1, 1603, 565, 379, 38, 424, 233, 83, 817, 104, 105, 618, 18, 88, 1240, 26, 88, 2196, 879, 621, 926, 1601, 319, 36, 87, 1602, 878, 3616, 425, 345, 49, 333, 63, 72, 2191, 1537, 143, 486, 411, 881, 2, 411, 122, 317, 1596, 143, 138, 246, 84, 138, 278, 680, 126, 60, 2212, 1388, 2213, 3617, 1242, 2, 1404, 3618, 126, 2214, 315, 296, 1910, 884, 247, 526, 1609, 2, 143, 1597, 2192, 2193, 2194, 1237, 3619, 2215, 421, 1238, 994, 337, 995, 802, 134, 2, 7, 1598, 677, 12, 544, 2, 424, 29, 69, 28, 206, 27, 91, 2216, 2, 116, 2217, 105, 638, 1243, 2218, 424, 1191, 23, 488, 3620, 1237, 2215, 234, 216, 779, 216, 399, 3621, 3622, 76, 1244, 7, 216, 138, 424, 1237, 154, 27, 84, 421, 1238, 994, 134, 3623, 234, 13, 545, 681, 627, 1542, 126, 359, 398, 994, 1, 2219, 1, 638, 79, 105, 261, 969, 212, 996, 1607, 3624, 988, 2220, 935, 489, 1245, 969, 1610, 682, 2221, 1246, 39, 1, 2222, 1509, 26, 2223, 83, 638, 3625, 566, 83, 259, 21, 334, 335, 21, 143, 567, 2224, 21, 1610, 613, 682, 21, 2, 86, 105, 259, 391, 997, 83, 3626, 3627, 1173, 777, 105, 1942, 467, 3628, 391, 362, 134, 2, 991, 273, 2, 143, 154, 208, 567, 154, 18, 566, 83, 679, 333, 83, 3629, 1611, 3630, 26, 566, 259, 21, 489, 1245, 3631, 47, 84, 638, 814, 969, 818, 126, 154, 993, 2225, 818, 126, 154, 362, 379, 2, 143, 3632, 3633, 216, 15, 779, 984, 2, 567, 154, 3634, 984, 2, 567, 154, 362, 2225, 984, 2, 88, 565, 60, 83, 817, 104, 2195, 358, 105, 618, 18, 621, 88, 1240, 379, 88, 10, 879, 926, 1601, 319, 36, 205, 1602, 878, 88, 1603, 615, 180, 83, 105, 88, 2197, 2198, 786, 227, 10, 276, 29, 69, 1493, 2199, 1241, 1, 2200, 134, 818, 678, 679, 126, 2201, 76, 3635, 1489, 47, 7, 998, 1604, 2202, 2203, 2204, 819, 2205, 2206, 2207, 2208, 820, 2209, 1599, 28, 54, 4, 149, 14, 5, 207, 4, 81, 162, 150, 33, 235, 33, 4, 110, 19, 20, 6, 17, 24], [1612, 83, 999, 2226, 1613, 345, 352, 5, 4, 478, 481, 2227, 1, 2228, 4, 821, 2229, 1247, 566, 83, 568, 47, 481, 4, 22, 2230, 8, 478, 1612, 1614, 1, 2226, 1613, 345, 352, 1612, 1248, 83, 999, 676, 3636, 3637, 214, 1615, 204, 76, 26, 1249, 12, 105, 991, 3638, 1394, 3639, 1599, 38, 276, 164, 819, 3640, 481, 2231, 567, 892, 3641, 3642, 3643, 154, 83, 965, 3644, 2229, 47, 55, 1244, 2, 3645, 3646, 3647, 1249, 84, 803, 105, 991, 396, 926, 2, 422, 2, 821, 60, 3648, 3649, 123, 801, 3650, 154, 83, 965, 676, 3651, 804, 195, 154, 83, 965, 676, 55, 2218, 333, 3652, 1609, 567, 1590, 1590, 489, 1245, 10, 3653, 3654, 3655, 3656, 3657, 154, 241, 1247, 567, 892, 566, 83, 1616, 1226, 47, 1617, 3, 7, 54, 3658, 3659, 1618, 3660, 107, 1247, 567, 892, 3661, 55, 3662, 3663, 1616, 154, 1244, 64, 3664, 819, 1616, 1226, 154, 1244, 1247, 566, 1613, 568, 3, 47, 3665, 358, 2228, 332, 2232, 948, 1615, 3666, 1480, 56, 722, 1615, 779, 240, 283, 1, 2232, 948, 204, 55, 927, 3, 7, 998, 3667, 2145, 1999, 3668, 2094, 2233, 128, 1192, 1542, 2, 815, 83, 359, 999, 489, 1245, 47, 87, 3669, 1250, 1985, 233, 3670, 62, 1249, 1, 822, 2233, 128, 2076, 1249, 12, 105, 991, 1248, 105, 85, 926, 10, 1614, 88, 1, 105, 1370, 3671, 3672, 49, 28, 54, 4, 149, 14, 5, 207, 4, 81, 162, 150, 33, 235, 33, 4, 110, 19, 20, 6, 17, 24], [29, 69, 336, 337, 305, 183, 44, 1236, 487, 488, 5, 4, 13, 141, 3673, 8, 276, 29, 69, 91, 336, 188, 64, 337, 305, 188, 29, 560, 183, 44, 1594, 273, 2, 334, 335, 29, 118, 105, 565, 91, 487, 645, 3674, 10, 488, 7, 2234, 7, 280, 29, 487, 487, 161, 630, 346, 336, 397, 2, 12, 13, 237, 12, 13, 1595, 23, 12, 273, 2, 305, 12, 237, 214, 12, 331, 214, 12, 397, 2, 748, 13, 234, 28, 206, 27, 224, 72, 12, 12, 226, 1, 12, 12, 234, 13, 12, 12, 630, 12, 12, 397, 2, 816, 273, 2, 424, 305, 1619, 3675, 10, 64, 2227, 421, 2173, 995, 802, 544, 996, 133, 1610, 682, 26, 105, 1620, 319, 3676, 2235, 2224, 2223, 21, 141, 1621, 1622, 1623, 30, 15, 141, 198, 124, 740, 191, 1000, 1624, 141, 1625, 19, 20, 17, 24], [2236, 306, 1626, 1251, 5, 4, 3677, 823, 824, 4, 3678, 8, 424, 306, 216, 23, 304, 426, 990, 4, 1608, 8, 1001, 1002, 813, 306, 1626, 1251, 333, 2237, 2238, 2239, 1627, 216, 779, 2, 104, 3679, 676, 2, 748, 334, 335, 3680, 1628, 265, 26, 2240, 306, 216, 216, 23, 23, 304, 3681, 426, 38, 72, 2240, 104, 2241, 26, 398, 1, 11, 2242, 116, 306, 304, 23, 1231, 1232, 151, 104, 426, 2, 2242, 116, 3682, 246, 3683, 306, 813, 996, 3, 415, 11, 60, 2183, 772, 1, 1235, 825, 3684, 2236, 1593, 1629, 268, 2, 424, 233, 3685, 306, 304, 1540, 1630, 7, 167, 104, 426, 2, 420, 1001, 306, 23, 1629, 306, 23, 2243, 306, 23, 304, 426, 2, 1002, 813, 306, 15, 1626, 1251, 1, 1, 2244, 986, 304, 23, 1231, 1232, 104, 426, 2, 1, 3686, 986, 304, 23, 1231, 1, 1168, 1233, 304, 23, 1232, 413, 2245, 772, 23, 304, 151, 104, 23, 2246, 23, 426, 2, 7, 998, 2246, 104, 413, 2245, 772, 2239, 3687, 304, 426, 16, 1001, 2238, 1627, 216, 180, 344, 151, 104, 1001, 2237, 1627, 216, 180, 987, 826, 142, 426, 192, 1, 304, 2247, 987, 304, 142, 2247, 2243, 2248, 114, 216, 23, 1, 1168, 986, 23, 1629, 306, 23, 1001, 306, 23, 683, 142, 987, 2249, 56, 426, 28, 54, 4, 149, 14, 5, 207, 4, 81, 162, 150, 33, 235, 33, 4, 110, 19, 20, 6, 17, 24], [1252, 271, 138, 12, 5, 4, 488, 1591, 1631, 2250, 8, 72, 421, 817, 1252, 271, 212, 827, 3688, 3689, 517, 334, 335, 7, 98, 240, 210, 72, 1252, 2251, 201, 96, 144, 201, 56, 72, 271, 212, 1252, 138, 7, 1631, 2185, 3690, 212, 3691, 467, 212, 2212, 12, 1119, 904, 212, 488, 1591, 244, 3692, 216, 271, 212, 827, 44, 90, 216, 13, 3693, 3694, 3695, 996, 2252, 1632, 138, 216, 3696, 3697, 3698, 13, 3699, 241, 517, 244, 421, 2253, 271, 212, 827, 44, 90, 260, 13, 3700, 2253, 201, 2, 1631, 3701, 421, 1253, 1003, 271, 212, 827, 3702, 1253, 44, 90, 3703, 3704, 1253, 1633, 2175, 1253, 800, 1150, 145, 2254, 1634, 817, 229, 677, 1618, 827, 1635, 90, 1607, 1635, 677, 212, 827, 816, 1634, 816, 2254, 1634, 1635, 677, 229, 13, 3705, 1569, 216, 44, 90, 2250, 8, 57, 215, 30, 15, 301, 125, 57, 119, 142, 302, 19, 20, 6, 17, 24], [1605, 2210, 52, 336, 337, 305, 183, 1172, 44, 1236, 5, 4, 334, 335, 21, 993, 138, 486, 411, 488, 12, 226, 990, 4, 1608, 8, 63, 23, 69, 91, 336, 188, 29, 271, 123, 337, 305, 29, 995, 183, 143, 44, 1620, 273, 2, 334, 335, 105, 1167, 26, 29, 69, 118, 105, 565, 91, 366, 105, 2255, 201, 1, 96, 7, 318, 2, 3706, 487, 164, 487, 488, 38, 7, 271, 3707, 353, 280, 205, 487, 1598, 205, 226, 1, 1510, 143, 3708, 15, 212, 489, 402, 205, 226, 1, 155, 489, 29, 560, 1130, 2, 7, 402, 205, 226, 1, 1510, 138, 3709, 993, 44, 90, 7, 998, 421, 817, 988, 2220, 828, 201, 2, 3710, 201, 96, 336, 337, 820, 2, 136, 305, 1254, 1545, 160, 781, 3711, 3712, 1619, 489, 305, 1619, 489, 919, 820, 2, 29, 560, 364, 995, 105, 1620, 273, 2, 143, 60, 993, 138, 87, 358, 1250, 15, 212, 397, 12, 336, 545, 60, 486, 411, 28, 206, 27, 224, 155, 226, 3713, 2, 305, 545, 155, 630, 334, 335, 21, 1250, 397, 273, 12, 336, 60, 13, 237, 12, 13, 1595, 23, 12, 214, 662, 2256, 12, 13, 372, 12, 305, 237, 214, 12, 331, 214, 12, 2256, 214, 12, 372, 214, 12, 161, 397, 2, 748, 2, 333, 13, 234, 154, 60, 28, 206, 27, 224, 3714, 489, 820, 161, 56, 336, 188, 397, 12, 12, 12, 72, 12, 12, 226, 234, 13, 12, 12, 72, 205, 12, 12, 161, 630, 143, 138, 1597, 3715, 1636, 490, 3716, 828, 517, 28, 54, 4, 149, 14, 5, 207, 4, 81, 162, 150, 33, 235, 33, 4, 110, 19, 20, 6, 17, 24], [684, 685, 1255, 569, 1256, 681, 930, 5, 4, 11, 684, 685, 3717, 1255, 3718, 3719, 681, 930, 570, 374, 571, 1255, 1502, 3720, 2257, 2258, 1436, 249, 472, 412, 27, 1004, 168, 685, 652, 684, 126, 569, 2259, 3721, 3, 41, 4, 658, 28, 2257, 2258, 1436, 249, 472, 412, 27, 1004, 168, 685, 652, 684, 569, 112, 569, 933, 1, 181, 68, 1966, 1637, 65, 398, 1256, 681, 930, 684, 1638, 2260, 479, 3722, 350, 684, 154, 6, 247, 1254, 1005, 154, 350, 3, 3723, 1630, 12, 112, 15, 1170, 2, 7, 2261, 1, 1171, 1639, 2262, 684, 685, 569, 569, 933, 289, 181, 751, 65, 1256, 3724, 1397, 569, 2263, 751, 65, 1256, 1539, 1255, 107, 569, 123, 2264, 3725, 16, 65, 1124, 2265, 8, 191, 137, 572, 46, 140, 4, 19, 20, 6, 17, 24, 242, 46, 44, 177, 14, 139, 167, 573, 574], [1257, 248, 61, 27, 575, 686, 137, 108, 338, 548, 829, 5, 4, 570, 374, 571, 234, 2266, 2267, 249, 472, 412, 27, 3726, 179, 103, 338, 548, 1004, 1432, 671, 564, 1257, 248, 61, 108, 41, 141, 413, 3727, 1257, 248, 61, 108, 338, 548, 829, 27, 575, 686, 137, 248, 234, 2266, 2267, 249, 472, 412, 27, 3728, 179, 103, 338, 548, 1004, 3729, 108, 51, 1258, 355, 248, 1432, 671, 564, 512, 1171, 1640, 1171, 112, 634, 3730, 3731, 642, 394, 248, 61, 288, 1005, 607, 9, 288, 1158, 575, 686, 296, 575, 686, 3732, 3733, 830, 3734, 1, 1005, 98, 3735, 1005, 205, 629, 3736, 3737, 27, 562, 575, 686, 137, 1, 289, 413, 3738, 3739, 3740, 355, 248, 671, 186, 575, 686, 1641, 9, 1642, 2264, 108, 338, 548, 829, 27, 575, 686, 137, 1, 289, 2, 44, 951, 3741, 1637, 248, 258, 671, 2268, 2269, 333, 562, 805, 409, 1006, 1005, 1158, 575, 3742, 137, 1, 3743, 247, 528, 3744, 2072, 3745, 3746, 179, 103, 338, 548, 1004, 1168, 300, 1186, 1618, 43, 687, 136, 248, 65, 2270, 3747, 3748, 108, 179, 103, 65, 182, 248, 2271, 953, 1259, 27, 288, 2272, 3749, 1212, 3750, 2273, 248, 2274, 27, 933, 65, 182, 2275, 2272, 2276, 525, 3751, 27, 562, 2277, 427, 27, 906, 1257, 248, 629, 671, 564, 97, 179, 103, 2271, 3752, 322, 248, 3753, 607, 1, 1222, 3754, 1260, 3755, 2278, 8, 191, 137, 572, 46, 140, 4, 19, 20, 6, 17, 24, 242, 46, 44, 177, 14, 139, 167, 573, 574], [831, 420, 338, 643, 688, 773, 1007, 288, 1261, 159, 1008, 2275, 65, 5, 4, 3756, 831, 241, 28, 206, 27, 1261, 182, 288, 289, 491, 570, 374, 571, 831, 420, 643, 688, 773, 159, 1008, 832, 41, 4, 2279, 28, 100, 783, 734, 643, 688, 2280, 249, 643, 688, 773, 338, 729, 365, 44, 295, 3757, 1609, 3758, 1643, 2276, 3759, 1505, 1, 831, 100, 167, 3760, 2281, 2282, 3761, 3762, 3763, 3764, 2283, 953, 1259, 107, 338, 2284, 1638, 112, 2283, 953, 1259, 831, 1007, 288, 1261, 657, 2, 831, 72, 1644, 688, 1009, 464, 289, 1007, 338, 1261, 3765, 1, 3, 400, 100, 1008, 409, 658, 28, 100, 783, 734, 643, 688, 773, 391, 249, 472, 412, 27, 338, 643, 688, 773, 729, 1638, 352, 355, 159, 1008, 409, 164, 289, 2235, 112, 65, 112, 159, 1008, 409, 107, 2274, 206, 27, 455, 2284, 65, 112, 63, 27, 65, 112, 2054, 422, 407, 625, 3766, 875, 2265, 8, 191, 137, 572, 46, 140, 4, 19, 20, 6, 17, 24, 242, 46, 44, 177, 14, 139, 167, 573, 574], [13, 1010, 1262, 3767, 5, 4, 570, 374, 571, 41, 141, 2285, 3768, 1010, 1524, 13, 1263, 413, 1010, 1262, 3769, 247, 13, 1263, 74, 562, 681, 749, 3770, 1010, 1262, 471, 462, 3771, 1264, 2286, 526, 38, 1263, 72, 963, 1010, 3772, 3773, 3774, 1265, 3775, 2140, 10, 179, 3776, 256, 1, 3, 13, 1262, 26, 450, 3777, 1265, 23, 1, 13, 1263, 2144, 492, 328, 3778, 3779, 3780, 2287, 3781, 625, 3782, 1, 47, 2278, 8, 191, 137, 572, 46, 140, 4, 19, 20, 6, 17, 24, 242, 46, 44, 177, 14, 139, 167, 573, 574], [493, 428, 202, 114, 156, 223, 766, 5, 4, 114, 1645, 317, 3783, 230, 1266, 1646, 1647, 833, 110, 834, 2288, 3784, 429, 1648, 40, 370, 77, 2289, 1267, 8, 114, 156, 1214, 3, 108, 2290, 413, 2291, 156, 2292, 464, 155, 493, 428, 45, 114, 530, 1649, 130, 493, 428, 202, 114, 454, 1268, 2148, 40, 54, 40, 370, 155, 1977, 3785, 56, 72, 493, 74, 480, 1645, 402, 205, 630, 86, 402, 205, 226, 3786, 114, 1395, 112, 208, 108, 1109, 3787, 114, 1920, 493, 74, 480, 1645, 181, 65, 182, 2293, 380, 134, 833, 110, 60, 114, 74, 156, 1269, 346, 68, 3788, 1269, 346, 247, 454, 43, 223, 1, 322, 56, 833, 110, 74, 156, 2294, 114, 45, 2295, 869, 66, 109, 114, 156, 230, 1266, 2296, 1011, 758, 1011, 2297, 2296, 1011, 60, 1230, 454, 43, 1424, 463, 114, 156, 45, 223, 3789, 333, 3790, 3791, 224, 2298, 1646, 417, 66, 230, 624, 2299, 3792, 3793, 1647, 51, 2, 758, 1011, 1230, 454, 43, 66, 463, 833, 110, 114, 74, 156, 223, 879, 257, 3794, 3795, 116, 911, 775, 3796, 2288, 834, 262, 3797, 101, 30, 262, 1268, 601, 39, 1, 494, 398, 70, 603, 398, 1900, 642, 3798, 15, 283, 613, 1268, 101, 494, 102, 66, 687, 34, 2300, 429, 911, 1, 2289, 1, 222, 132, 3799, 833, 110, 74, 156, 495, 108, 2290, 31, 623, 1084, 3800, 2301, 284, 74, 156, 2302, 1650, 476, 3801, 2303, 31, 39, 676, 2, 3, 333, 1270, 2293, 74, 156, 1268, 429, 3802, 54, 40, 66, 204, 7, 98, 450, 625, 3803, 273, 2, 49, 833, 110, 134, 3804, 1271, 777, 2304, 1192, 163, 114, 156, 45, 223, 647, 74, 156, 907, 2304, 3805, 811, 2305, 74, 156, 279, 545, 125, 682, 339, 176, 3806, 3807, 494, 70, 1492, 3808, 74, 3809, 3810, 283, 70, 3811, 51, 3812, 359, 2306, 458, 2307, 1272, 1600, 74, 156, 31, 3813, 3814, 192, 204, 1, 3815, 1490, 634, 3816, 3817, 3818, 1490, 2184, 829, 296, 1485, 443, 15, 1273, 114, 156, 346, 66, 463, 72, 74, 156, 114, 494, 2308, 402, 205, 59, 3819, 1, 1130, 2, 2298, 1646, 3820, 2299, 1647, 1650, 476, 109, 3821, 1011, 3822, 3823, 3824, 230, 1266, 400, 101, 493, 428, 496, 114, 156, 40, 370, 1481, 257, 7, 428, 202, 3825, 1651, 2309, 101, 681, 3826, 1012, 2059, 3, 202, 233, 318, 2309, 11, 2310, 866, 1, 258, 148, 689, 257, 3827, 1270, 1, 804, 156, 3828, 21, 3, 163, 429, 911, 300, 774, 690, 935, 3829, 2311, 241, 3], [2312, 51, 921, 45, 135, 79, 5, 4, 1267, 8, 3830, 3831, 78, 51, 921, 45, 135, 144, 51, 921, 45, 62, 3832, 1089, 109, 244, 358, 45, 3833, 3834, 13, 3835, 3836, 3837, 365, 1228, 3838, 1235, 3839, 237, 3840, 3841, 3842, 45, 3843, 1274, 3844, 331, 3845, 45, 1502, 1652, 2313, 3846, 1653, 2313, 3847, 1654, 3848, 1085, 3849, 45, 928, 3850, 3851, 3852, 1275, 825, 1140, 3853, 3854, 3855, 45, 478, 3856, 3857, 3858, 3859, 151, 3860, 228, 3861, 45, 283, 2, 2312, 2314, 63, 135, 2, 51, 921, 45, 623, 1654, 494, 128, 2314, 74, 2310, 6, 358, 1654, 2315, 815, 74, 2222, 76, 471, 45, 2316, 1087, 3862, 3863, 3864, 789, 76, 79], [2317, 3865, 10, 29, 307, 1107, 5, 4, 2318, 2319, 576, 3866, 1267, 8, 2317, 54, 193, 29, 307, 576, 2320, 196, 2321, 279, 1, 1655, 3867, 2302, 2322, 2323, 1656, 10, 1276, 31, 957, 512, 3868, 2, 1657, 39, 1, 577, 1107, 3, 2318, 2319, 2324, 835, 2325, 2326, 200, 1277, 964, 291, 266, 10, 3869, 193, 3870, 2327, 3871, 2324, 835, 608, 2328, 1, 1658, 279, 691, 497, 157, 3872, 1655, 1894, 1656, 2326, 1013, 1659, 758, 1660, 627, 61, 3, 2329, 1148, 1661, 2325, 1642, 339, 1996, 266, 3873, 1662, 2330, 109, 3874, 1014, 2331, 3875, 3876, 497, 497, 157, 2, 2321, 109, 80, 1873, 15, 58, 1663, 1276, 1015, 274, 492, 1278, 469, 3877, 1277, 2332, 2333, 1657, 307, 738, 3878, 2332, 34, 3879, 1207, 1892, 3880, 692, 3881, 1277, 1659, 466, 61, 3, 2329, 1273, 1656, 963, 1278, 31, 1016, 2320, 152, 14, 97, 2333, 790, 723, 1664, 1, 578, 2, 339, 3882, 247, 1278, 213, 1276, 3883, 3884, 3885, 3886, 835, 3887, 2334, 515, 3888, 982, 835, 31, 39, 180, 3889, 2335, 1014, 23, 982, 835, 31, 2336, 1017, 207, 1939, 157, 555, 2337, 2338, 2, 3, 92, 1665, 1277, 3890, 944, 2339, 836, 362, 174, 869, 2294, 43, 399, 676, 3891, 497, 3892, 680, 369, 1666, 2157, 356, 3893, 1416, 3894, 3895, 814, 2, 176, 1013, 3896, 339, 3897, 492, 693, 1, 1187, 1657, 3898, 637, 3899, 2285, 692, 1880, 723, 1279, 2340, 2341, 2342, 647], [1667, 149, 308, 559, 498, 75, 564, 491, 245, 5, 4, 570, 374, 571, 673, 2343, 2344, 183, 13, 1668, 2345, 1669, 2346, 954, 249, 464, 308, 1009, 1644, 308, 1009, 308, 559, 498, 75, 491, 245, 308, 559, 498, 75, 308, 498, 136, 3, 41, 41, 348, 2347, 191, 137, 572, 46, 140, 4, 19, 20, 6, 17, 24, 242, 46, 44, 177, 14, 139, 167, 573, 574], [1670, 1431, 1280, 281, 1, 1018, 1281, 5, 4, 1267, 8, 1670, 1414, 3900, 193, 213, 3901, 3902, 10, 2348, 1018, 41, 3903, 2349, 633, 1281, 1018, 1671, 381, 2350, 78, 2351, 339, 1, 3904, 1018, 37, 480, 324, 1, 1670, 1431, 1280, 92, 2045, 200, 1672, 837, 2, 3, 1672, 3905, 278, 56, 39, 1, 918, 2348, 1661, 2, 1673, 319, 56, 3906, 157, 3, 44, 66, 665, 13, 3907, 3908, 1569, 113, 1671, 381, 2350, 497, 1144, 966, 3909, 2339, 3910, 896, 339, 1552, 381, 3911, 1003, 942, 3912, 1671, 339, 26, 3913, 553, 3, 3914, 1674, 3915, 63, 1281, 1, 113, 551, 1018, 2349, 633, 903, 1282, 97, 1279, 1, 758, 1, 3, 339, 2, 1013, 381, 1283, 210, 3, 49], [1667, 149, 978, 308, 498, 75, 2343, 2352, 362, 5, 4, 570, 374, 571, 190, 308, 2344, 183, 13, 1668, 2345, 1669, 2346, 954, 249, 464, 308, 1009, 1644, 308, 1009, 308, 559, 498, 75, 491, 245, 308, 559, 498, 75, 308, 498, 136, 3, 41, 41, 348, 2347, 191, 137, 572, 46, 140, 4, 19, 20, 6, 17, 24, 242, 46, 44, 177, 14, 139, 167, 573, 574], [127, 1284, 127, 1285, 265, 28, 27, 430, 269, 838, 3916, 134, 5, 4, 127, 1285, 265, 21, 13, 4, 1019, 1020, 2353, 127, 1284, 28, 206, 27, 430, 3917, 948, 518, 1, 28, 2354, 1286, 127, 269, 838, 1021, 180, 38, 127, 1284, 2028, 127, 1285, 265, 208, 1675, 127, 822, 39, 1, 127, 269, 692, 3, 269, 838, 809, 78, 134, 127, 1285, 265, 1675, 127, 127, 1284, 78, 629, 269, 838, 1, 3, 469, 269, 838, 809, 127, 2355, 3918, 21, 2, 123, 3919, 376, 300, 127, 3920, 2355, 611, 3921, 3922, 650, 2, 2124, 1655, 369, 2356, 127, 685, 291, 3, 3923, 543, 694, 1, 1, 839, 2353, 127, 1391, 2357, 206, 369, 2356, 127, 11, 840, 456, 2358, 2259, 692, 1265, 962, 2, 1445, 422, 2, 3924, 3925, 1179, 553, 3, 333, 3926, 127, 11, 3927, 1676, 38, 3928, 2359, 1675, 127, 11, 2360, 127, 206, 369, 3929, 190, 3930, 269, 997, 2, 2361, 1677, 685, 3931, 2362, 468, 1678, 9, 2361, 2362, 468, 30, 3932, 39, 1, 765, 492, 3933, 1265, 239, 2, 3934, 49, 7, 838, 809, 516, 303, 134, 2, 27, 365, 1287, 3935, 21, 2, 28, 2354, 1286, 518, 1, 179, 127, 3936, 617, 206, 27, 2363, 3937, 692, 13, 291, 3, 841, 842, 843, 349, 639, 4, 33, 28, 54, 4, 149, 14, 5, 207, 4, 81, 162, 150, 33, 235, 33, 4, 110, 19, 20, 6, 17, 24], [579, 382, 576, 196, 5, 4, 579, 21, 13, 4, 1019, 1020, 1084, 1022, 193, 579, 576, 382, 1679, 1145, 208, 382, 1023, 1024, 196, 375, 63, 576, 579, 1680, 3938, 1025, 382, 97, 499, 746, 2, 1025, 431, 665, 382, 3939, 492, 1681, 382, 1679, 1145, 3940, 1288, 291, 274, 382, 1022, 376, 2364, 2365, 2366, 3941, 3942, 296, 625, 1025, 499, 2366, 2367, 395, 665, 382, 1023, 1024, 3943, 579, 332, 2368, 1, 1681, 1023, 1024, 1682, 1272, 210, 691, 382, 1022, 629, 1683, 1023, 1024, 549, 3944, 1, 1533, 97, 2328, 1, 1022, 579, 382, 1680, 691, 97, 1025, 431, 665, 1084, 1022, 72, 3945, 3946, 3947, 3948, 1278, 196, 1419, 736, 1129, 43, 3949, 1926, 3950, 1091, 1428, 63, 576, 163, 72, 579, 1016, 3951, 262, 1679, 1145, 1524, 1, 1023, 1024, 2368, 1, 967, 67, 196, 123, 422, 407, 3, 382, 244, 2369, 3952, 579, 67, 493, 689, 26, 102, 192, 2369, 353, 3953, 262, 2, 841, 842, 843, 349, 639, 4, 33, 28, 54, 4, 149, 14, 5, 207, 4, 81, 162, 150, 33, 235, 33, 4, 110, 19, 20, 6, 17, 24], [383, 298, 447, 3954, 1684, 580, 5, 4, 383, 298, 21, 13, 4, 1019, 1020, 1685, 1026, 1289, 383, 298, 29, 69, 213, 383, 298, 1684, 549, 432, 549, 580, 375, 1289, 383, 298, 63, 29, 69, 3955, 1213, 1686, 3956, 1439, 6, 62, 3957, 1660, 3958, 645, 1213, 3, 580, 53, 2, 7, 10, 3959, 518, 1, 2370, 2371, 26, 383, 298, 549, 432, 580, 383, 298, 3960, 2360, 3961, 2370, 2371, 695, 3962, 1377, 3963, 694, 1, 181, 432, 580, 3964, 53, 1464, 480, 617, 383, 298, 549, 432, 6, 3965, 746, 1685, 1026, 1289, 383, 298, 233, 1687, 2372, 992, 415, 653, 1003, 113, 1193, 115, 1464, 1684, 580, 2, 195, 580, 316, 1, 260, 383, 298, 1290, 2, 3, 39, 1, 115, 1688, 113, 1689, 564, 248, 49, 400, 1685, 1026, 1289, 383, 298, 2373, 3966, 3967, 839, 10, 3968, 3969, 79, 6, 3970, 903, 10, 269, 75, 580, 39, 1, 115, 1688, 113, 1193, 841, 842, 843, 349, 639, 4, 33, 28, 54, 4, 149, 14, 5, 207, 4, 81, 162, 150, 33, 235, 33, 4, 110, 19, 20, 6, 17, 24], [3971, 1027, 1028, 844, 288, 1291, 519, 5, 4, 309, 1028, 844, 22, 3972, 3973, 2, 22, 1690, 309, 22, 101, 2374, 1291, 2, 15, 3, 1028, 844, 1027, 330, 186, 375, 3974, 844, 3975, 3976, 1027, 1691, 3977, 195, 1617, 3978, 2375, 54, 112, 3, 1691, 491, 3, 240, 1593, 22, 2376, 491, 3, 133, 3979, 3980, 3981, 3982, 844, 22, 2377, 1028, 1002, 3983, 679, 288, 22, 1291, 679, 519, 3, 22, 2377, 1027, 3984, 78, 22, 1690, 362, 180, 2, 1690, 3985, 1027, 3986, 48, 3987, 1105, 500, 3988, 1606, 906, 3989, 416, 3990, 3991, 138, 1, 22, 1291, 278, 3992, 3993, 1691, 6, 2376, 240, 396, 2378, 22, 2100, 1692, 515, 3, 2379, 61, 2380, 431, 14, 3994, 1086, 375, 2379, 1210, 394, 3995, 1379, 1204, 34, 844, 22, 3996, 92, 99, 1097, 3997, 22, 563, 3998, 3999, 474, 2381, 309, 826, 309, 67, 124, 696, 1, 140, 1029, 1292, 278, 1693, 973, 309, 19, 20, 6, 17, 24], [237, 697, 698, 531, 303, 954, 4000, 1030, 250, 2382, 75, 1031, 5, 4, 697, 698, 21, 13, 4, 1019, 1020, 237, 1032, 250, 193, 697, 698, 64, 1030, 250, 2382, 75, 78, 531, 543, 1031, 38, 63, 75, 107, 53, 2, 697, 698, 1032, 250, 78, 1030, 269, 75, 2033, 53, 2, 7, 2383, 39, 1, 1476, 1032, 250, 269, 1101, 1293, 3, 635, 203, 1223, 1030, 1579, 2351, 26, 39, 1, 75, 650, 76, 379, 333, 355, 430, 492, 2384, 250, 898, 39, 1, 1030, 1032, 250, 291, 3, 4001, 239, 329, 2385, 1575, 10, 4002, 1294, 469, 2, 3, 697, 698, 51, 697, 698, 1998, 1541, 250, 92, 1124, 97, 90, 237, 822, 2316, 480, 638, 1694, 4003, 1032, 250, 78, 39, 1, 692, 291, 3, 2337, 75, 2383, 53, 38, 400, 697, 698, 1122, 174, 26, 186, 2, 237, 2386, 4004, 602, 4005, 1270, 1, 1550, 51, 4006, 250, 4007, 74, 250, 1455, 1286, 894, 476, 11, 250, 4008, 65, 135, 2, 3, 72, 101, 829, 250, 4009, 476, 4010, 53, 2, 101, 44, 1541, 250, 65, 135, 2, 841, 842, 843, 349, 639, 4, 33, 28, 54, 4, 149, 14, 5, 207, 4, 81, 162, 150, 33, 235, 33, 4, 110, 19, 20, 6, 17, 24], [531, 10, 4011, 1295, 1695, 1696, 501, 433, 432, 5, 4, 1697, 4012, 1, 378, 4013, 13, 4, 1019, 1020, 4014, 4015, 364, 1296, 46, 307, 315, 4016, 4017, 531, 743, 338, 652, 699, 307, 4018, 300, 4019, 699, 1424, 44, 109, 276, 2387, 118, 1, 699, 307, 501, 433, 281, 1, 1696, 501, 433, 432, 2388, 633, 1195, 281, 70, 555, 2389, 513, 2286, 433, 157, 1, 3, 4020, 2303, 4021, 501, 433, 432, 1698, 531, 469, 1297, 4022, 469, 1699, 682, 4023, 4024, 21, 1698, 1696, 501, 433, 432, 1195, 2390, 699, 4025, 1700, 2391, 2392, 1701, 696, 432, 1702, 2392, 501, 433, 2390, 699, 549, 501, 433, 296, 1697, 1649, 699, 169, 501, 433, 2391, 296, 376, 1703, 501, 433, 1701, 796, 2393, 376, 1703, 4026, 1033, 696, 432, 1702, 296, 1702, 159, 2394, 161, 2393, 1703, 1697, 1649, 4027, 4028, 65, 4029, 4030, 841, 842, 843, 349, 639, 4, 33, 28, 54, 4, 149, 14, 5, 207, 4, 81, 162, 150, 33, 235, 33, 4, 110, 19, 20, 6, 17, 24], [1458, 349, 340, 753, 4031, 602, 1159, 541, 39, 1, 1294, 2395, 1295, 1695, 1461, 5, 4, 35, 466, 763, 764, 140, 4032, 71, 539, 1146, 340, 230, 4033, 4034, 241, 340, 4035, 2396, 3, 351, 48, 23, 458, 1704, 2397, 1, 340, 323, 193, 340, 581, 51, 1705, 2398, 502, 4036, 3, 4037, 4038, 1706, 4039, 340, 581, 2399, 4040, 175, 4041, 4042, 943, 1707, 581, 1142, 1142, 4043, 4044, 943, 1707, 828, 416, 4045, 2367, 2400, 1, 4046, 633, 2399, 581, 582, 997, 1859, 4047, 1707, 4048, 602, 128, 1, 2401, 1708, 92, 416, 4049, 828, 416, 4050, 117, 356, 2048, 1034, 1709, 1294, 2395, 616, 1550, 340, 842, 2000, 170, 1035, 2, 1128, 157, 4051, 2402, 925, 4052, 4053, 71, 1445, 691, 1248, 1708, 4054, 4055, 1687, 2389, 760, 3, 250, 4056, 2403, 3, 677, 4057, 214, 1706, 2386, 4058, 4059, 2404, 728, 4060, 25, 1236, 340, 581, 1705, 51, 1705, 51, 2163, 4061, 2405, 353, 4062, 1, 1962, 4063, 616, 4064, 1159, 4065, 2406, 64, 499, 55, 692, 2407, 723, 340, 581, 693, 7, 26, 1120, 4066, 1372, 192, 1298, 410, 1295, 1695, 1461, 4067, 1371, 4068, 55, 1463, 345, 237, 4069, 4070, 4071, 1710, 2408, 2409, 340, 581, 690, 999, 690, 4072, 4073, 23, 458, 1704, 2188, 620, 4074, 1674, 79, 4075, 1016, 134, 25, 1440, 2407, 4076, 4077, 502, 4078, 530, 1036, 2410, 2, 431, 1234, 2411, 2398, 502, 733, 603, 227, 2352, 51, 25, 349, 458, 1, 1458, 340, 1294, 128, 340, 581, 193, 2412, 814, 10, 347, 2002, 35, 67, 124, 35, 534, 535, 35, 67, 536, 81, 139, 33, 537, 35], [806, 2413, 1682, 1, 2402, 4079, 644, 4080, 2414, 2415, 825, 62, 2416, 5, 4, 35, 466, 763, 764, 490, 4081, 518, 1, 1275, 825, 62, 806, 310, 1026, 90, 496, 888, 625, 2417, 2024, 181, 503, 845, 310, 51, 1711, 4082, 230, 624, 7, 62, 4083, 2416, 1105, 3, 503, 845, 310, 1711, 51, 1711, 51, 2418, 1159, 541, 765, 2406, 503, 4084, 134, 2418, 958, 7, 260, 4085, 4086, 541, 236, 905, 1712, 2, 310, 523, 51, 480, 4087, 4088, 4089, 2, 4090, 4091, 91, 1713, 310, 431, 227, 4092, 128, 2414, 2415, 1026, 503, 845, 310, 2397, 1, 7, 4093, 4094, 431, 2419, 70, 2411, 4095, 503, 4096, 4097, 296, 4098, 1713, 2, 1287, 1714, 13, 667, 4099, 1299, 161, 806, 2413, 499, 1682, 4100, 1014, 2420, 310, 2421, 431, 2126, 291, 3, 4101, 503, 1287, 1714, 2364, 366, 839, 1134, 2422, 295, 4102, 902, 4103, 4104, 2403, 760, 431, 925, 1714, 4105, 4106, 350, 2401, 1708, 4107, 1410, 4108, 4109, 760, 839, 2422, 4110, 431, 4111, 4112, 4113, 1402, 503, 845, 310, 329, 2385, 922, 2404, 4114, 683, 632, 4115, 1673, 4116, 3, 1146, 70, 555, 34, 4117, 1288, 2423, 4118, 1181, 983, 4119, 503, 845, 310, 502, 825, 62, 690, 999, 2, 1650, 476, 1715, 843, 756, 690, 2424, 825, 378, 4120, 1706, 2115, 399, 109, 1129, 1565, 4121, 523, 51, 4122, 4123, 561, 56, 1994, 911, 616, 690, 244, 261, 352, 503, 845, 310, 4124, 404, 51, 294, 1, 1713, 806, 690, 1120, 620, 4125, 318, 4126, 26, 137, 502, 79, 195, 310, 1234, 227, 62, 4127, 213, 2410, 4128, 310, 431, 627, 395, 4129, 213, 4130, 1651, 2, 4131, 1154, 38, 35, 67, 124, 35, 534, 535, 35, 67, 536, 81, 139, 33, 537, 35], [151, 1716, 155, 4132, 1636, 43, 516, 1037, 1441, 5, 4, 1038, 1039, 4133, 396, 62, 155, 479, 1033, 133, 1582, 1037, 732, 4134, 273, 2, 1717, 56, 114, 2248, 1189, 518, 1, 4135, 4136, 660, 4137, 244, 4138, 90, 928, 1039, 1038, 1275, 2425, 2426, 490, 365, 2427, 62, 479, 1033, 748, 2, 13, 2252, 4139, 303, 4140, 479, 4141, 260, 3, 479, 802, 983, 928, 1039, 1038, 623, 90, 479, 1033, 4142, 260, 3, 2428, 4143, 4144, 1172, 4145, 1501, 1717, 1901, 273, 4146, 1038, 1716, 928, 1039, 1038, 1716, 772, 4147, 4148, 1275, 2425, 2426, 490, 365, 2427, 1037, 462, 396, 1636, 490, 478, 490, 1085, 490, 4149, 4150, 144, 2, 526, 4151, 4152, 2428, 4153, 4154, 4155, 1172, 34, 4156, 4157, 297, 136, 1582, 1037, 13, 1653, 372, 331, 1274, 237, 151, 1127, 2, 62, 516, 4158, 13, 1653, 372, 331, 1274, 237, 151, 1534, 4159, 337, 241, 516, 4160, 1486, 2, 1037, 4161, 182, 1717, 2234, 4162, 8, 1040, 246, 1111, 4163, 121, 4164, 2429, 1578, 4, 167, 191, 4165, 19, 20, 17, 24], [1718, 2430, 434, 4166, 9, 43, 777, 1719, 5, 4, 309, 1028, 4167, 4168, 22, 408, 496, 4169, 4170, 22, 2431, 650, 2, 39, 1, 4171, 22, 31, 22, 4172, 317, 1719, 4173, 4174, 2, 3, 22, 170, 1110, 4175, 1718, 2430, 1720, 1720, 846, 4176, 113, 2219, 4177, 376, 2, 133, 1720, 1718, 1721, 4178, 4179, 609, 998, 4180, 4181, 3, 4182, 1447, 23, 22, 1248, 434, 1719, 176, 370, 499, 434, 1215, 1041, 1042, 4183, 4184, 78, 760, 4185, 2432, 4186, 1722, 434, 1963, 1454, 1179, 653, 4187, 1938, 403, 274, 887, 1, 22, 2433, 4188, 1161, 4189, 170, 4190, 499, 1893, 94, 4191, 266, 4192, 403, 640, 3, 133, 4193, 22, 1701, 2434, 2435, 660, 161, 2435, 1704, 90, 4194, 170, 4195, 1391, 1042, 1002, 4196, 640, 3, 639, 1290, 2, 4197, 4198, 4199, 1954, 90, 100, 188, 4200, 1721, 499, 1273, 4201, 403, 1290, 2, 2381, 309, 826, 309, 67, 124, 696, 1, 140, 1029, 1292, 278, 1693, 973, 309, 19, 20, 6, 17, 24], [1108, 700, 701, 4202, 5, 4, 1300, 1723, 583, 811, 99, 1012, 96, 2436, 1043, 251, 1301, 295, 1302, 8, 701, 107, 776, 4203, 847, 186, 1303, 776, 1121, 4204, 2, 77, 959, 701, 1304, 700, 847, 1, 4205, 1108, 700, 847, 186, 1, 701, 229, 276, 700, 607, 376, 100, 3, 1012, 701, 56, 341, 2437, 583, 249, 847, 222, 4206, 1, 2067, 4207, 99, 1012, 96, 2436, 49, 755, 99, 700, 1724, 1136, 279, 1104, 1725, 384, 251, 4208, 2438, 2439, 384, 11, 4209, 522, 18, 2440, 66, 333, 72, 1300, 467, 4210, 4211, 584, 1, 18, 2441, 4212, 136, 3, 384, 242, 98, 776, 522, 1437, 2, 11, 359, 1044, 1301, 34, 585, 123, 4213, 2442, 6, 174, 814, 2433, 4214, 4215, 347, 4216, 2, 415, 701, 247, 1254, 847, 4217, 1091, 1428, 427, 222, 776, 2443, 4218, 2036, 12, 775, 4219, 4220, 476, 4221, 4222, 955, 1121, 159, 538, 968, 1043, 251, 1301, 6, 1044, 261, 1163, 2068, 414, 2, 222, 538, 1, 159, 1726, 473, 1305, 3, 222, 1044, 261, 10, 522, 500, 1, 1045, 1043, 776, 2442, 76, 414, 10, 1044, 261, 4223, 2, 1305, 1727, 983, 3, 701, 293, 23, 847, 1, 47, 202, 958, 1728, 1043, 945, 324, 1, 56, 11, 867, 4224, 2, 14, 3, 22, 1699, 2444, 1729, 4225, 73, 11, 251, 1301, 256, 48, 700, 96, 2217, 2445, 11, 2446, 147, 2168, 48, 700, 96, 14, 34, 11, 50, 384, 1495, 1043, 4226, 550, 136, 49, 1302, 8, 57, 215, 30, 15, 301, 125, 57, 119, 142, 302, 19, 20, 6, 17, 24], [2447, 1046, 888, 1306, 1007, 1031, 5, 4, 1307, 8, 4227, 64, 2050, 110, 4228, 808, 1046, 1730, 702, 779, 635, 4229, 1731, 122, 1, 4230, 1007, 1731, 4231, 702, 1031, 435, 636, 1046, 1730, 702, 492, 359, 222, 132, 1046, 1730, 702, 4232, 2448, 4233, 4234, 1882, 1241, 1, 1046, 84, 1258, 504, 4235, 702, 601, 53, 2447, 79, 1731, 1732, 84, 70, 586, 209, 23, 84, 83, 678, 192, 1, 399, 702, 4236, 608, 2, 702, 258, 680, 83, 996, 192, 132, 1307, 8, 57, 215, 30, 15, 301, 125, 57, 119, 142, 302, 19, 20, 6, 17, 24], [1308, 587, 1733, 174, 5, 4, 1047, 193, 959, 262, 1307, 8, 2449, 2120, 369, 4237, 84, 41, 587, 1733, 174, 144, 587, 174, 587, 1047, 524, 4238, 1309, 1308, 1047, 524, 834, 1733, 174, 10, 511, 553, 64, 4239, 587, 126, 2450, 868, 174, 186, 256, 1308, 63, 1734, 511, 26, 464, 45, 756, 587, 45, 1934, 257, 1734, 1044, 935, 47, 1047, 524, 107, 587, 249, 4240, 4241, 4242, 330, 152, 4243, 1308, 583, 2449, 1047, 524, 576, 1423, 74, 45, 4244, 4245, 78, 347, 49, 1307, 8, 57, 215, 30, 15, 301, 125, 57, 119, 142, 302, 19, 20, 6, 17, 24], [203, 4246, 2451, 1735, 1219, 5, 4, 4247, 4248, 4249, 65, 286, 84, 1735, 1219, 70, 930, 4250, 262, 186, 2452, 2143, 264, 2451, 1735, 1219, 1736, 2448, 1165, 1241, 626, 264, 1222, 4251, 925, 1736, 264, 1736, 264, 57, 215, 30, 15, 301, 125, 57, 119, 142, 302, 19, 20, 6, 17, 24], [158, 4252, 84, 85, 67, 669, 85, 84, 584, 40, 5, 4, 158, 130, 251, 36, 1, 93, 54, 67, 669, 4253, 584, 118, 40, 21, 1, 158, 144, 1, 585, 1152, 26, 251, 169, 36, 1, 803, 25, 84, 1737, 88, 848, 1, 1048, 4254, 1310, 1738, 485, 116, 18, 1217, 669, 375, 525, 11, 436, 504, 93, 1, 14, 18, 703, 36, 1, 169, 84, 88, 176, 224, 1095, 619, 96, 56, 848, 116, 1739, 1, 1311, 34, 256, 2453, 158, 169, 36, 1, 25, 849, 160, 1312, 2454, 2454, 87, 445, 84, 2455, 59, 18, 1049, 118, 40, 21, 1, 7, 25, 93, 604, 1740, 40, 680, 1741, 544, 1313, 704, 850, 1742, 850, 21, 371, 25, 65, 1743, 1123, 584, 21, 2, 344, 4255, 158, 812, 25, 422, 636, 10, 447, 2456, 513, 1, 44, 109, 399, 25, 1034, 1744, 347, 851, 49, 4256, 8], [705, 9, 1050, 9, 1314, 621, 511, 5, 4, 2457, 2458, 1745, 8, 705, 244, 1520, 811, 99, 101, 196, 344, 705, 9, 1050, 9, 1314, 41, 21, 705, 9, 846, 1050, 9, 846, 1314, 621, 511, 53, 38, 9, 846, 1050, 9, 846, 341, 4257, 705, 87, 2459, 1746, 339, 706, 2405, 1747, 852, 1306, 279, 1, 852, 264, 152, 4258, 233, 814, 2, 1748, 1013, 4259, 4260, 4261, 4262, 339, 4263, 836, 1306, 168, 924, 4264, 2408, 26, 705, 1749, 1226, 4265, 1, 168, 4266, 693, 1, 132, 262, 15, 9, 846, 1050, 264, 1747, 852, 852, 9, 1314, 264, 1747, 852, 852, 244, 705, 1520, 26, 1113, 16, 3, 4267, 1214, 4268, 139, 613, 882, 14, 307, 16, 4269, 1306, 2458, 1745, 8, 191, 1051, 136, 2460, 4, 2457, 19, 20, 6, 17, 24], [4270, 149, 1315, 2123, 2461, 84, 1750, 429, 1316, 341, 428, 5, 4, 309, 1751, 323, 203, 812, 4271, 793, 1633, 1315, 1316, 341, 428, 498, 40, 152, 3, 1316, 341, 428, 4272, 512, 138, 722, 518, 1750, 1408, 405, 1315, 1082, 1222, 2461, 84, 1750, 2462, 429, 1317, 707, 4273, 1678, 40, 1633, 1315, 771, 51, 203, 1316, 341, 428, 40, 300, 41, 543, 53, 3, 41, 21, 309, 826, 309, 67, 124, 696, 1, 140, 1029, 1292, 278, 1693, 973, 309, 19, 20, 6, 17, 24], [158, 93, 36, 54, 346, 4274, 1709, 4275, 159, 144, 5, 4, 570, 374, 571, 158, 129, 25, 118, 40, 4276, 2380, 41, 158, 129, 4277, 1752, 158, 130, 93, 36, 1029, 54, 669, 585, 38, 371, 25, 118, 40, 379, 144, 158, 7, 222, 129, 26, 126, 1737, 88, 1, 18, 1048, 1310, 1738, 4278, 1, 116, 1217, 2141, 93, 36, 1029, 18, 67, 585, 144, 222, 200, 4279, 4280, 849, 160, 1312, 25, 59, 346, 40, 379, 757, 169, 36, 2, 2463, 264, 200, 2464, 849, 25, 2307, 458, 40, 261, 379, 38, 133, 251, 169, 36, 25, 1753, 880, 116, 2465, 1311, 25, 422, 636, 31, 21, 3, 195, 670, 851, 797, 2373, 158, 251, 169, 36, 1, 1467, 1181, 201, 53, 275, 408, 708, 6, 541, 236, 920, 496, 853, 108, 868, 408, 708, 108, 490, 675, 6, 11, 2466, 675, 877, 246, 236, 4281, 1728, 275, 408, 708, 157, 1754, 704, 2467, 56, 93, 36, 784, 2468, 687, 449, 325, 2, 222, 132, 1754, 704, 2469, 284, 4282, 2470, 4283, 48, 275, 4284, 161, 4285, 1452, 10, 2469, 4286, 704, 400, 130, 11, 436, 2471, 256, 18, 284, 2453, 11, 436, 504, 275, 703, 36, 1, 93, 12, 107, 176, 224, 61, 3, 169, 1, 268, 1299, 968, 322, 158, 703, 36, 1, 169, 126, 88, 176, 6, 1748, 224, 1095, 619, 96, 1555, 275, 703, 36, 1, 93, 848, 116, 1739, 1, 1305, 1755, 1756, 2472, 949, 1125, 2473, 989, 1, 1045, 853, 63, 85, 54, 126, 88, 1455, 10, 93, 1757, 36, 884, 39, 1, 678, 588, 1264, 905, 4287, 222, 284, 1128, 1298, 804, 43, 268, 93, 1729, 1318, 275, 1757, 36, 884, 39, 1, 678, 588, 83, 588, 1188, 1264, 1, 96, 2474, 2475, 1758, 2476, 268, 275, 2477, 43, 34, 169, 126, 34, 126, 1318, 1662, 2478, 2479, 730, 318, 1, 96, 687, 34, 619, 96, 11, 436, 504, 1319, 500, 265, 1242, 132, 158, 144, 1, 67, 585, 185, 323, 853, 1296, 25, 150, 2480, 606, 1877, 1296, 25, 150, 971, 485, 158, 251, 169, 36, 1, 803, 84, 1737, 88, 848, 1, 1048, 563, 1105, 25, 1310, 1738, 485, 116, 1217, 669, 485, 2481, 93, 36, 4288, 293, 1220, 1759, 585, 1218, 485, 396, 25, 11, 436, 504, 1546, 1319, 500, 142, 9, 1628, 53, 1759, 25, 4289, 4290, 10, 11, 436, 504, 1319, 500, 265, 1628, 236, 750, 4291, 1242, 670, 347, 851, 853, 1729, 629, 399, 88, 398, 10, 84, 1318, 275, 284, 1128, 1298, 804, 43, 268, 93, 1757, 36, 884, 39, 1, 678, 588, 83, 588, 1188, 1264, 1, 96, 2474, 2475, 1758, 2476, 268, 275, 2477, 43, 34, 169, 84, 34, 84, 1318, 1662, 2478, 2479, 730, 318, 1, 96, 687, 34, 619, 96, 11, 436, 504, 1319, 500, 265, 1242, 1, 3, 4292, 513, 4293, 275, 703, 36, 1, 93, 12, 107, 176, 224, 61, 3, 1310, 2482, 1406, 169, 1, 268, 1299, 158, 703, 36, 1, 169, 84, 88, 176, 6, 1748, 224, 1095, 65, 98, 619, 96, 1555, 275, 703, 36, 1, 93, 848, 116, 1739, 1, 1, 4294, 11, 436, 2471, 63, 256, 1755, 1756, 2472, 34, 1125, 2473, 989, 1, 4295, 1311, 34, 256, 9, 366, 158, 251, 169, 36, 1, 1467, 201, 10, 275, 408, 708, 6, 541, 236, 920, 496, 853, 108, 868, 408, 708, 108, 490, 675, 6, 11, 2466, 675, 877, 246, 236, 18, 201, 53, 242, 201, 1320, 275, 408, 708, 157, 1754, 704, 2467, 56, 93, 36, 784, 2468, 687, 325, 2481, 36, 2, 93, 848, 735, 25, 150, 1048, 2483, 880, 1519, 565, 4296, 251, 169, 36, 403, 853, 251, 169, 36, 25, 1753, 880, 116, 2465, 1311, 25, 422, 636, 31, 1, 3, 195, 670, 851, 242, 201, 1, 1320, 757, 169, 36, 2, 2463, 264, 6, 2464, 1118, 2484, 25, 163, 4297, 1, 447, 515, 2484, 116, 4298, 4299, 322, 158, 63, 251, 36, 1, 93, 25, 150, 1048, 1753, 880, 18, 1566, 447, 210, 25, 849, 160, 1312, 25, 261, 59, 25, 59, 1049, 371, 25, 118, 40, 293, 1220, 379, 293, 25, 118, 40, 122, 264, 122, 25, 1052, 849, 1, 84, 1118, 1, 25, 371, 25, 118, 40, 240, 59, 1049, 209, 93, 604, 6, 1740, 40, 680, 1741, 544, 1313, 704, 850, 1742, 850, 21, 371, 25, 65, 1743, 1123, 584, 25, 118, 40, 122, 25, 1052, 849, 1, 84, 1118, 1, 25, 371, 25, 118, 40, 240, 59, 1049, 209, 93, 604, 6, 1740, 40, 680, 1741, 4300, 544, 1313, 704, 850, 1742, 850, 21, 371, 25, 65, 1743, 1123, 584, 1321, 1, 240, 6, 142, 278, 1759, 129, 4301, 1, 25, 1034, 4302, 1089, 1134, 158, 195, 25, 422, 636, 10, 447, 2456, 44, 109, 399, 25, 1034, 1744, 10, 347, 851, 735, 158, 25, 465, 1549, 965, 1251, 4303, 662, 222, 267, 1170, 971, 812, 695, 839, 1665, 4304, 8, 191, 137, 572, 46, 140, 4, 19, 20, 6, 17, 24, 242, 46, 44, 177, 14, 139, 167, 573, 574], [557, 423, 11, 314, 47, 538, 5, 4, 57, 1760, 1761, 1762, 1196, 232, 1304, 1763, 4305, 1053, 111, 74, 444, 583, 505, 204, 47, 38, 606, 79, 414, 9, 293, 2485, 2486, 2487, 8, 108, 1725, 384, 11, 50, 11, 505, 4306, 54, 1, 435, 780, 746, 1, 18, 1762, 1196, 232, 1304, 1763, 521, 41, 11, 557, 423, 505, 204, 6, 314, 47, 295, 538, 166, 414, 3, 49, 1764, 521, 355, 4307, 4308, 4309, 13, 2488, 249, 57, 1760, 1761, 1303, 2489, 1053, 111, 74, 55, 444, 583, 1765, 505, 2490, 780, 49, 3, 7, 38, 2446, 4310, 2445, 147, 50, 359, 4311, 79, 561, 1, 300, 437, 1088, 4312, 513, 1764, 521, 1053, 111, 583, 4313, 1983, 1765, 50, 1865, 1322, 1, 3, 74, 203, 11, 64, 55, 48, 557, 423, 4314, 4315, 616, 981, 316, 116, 1299, 49, 2491, 557, 423, 505, 204, 47, 314, 47, 295, 538, 1208, 423, 47, 962, 64, 50, 166, 4316, 4317, 3, 797, 1054, 232, 522, 500, 18, 2492, 4318, 2438, 4319, 1212, 4320, 1, 1041, 14, 3, 1548, 101, 2492, 1206, 603, 1766, 2493, 2342, 647, 3, 1116, 232, 603, 1, 9, 443, 4321, 114, 1415, 85, 4322, 48, 114, 4323, 242, 603, 398, 427, 797, 1764, 521, 2287, 2255, 2443, 561, 4324, 4325, 18, 203, 2440, 54, 929, 775, 2, 145, 2494, 1170, 427, 116, 3, 929, 4326, 38, 2485, 2486, 2487, 8, 57, 215, 30, 15, 301, 125, 57, 119, 142, 302, 19, 20, 6, 17, 24], [11, 50, 423, 1323, 1163, 34, 1051, 2495, 23, 108, 4327, 5, 4, 1300, 1723, 1503, 178, 50, 2496, 2497, 867, 126, 128, 174, 1767, 4328, 1324, 1325, 316, 1768, 2498, 227, 108, 1326, 1327, 147, 2499, 4329, 2500, 2501, 435, 958, 50, 2078, 319, 231, 79, 1769, 2502, 2503, 1302, 8, 108, 384, 11, 50, 11, 18, 231, 79, 561, 1, 181, 1115, 1235, 4330, 4331, 352, 3, 2437, 1152, 8, 11, 50, 108, 1725, 885, 2417, 1, 4332, 1092, 3, 2452, 4333, 4334, 949, 11, 18, 231, 79, 2504, 601, 522, 500, 545, 76, 4335, 214, 50, 437, 2496, 2497, 1015, 4336, 231, 79, 352, 34, 1208, 2505, 872, 325, 889, 435, 2169, 65, 756, 11, 50, 437, 4337, 733, 3, 889, 435, 18, 737, 2494, 295, 2506, 47, 1762, 1196, 232, 1304, 1763, 521, 13, 2488, 249, 57, 1760, 1761, 1303, 2489, 1053, 111, 74, 55, 444, 583, 1765, 505, 2490, 780, 1208, 11, 18, 423, 314, 6, 505, 204, 47, 4338, 38, 9, 7, 962, 64, 166, 414, 1, 797, 11, 18, 231, 79, 414, 1191, 2251, 2507, 204, 47, 76, 295, 2506, 415, 384, 11, 4339, 1300, 1723, 4340, 1567, 295, 4341, 316, 3, 1127, 2022, 626, 70, 401, 4342, 384, 275, 4343, 70, 2508, 1637, 11, 92, 568, 126, 4344, 709, 31, 76, 4345, 16, 74, 45, 895, 1567, 384, 11, 4346, 1770, 1554, 1, 651, 710, 2, 3, 11, 242, 384, 931, 2508, 1770, 2434, 1051, 1417, 423, 1323, 989, 180, 2, 11, 50, 437, 231, 79, 969, 11, 423, 1323, 989, 180, 2, 3, 4347, 931, 371, 43, 359, 1734, 257, 251, 11, 2439, 4348, 4349, 678, 126, 867, 126, 1423, 174, 1767, 155, 1088, 4350, 18, 2509, 161, 1141, 2510, 4351, 1127, 90, 11, 384, 1417, 2511, 1049, 966, 1324, 1325, 1, 2507, 204, 735, 1770, 2306, 4352, 1045, 1324, 1325, 26, 1327, 147, 898, 4353, 4354, 2509, 161, 1052, 231, 819, 561, 350, 273, 2, 50, 4355, 233, 4356, 16, 2512, 4357, 1051, 2511, 2020, 1324, 1325, 1, 1052, 231, 908, 1667, 296, 437, 1327, 147, 4358, 29, 279, 202, 233, 553, 1088, 511, 72, 4359, 2, 437, 118, 1547, 4360, 4361, 34, 231, 819, 60, 505, 1378, 256, 18, 4362, 166, 16, 49, 2513, 805, 2514, 2515, 2502, 11, 4363, 2516, 50, 437, 4364, 3, 4365, 79, 435, 912, 155, 1483, 1327, 147, 2499, 7, 60, 437, 1771, 296, 1051, 1483, 877, 642, 437, 516, 4366, 11, 18, 2500, 2501, 328, 314, 62, 679, 76, 26, 2032, 4367, 435, 3, 1303, 420, 130, 166, 4368, 1, 1053, 111, 583, 257, 251, 174, 6, 262, 47, 2517, 1136, 1912, 4369, 1194, 2420, 1768, 1, 375, 202, 64, 589, 854, 1768, 2498, 227, 352, 14, 3, 920, 496, 6, 4370, 2044, 1150, 778, 904, 11, 1152, 2518, 4371, 11, 50, 555, 4372, 66, 116, 138, 3, 318, 4373, 4374, 231, 79, 352, 951, 319, 2505, 4375, 1, 203, 4376, 2075, 1198, 651, 2519, 958, 11, 50, 231, 79, 2504, 1769, 256, 316, 34, 627, 407, 4377, 232, 1167, 95, 2520, 2513, 805, 2514, 297, 2515, 159, 2, 1769, 394, 2521, 467, 316, 34, 279, 22, 1699, 2444, 73, 11, 18, 437, 1924, 1944, 4378, 76, 905, 145, 1378, 256, 1, 2116, 1, 2503, 1302, 8, 57, 215, 30, 15, 301, 125, 57, 119, 142, 302, 19, 20, 6, 17, 24], [1328, 10, 86, 217, 218, 2493, 1055, 435, 710, 2, 5, 4, 342, 1326, 1772, 8, 1773, 64, 286, 86, 217, 218, 854, 976, 129, 86, 217, 218, 1055, 1056, 86, 217, 218, 590, 4379, 99, 1056, 710, 2, 711, 871, 10, 457, 2522, 364, 2023, 1774, 590, 4380, 712, 184, 664, 468, 2523, 1480, 710, 2, 1326, 1772, 936, 410, 8, 293, 959, 4381, 4382, 286, 771, 86, 217, 218, 854, 86, 217, 218, 18, 2441, 4383, 1563, 2, 976, 129, 539, 638, 948, 86, 217, 218, 1055, 435, 1056, 86, 217, 218, 4384, 590, 147, 99, 1056, 710, 2, 86, 217, 218, 874, 2524, 4385, 18, 4386, 2525, 2526, 4387, 4388, 4389, 2527, 1055, 7, 1045, 590, 217, 218, 1204, 186, 2, 86, 217, 218, 4390, 1329, 1330, 2524, 1092, 2, 319, 4391, 1506, 2528, 2529, 2, 4392, 401, 4393, 712, 171, 1980, 137, 1166, 712, 1330, 171, 2525, 16, 4394, 137, 240, 1166, 667, 1331, 26, 474, 249, 1775, 1776, 1057, 506, 10, 217, 218, 331, 4395, 2, 4396, 4397, 2530, 2529, 2, 1, 832, 4398, 1775, 1776, 1057, 506, 4399, 4400, 2, 1332, 113, 611, 396, 923, 4401, 1, 913, 4402, 4403, 4404, 500, 1320, 331, 1516, 970, 455, 77, 667, 1331, 1775, 4405, 10, 951, 784, 1776, 1057, 506, 4406, 604, 195, 29, 61, 18, 971, 457, 7, 1203, 482, 258, 947, 4407, 4408, 18, 941, 530, 109, 4409, 18, 651, 4410, 970, 507, 4411, 73, 4412, 1167, 4413, 801, 1472, 1, 1329, 18, 4414, 2530, 2016, 552, 18, 945, 1515, 4415, 50, 4416, 2019, 2491, 4417, 54, 2, 396, 1331, 1329, 4418, 1997, 1179, 553, 133, 400, 50, 585, 4419, 4420, 4421, 887, 1, 1724, 1203, 258, 86, 217, 218, 2531, 871, 10, 457, 585, 38, 712, 346, 2035, 4422, 471, 525, 664, 4423, 1177, 123, 590, 4424, 258, 86, 217, 218, 4425, 4426, 2523, 651, 4427, 710, 2, 1777, 123, 590, 455, 2532, 590, 4428, 1506, 1333, 875, 90, 651, 16, 276, 86, 217, 218, 854, 712, 1055, 435, 1056, 710, 2, 7, 98, 1677, 1239, 1543, 4429, 4430, 4431, 50, 267, 1305, 1331, 2532, 135, 116, 712, 4432, 1774, 18, 941, 4433, 1433, 136, 507, 4434, 73, 4435, 4436, 4437, 242, 661, 14, 4438, 806, 4439, 4440, 1, 550, 136, 195, 552, 43, 552, 281, 270, 1184, 1984, 46, 34, 34, 4441, 642, 86, 217, 218, 2211, 1033, 274, 1538, 14, 1334, 871, 897, 2292, 10, 457, 2522, 364, 590, 997, 1412, 4442, 4443, 407, 4444, 178, 662, 1015, 71, 712, 184, 4445, 664, 468, 2533, 4446, 4447, 2510, 1433, 4, 1326, 1772, 28, 27, 834, 1335, 960, 14, 342, 8, 1778, 1779, 1, 342, 1780, 191, 342, 19, 20, 6, 17, 24], [77, 356, 759, 1781, 821, 79, 556, 1418, 55, 135, 5, 4, 13, 141, 77, 356, 759, 1782, 1783, 1784, 1781, 821, 79, 556, 778, 391, 55, 135, 2, 13, 141, 4448, 141, 179, 77, 356, 759, 1782, 1783, 1784, 1781, 821, 79, 556, 778, 391, 55, 135, 2, 556, 778, 391, 2423, 62, 1570, 323, 1113, 1785, 502, 79, 767, 907, 2, 738, 2534, 40, 21, 50, 79, 55, 322, 56, 77, 356, 759, 64, 55, 53, 2, 931, 164, 4449, 50, 79, 556, 4450, 556, 778, 391, 55, 742, 2, 55, 135, 56, 77, 356, 759, 2535, 4451, 1715, 4452, 616, 4453, 1482, 1392, 736, 62, 1786, 62, 897, 126, 4454, 1786, 556, 55, 1787, 1744, 47, 322, 1783, 1782, 323, 1113, 4455, 4456, 2536, 6, 1785, 502, 54, 767, 55, 155, 742, 2, 971, 195, 4457, 2537, 1570, 10, 2536, 6, 1785, 502, 907, 2, 2534, 40, 21, 670, 347, 2001, 49, 141, 1621, 1622, 1623, 30, 15, 141, 198, 124, 740, 191, 1000, 1624, 141, 1625, 19, 20, 17, 24], [4458, 771, 289, 855, 4459, 5, 4, 771, 4460, 652, 86, 51, 289, 658, 234, 4461, 2538, 391, 28, 206, 27, 27, 2539, 369, 2540, 4462, 430, 1788, 1, 3, 4463, 145, 181, 4464, 289, 22, 289, 4465, 4466, 289, 1058, 4467, 4468, 4469, 591, 4470, 27, 2539, 369, 2540, 21, 4471, 8, 1336, 1337, 129, 30, 15], [1789, 1790, 2541, 2542, 106, 5, 4, 342, 4472, 8, 1790, 1789, 591, 107, 4473, 2543, 4474, 2541, 1, 1789, 591, 2544, 4475, 694, 4476, 4477, 4478, 4479, 2545, 2071, 145, 873, 4480, 2, 4481, 786, 1041, 427, 4482, 4483, 1791, 2544, 4484, 1330, 4485, 4486, 331, 4487, 1258, 247, 1254, 4488, 2, 190, 115, 1203, 482, 1466, 1791, 4489, 50, 455, 331, 4490, 4491, 2021, 4492, 4493, 56, 4494, 135, 2, 1, 258, 372, 4495, 1790, 1115, 4496, 4497, 10, 4498, 1791, 4499, 372, 4500, 4501, 873, 4502, 180, 2, 2542, 106, 485, 145, 331, 4503, 4504, 2, 28, 27, 834, 1335, 960, 14, 342, 8, 1778, 1779, 1, 342, 1780, 191, 342, 19, 20, 6, 17, 24], [311, 252, 219, 385, 589, 321, 856, 471, 5, 4, 792, 252, 311, 404, 1338, 311, 252, 219, 385, 857, 1059, 1339, 589, 855, 10, 267, 2546, 219, 321, 856, 471, 252, 311, 404, 1792, 2547, 2548, 1338, 184, 219, 385, 78, 589, 219, 321, 856, 471, 38, 252, 311, 404, 21, 589, 854, 311, 252, 219, 385, 219, 321, 856, 348, 614, 352, 1, 244, 252, 4505, 311, 62, 404, 1792, 2547, 2548, 1338, 184, 219, 385, 589, 348, 614, 10, 1059, 1339, 2549, 115, 2550, 10, 348, 913, 348, 1552, 370, 1, 295, 1059, 1339, 1376, 1158, 135, 10, 252, 219, 385, 267, 2546, 2517, 2551, 1, 38, 420, 394, 589, 855, 10, 252, 219, 385, 219, 321, 856, 414, 471, 1, 219, 321, 856, 252, 4506, 147, 601, 18, 1000, 4507, 780, 4508, 414, 26, 4509, 1, 132, 252, 311, 404, 1191, 107, 62, 404, 2431, 2552, 2553, 2554, 2555, 1793, 385, 1792, 4510, 857, 184, 857, 2556, 252, 62, 2557, 4511, 2552, 1340, 506, 4512, 2553, 1340, 123, 348, 506, 2357, 4513, 2554, 1340, 506, 4514, 2555, 1340, 506, 694, 7, 1418, 252, 311, 404, 1793, 219, 385, 662, 4515, 1777, 1329, 4516, 4517, 1516, 4518, 2, 4519, 4520, 1777, 4521, 2079, 1332, 1184, 4522, 4523, 350, 311, 4524, 4525, 1059, 1774, 1002, 1332, 1184, 229, 4526, 3, 219, 1332, 920, 26, 1689, 614, 415, 407, 4527, 219, 385, 4528, 147, 4529, 922, 4530, 267, 1794, 589, 321, 1794, 2558, 1659, 1794, 370, 1, 2556, 252, 311, 2557, 1338, 2323, 4531, 1147, 855, 311, 62, 1059, 1339, 2549, 115, 2550, 4532, 1957, 252, 219, 385, 1689, 4533, 348, 913, 370, 311, 4534, 822, 422, 636, 670, 851, 2559, 4535, 1150, 8, 792, 1557, 2, 1558, 119, 46, 910, 241, 1559, 1560, 1197, 1561, 19, 20, 6, 17, 24], [2560, 1611, 1060, 840, 243, 10, 5, 4, 342, 7, 1795, 2561, 1773, 276, 4536, 213, 4537, 1283, 612, 1796, 4538, 4539, 4540, 592, 52, 22, 73, 117, 4541, 1531, 1797, 243, 112, 2562, 955, 1771, 474, 52, 7, 1795, 2561, 532, 4542, 1060, 840, 243, 10, 1796, 1341, 4543, 1342, 2130, 1398, 4544, 1, 4545, 1343, 113, 241, 1175, 11, 1283, 2563, 1796, 73, 270, 1344, 2564, 2562, 112, 1, 52, 592, 508, 270, 1344, 1797, 223, 865, 353, 164, 2280, 966, 1003, 4546, 816, 4547, 18, 4548, 4549, 2055, 4550, 52, 270, 1797, 1344, 171, 4551, 307, 2565, 4552, 1, 4553, 243, 416, 508, 26, 1344, 1060, 840, 1211, 121, 133, 508, 390, 664, 941, 887, 132, 661, 181, 664, 2566, 1141, 279, 4554, 1061, 4555, 1798, 2567, 2568, 1341, 445, 1799, 1382, 4556, 1218, 1342, 667, 2569, 37, 1798, 2567, 2568, 591, 2560, 508, 2483, 90, 4557, 7, 326, 1334, 4558, 1061, 34, 46, 798, 159, 1061, 46, 99, 243, 246, 4559, 2570, 4560, 4561, 52, 171, 1060, 1898, 713, 171, 1042, 4562, 171, 1061, 1767, 208, 1060, 390, 4563, 10, 1793, 865, 4564, 2571, 1218, 1406, 538, 133, 228, 4565, 1592, 270, 1240, 1061, 1518, 78, 1927, 4566, 4567, 4568, 1334, 737, 4569, 71, 1800, 3, 1611, 1062, 706, 2572, 1, 1345, 937, 1346, 1063, 1345, 4570, 1341, 4571, 1799, 1617, 4572, 1342, 101, 829, 1345, 937, 1346, 1063, 1062, 706, 2572, 1, 52, 4573, 1345, 937, 1346, 1063, 1062, 522, 648, 1661, 706, 4574, 937, 1346, 1063, 14, 1726, 1553, 4575, 3, 52, 1592, 1062, 1012, 1101, 1450, 3, 912, 4576, 623, 1062, 522, 4577, 1, 1063, 1500, 4578, 208, 4579, 4580, 76, 78, 538, 4, 7, 1795, 654, 972, 4581, 1801, 654, 4582, 4583, 28, 27, 834, 1335, 960, 14, 342, 8, 1778, 1779, 1, 342, 1780, 191, 342, 19, 20, 6, 17, 24], [354, 343, 711, 343, 303, 107, 2573, 286, 5, 4, 354, 664, 4584, 1584, 1, 354, 343, 711, 343, 303, 107, 2573, 286, 354, 343, 711, 4585, 591, 4586, 2574, 355, 183, 711, 1787, 391, 1787, 954, 106, 610, 343, 303, 53, 343, 303, 923, 4587, 4588, 4589, 644, 4590, 181, 184, 713, 106, 2575, 1802, 2576, 2577, 2578, 2579, 4591, 2518, 713, 2580, 4592, 1347, 828, 12, 885, 53, 2, 343, 303, 923, 837, 2, 2581, 713, 179, 713, 4593, 1565, 2581, 713, 179, 4594, 447, 46, 1803, 241, 563, 4595, 4596, 4597, 2582, 2583, 731, 591, 1348, 106, 939, 1605, 106, 26, 171, 332, 190, 4598, 450, 2, 190, 106, 2584, 1665, 4599, 106, 485, 171, 4600, 106, 485, 171, 4601, 395, 171, 4602, 61, 279, 133, 190, 106, 2585, 2586, 3, 1348, 106, 3, 106, 1804, 749, 587, 1272, 16, 661, 56, 798, 4603, 1803, 749, 1114, 2575, 106, 2587, 495, 77, 2588, 2589, 695, 2590, 1805, 2591, 2592, 4604, 2593, 897, 2594, 2595, 2596, 2597, 483, 343, 303, 2179, 16, 446, 171, 4605, 100, 1806, 1, 106, 1807, 236, 263, 672, 1, 711, 1806, 1, 106, 923, 106, 4606, 4607, 1, 2, 100, 290, 1959, 4608, 270, 981, 171, 4609, 290, 711, 2112, 64, 290, 290, 2598, 290, 290, 2598, 290, 10, 457, 4610, 7, 290, 343, 303, 694, 1, 145, 1806, 1, 106, 1807, 240, 210, 1575, 672, 446, 1348, 106, 4611, 409, 2586, 1036, 4612, 4613, 672, 2599, 4614, 2564, 1, 446, 1348, 106, 4615, 2559, 1, 2576, 22, 1058, 1349, 495, 4616, 495, 695, 2590, 2597, 2591, 2595, 68, 4617, 106, 4618, 647, 152, 2577, 2594, 495, 4619, 4620, 4621, 2587, 4622, 562, 182, 1040, 4623, 4624, 23, 4625, 2018, 23, 803, 2585, 166, 23, 4626, 906, 240, 263, 2600, 495, 2601, 2602, 2603, 2604, 1805, 2605, 2606, 2607, 1802, 171, 799, 648, 1273, 1802, 1507, 106, 977, 563, 171, 4627, 447, 648, 2600, 2601, 2602, 2603, 2604, 1805, 4628, 2606, 2607, 4629, 44, 4630, 121, 977, 4631, 181, 2578, 152, 2596, 495, 77, 2588, 2589, 4632, 4633, 2592, 4634, 2593, 800, 713, 1104, 4635, 2579, 2608, 22, 1058, 1349, 495, 2609, 495, 4636, 2605, 4637, 4638, 646, 4639, 246, 4640, 562, 182, 4641, 563, 4642, 4643, 22, 1058, 1349, 354, 1808, 4644, 2608, 2609, 1085, 1808, 477, 343, 303, 12, 4645, 106, 805, 68, 243, 106, 241, 1501, 2582, 2583, 731, 591, 4646, 2580, 1136, 2, 2574, 22, 1058, 1349, 793, 354, 1808, 343, 303, 4647, 415, 653, 1745, 2, 12, 694, 1, 246, 378, 2610, 106, 1211, 2611, 49, 4648, 4649, 22, 2612, 8, 1336, 1337, 129, 30, 15], [2613, 271, 2614, 1804, 159, 171, 1800, 2615, 2, 10, 5, 4, 2614, 1804, 159, 171, 1800, 2615, 2, 10, 2616, 798, 159, 1, 1086, 4650, 2613, 1336, 1337, 129, 30, 15], [955, 457, 5, 4, 2617, 2618, 592, 523, 1583, 4651, 858, 2619, 2620, 1350, 1809, 4652, 1334, 1351, 784, 798, 4653, 4654, 4655, 390, 1891, 4656, 91, 64, 186, 2, 276, 2617, 854, 7, 784, 4657, 415, 653, 457, 4658, 1213, 2618, 474, 4659, 2621, 858, 111, 297, 2619, 294, 2620, 794, 1350, 554, 263, 1351, 409, 492, 307, 2622, 2623, 523, 858, 695, 37, 68, 2624, 533, 4660, 4661, 350, 3, 63, 1351, 4662, 4663, 695, 1809, 229, 1771, 263, 523, 858, 111, 450, 3, 23, 190, 208, 117, 68, 4664, 455, 4665, 73, 4666, 9, 9, 317, 858, 4667, 2, 4668, 687, 190, 2162, 1577, 26, 190, 4669, 4670, 1551, 4671, 332, 4672, 60, 190, 157, 1351, 409, 492, 307, 1390, 1350, 1809, 2584, 2570, 317, 2622, 2623, 2017, 634, 2625, 121, 117, 68, 3, 523, 858, 4673, 2565, 1333, 653, 117, 4674, 48, 4675, 1352, 4676, 1903, 4677, 23, 4678, 2624, 533, 2626, 4679, 409, 4680, 4681, 4682, 117, 4683, 525, 7, 457, 190, 229, 268, 121, 52, 1390, 1350, 554, 4684, 2621, 1669, 2081, 1274, 4685, 4686, 680, 637, 2, 4687, 8, 2627, 30, 15, 1064, 119, 142, 1, 1576, 1064, 19, 20, 6, 17, 24], [2279, 5, 4, 438, 4688, 727, 438, 406, 4689, 211, 406, 10, 2528, 278, 4690, 9, 236, 4691, 324, 2626, 97, 66, 438, 324, 48, 438, 406, 97, 438, 2628, 2629, 1477, 438, 438, 563, 1727, 1989, 2630, 1211, 211, 4692, 211, 661, 7, 4693, 7, 1727, 4694, 171, 727, 438, 406, 798, 2616, 1803, 211, 438, 727, 4695, 855, 4696, 190, 243, 181, 214, 2631, 970, 2629, 2631, 278, 4697, 406, 525, 171, 278, 23, 136, 727, 438, 406, 2610, 855, 4698, 4699, 2, 2611, 4700, 591, 4701, 4702, 1336, 1337, 129, 30, 15], [593, 594, 714, 1810, 23, 29, 69, 103, 184, 5, 4, 593, 992, 1811, 593, 21, 992, 1811, 103, 13, 4, 2632, 8, 2221, 4703, 593, 51, 4704, 4705, 29, 69, 213, 992, 1811, 2633, 178, 662, 103, 180, 38, 593, 64, 29, 69, 91, 366, 795, 593, 594, 584, 163, 715, 172, 494, 102, 87, 155, 586, 209, 16, 3, 7, 103, 91, 194, 2634, 715, 172, 117, 1266, 43, 102, 1, 25, 1732, 122, 64, 157, 192, 1, 593, 1568, 4706, 1227, 253, 344, 2635, 4707, 405, 246, 102, 25, 714, 1810, 714, 2636, 4708, 209, 21, 886, 43, 102, 1, 25, 181, 1347, 26, 707, 359, 584, 163, 64, 29, 645, 51, 1146, 4709, 1195, 4710, 4711, 310, 4712, 324, 39, 1, 4713, 253, 103, 362, 379, 4714, 43, 102, 160, 4715, 192, 594, 754, 52, 2214, 66, 4716, 341, 529, 1173, 307, 16, 3, 101, 194, 1491, 194, 4717, 194, 494, 43, 102, 87, 65, 2637, 2638, 1812, 582, 2639, 582, 1813, 245, 164, 1, 515, 2640, 253, 103, 53, 43, 102, 60, 2640, 324, 3, 65, 2637, 2638, 1812, 582, 859, 1812, 4718, 164, 837, 2, 3, 2639, 582, 4719, 4720, 783, 4721, 4722, 4723, 837, 1871, 316, 1, 577, 210, 1813, 245, 4724, 52, 4725, 4726, 1813, 281, 192, 1, 4727, 859, 789, 2, 593, 29, 69, 103, 18, 1321, 1, 780, 129, 26, 325, 3, 28, 54, 4, 149, 14, 5, 207, 4, 81, 162, 150, 33, 235, 33, 4, 110, 19, 20, 6, 17, 24], [595, 29, 69, 103, 155, 594, 253, 5, 4, 595, 194, 29, 69, 103, 595, 21, 2641, 1632, 315, 586, 209, 180, 13, 4, 2632, 8, 276, 660, 288, 645, 213, 595, 194, 2633, 178, 662, 103, 53, 344, 38, 595, 194, 51, 4728, 69, 91, 366, 373, 52, 2642, 102, 3, 811, 209, 543, 2216, 157, 192, 1, 594, 155, 253, 594, 2462, 87, 4729, 157, 192, 722, 157, 588, 405, 194, 4730, 102, 87, 4731, 61, 3, 811, 209, 103, 668, 1227, 1732, 795, 1683, 325, 87, 155, 4732, 4733, 2643, 147, 87, 194, 4734, 732, 682, 187, 87, 1052, 2644, 87, 4735, 677, 194, 682, 4736, 87, 164, 39, 1, 594, 4737, 16, 3, 595, 194, 2641, 1632, 315, 586, 209, 180, 2645, 774, 429, 87, 679, 194, 4738, 155, 586, 209, 21, 4739, 4740, 4741, 429, 87, 586, 209, 2645, 429, 87, 586, 209, 23, 4742, 315, 429, 25, 122, 586, 608, 2, 163, 595, 194, 64, 183, 1469, 177, 16, 551, 184, 582, 194, 102, 25, 1694, 39, 1, 307, 23, 103, 122, 194, 43, 102, 87, 2646, 2647, 1801, 43, 102, 87, 245, 699, 1801, 43, 102, 87, 4743, 1446, 253, 683, 707, 4744, 1, 7, 488, 90, 645, 4745, 660, 4746, 316, 405, 1648, 373, 2634, 103, 362, 53, 103, 668, 418, 1292, 781, 349, 276, 350, 736, 324, 102, 87, 157, 192, 1, 1313, 1227, 253, 7, 78, 268, 103, 668, 595, 194, 4747, 4748, 4749, 1, 605, 194, 532, 3, 2569, 997, 645, 69, 1288, 1065, 274, 254, 69, 91, 366, 373, 1, 140, 505, 4750, 39, 1, 765, 52, 605, 28, 54, 4, 149, 14, 5, 207, 4, 81, 162, 150, 33, 235, 33, 4, 110, 19, 20, 6, 17, 24], [1814, 1066, 14, 1815, 260, 978, 1816, 3, 5, 4, 792, 1353, 596, 73, 247, 254, 4751, 48, 1190, 37, 22, 799, 657, 1354, 934, 2648, 2649, 1817, 4752, 220, 239, 1818, 321, 1067, 1819, 1355, 2650, 2558, 4753, 2651, 4754, 356, 121, 178, 1353, 2652, 247, 32, 166, 220, 71, 592, 596, 950, 73, 41, 1820, 8, 22, 89, 86, 419, 649, 1356, 2, 48, 917, 1356, 2, 649, 254, 26, 1041, 1110, 1190, 7, 254, 4755, 693, 822, 693, 1357, 4756, 2653, 1358, 465, 1006, 52, 539, 693, 2654, 10, 469, 294, 1, 1358, 1355, 2655, 4757, 2521, 4758, 951, 966, 1067, 942, 1376, 547, 4759, 1, 1353, 596, 950, 73, 37, 22, 89, 473, 254, 166, 220, 71, 1821, 592, 13, 4760, 1822, 4761, 4762, 8, 332, 72, 467, 1243, 258, 254, 247, 22, 89, 1823, 4763, 220, 71, 4764, 330, 1818, 321, 182, 37, 1354, 934, 9, 1054, 934, 4765, 4766, 1643, 2171, 4767, 2038, 2, 2648, 1054, 2649, 113, 2656, 2147, 2657, 48, 2658, 1817, 37, 4768, 2659, 136, 208, 98, 4769, 2654, 1, 4770, 658, 1652, 1189, 4771, 4772, 1872, 4773, 1, 145, 4774, 1125, 2, 596, 73, 254, 166, 220, 71, 22, 89, 239, 4775, 89, 1824, 4776, 729, 240, 229, 1678, 7, 254, 4777, 9, 2656, 4778, 4779, 4780, 4781, 2301, 2122, 145, 1354, 2660, 236, 9, 145, 1354, 520, 364, 263, 509, 239, 208, 166, 220, 71, 22, 89, 2661, 1330, 326, 4782, 2662, 8, 1584, 509, 1824, 4783, 236, 263, 22, 89, 4784, 4785, 860, 2663, 2664, 243, 799, 2097, 26, 419, 2665, 4786, 99, 9, 860, 220, 117, 254, 22, 89, 1728, 860, 1205, 4787, 162, 596, 73, 41, 1820, 8, 425, 73, 1825, 4788, 4789, 484, 186, 2, 1825, 788, 1355, 2650, 2630, 2666, 1014, 23, 484, 612, 1640, 1067, 1819, 1508, 239, 61, 3, 99, 484, 18, 1826, 1126, 9, 4790, 258, 241, 4791, 2655, 4792, 1825, 788, 4793, 796, 61, 34, 98, 1322, 491, 245, 4794, 474, 917, 2667, 123, 1816, 1205, 2131, 1000, 1, 4795, 239, 61, 3, 63, 27, 1498, 48, 1814, 673, 2668, 1057, 506, 1066, 4796, 1066, 4797, 1499, 61, 1815, 9, 188, 3, 49, 788, 513, 132, 2, 90, 484, 612, 1640, 1194, 61, 3, 49, 9, 1243, 208, 166, 220, 71, 1357, 420, 68, 283, 1, 2651, 1407, 356, 657, 1134, 293, 254, 297, 38, 1820, 1556, 8, 792, 1557, 2, 1558, 119, 46, 910, 241, 1559, 1560, 1197, 1561, 19, 20, 6, 17, 24], [312, 68, 313, 509, 99, 597, 5, 4, 1068, 656, 365, 312, 716, 16, 427, 313, 368, 507, 92, 717, 439, 68, 313, 509, 99, 597, 38, 41, 439, 312, 21, 656, 139, 33, 6, 41, 102, 1526, 8, 957, 1527, 790], [52, 1065, 1069, 185, 417, 1827, 5, 4, 1686, 2669, 22, 1006, 551, 1371, 52, 1293, 446, 10, 52, 1065, 1069, 185, 417, 1827, 2, 961, 2670, 21, 961, 2670, 52, 373, 2671, 10, 52, 1065, 1069, 185, 417, 597, 72, 420, 164, 1827, 52, 373, 274, 513, 100, 85, 330, 52, 373, 2671, 1206, 1995, 7, 417, 4798, 185, 2186, 48, 330, 1198, 1069, 185, 98, 887, 4799, 446, 52, 373, 4800, 4801, 2387, 97, 2300, 52, 2672, 3, 92, 707, 7, 10, 1069, 185, 417, 4802, 1, 77, 52, 4803, 2673, 92, 4804, 2643, 2674, 554, 239, 4805, 239, 98, 4806, 39, 1, 4807, 180, 2, 882, 4808, 919, 4809, 121, 1207, 4810, 644, 4811, 26, 190, 4812, 540, 1040, 85, 2672, 3, 97, 373, 52, 3, 1828, 3, 54, 52, 532, 351, 52, 229, 4813, 4814, 236, 815, 2, 3, 63, 292, 15, 4815, 68, 290, 1, 4816, 1102, 68, 1, 3, 2675, 1554, 4817, 7, 417, 1686, 2669, 22, 299, 1006, 147, 1, 4818, 4819, 8, 472, 412, 27, 1036, 2551, 28, 291, 5, 2676, 1829, 4, 119, 1829, 124, 1829, 19, 20, 6, 17, 24], [312, 801, 716, 313, 368, 507, 439, 597, 5, 4, 1068, 656, 22, 4820, 8, 365, 312, 716, 16, 427, 313, 368, 507, 92, 717, 439, 68, 313, 509, 99, 597, 38, 312, 68, 313, 509, 99, 597, 1068, 656, 365, 312, 716, 16, 427, 313, 368, 507, 92, 717, 439, 68, 313, 509, 99, 597, 38, 41, 439, 312, 21, 656, 668, 1700, 7, 439, 312, 471, 717, 1525, 2677, 55, 1320, 92, 87, 1122, 16, 419, 85, 460, 1068, 62, 801, 716, 313, 368, 507, 121, 407, 4821, 62, 482, 4822, 4823, 1, 1359, 2678, 299, 368, 2679, 4824, 160, 4825, 4826, 4827, 4828, 2680, 4829, 2, 4830, 2263, 2681, 299, 368, 4831, 4832, 828, 4833, 4834, 62, 482, 753, 4835, 4836, 15, 2659, 4837, 4838, 1156, 4839, 299, 368, 4840, 657, 7, 4841, 871, 4842, 10, 717, 4843, 1967, 716, 16, 427, 509, 18, 1260, 313, 368, 2682, 243, 18, 121, 564, 1359, 2678, 299, 4844, 430, 41, 2681, 299, 4845, 4846, 717, 832, 41, 2683, 2677, 2, 312, 439, 244, 4847, 371, 4848, 2684, 2231, 460, 312, 2537, 2684, 2305, 4849, 17, 1068, 62, 717, 2599, 95, 2685, 832, 281, 47, 133, 716, 16, 313, 507, 832, 439, 359, 597, 344, 4850, 312, 1021, 355, 4851, 439, 147, 1, 368, 4852, 4853, 4854, 312, 4855, 138, 4856, 419, 85, 460, 4857, 1021, 694, 1, 181, 857, 184, 313, 4858, 4859, 4860, 90, 46, 1260, 4861, 61, 3, 4862, 4863, 4864, 364, 750, 1156, 49, 139, 33, 6, 41, 102, 1526, 8, 957, 1527, 790], [1830, 598, 2686, 823, 709, 823, 709, 861, 5, 4, 1830, 1831, 2687, 598, 1831, 4, 4865, 8, 4866, 598, 2686, 823, 709, 823, 709, 592, 1176, 2, 3, 657, 1831, 2687, 4867, 4868, 1935, 4869, 1830, 598, 445, 598, 1054, 326, 48, 4870, 2291, 4871, 4872, 2, 695, 823, 709, 18, 1832, 10, 1343, 805, 861, 9, 4873, 598, 628, 71, 2688, 1343, 113, 2689, 4874, 4875, 208, 4876, 2690, 4877, 509, 1606, 862, 2657, 76, 861, 862, 4878, 771, 197, 37, 751, 1833, 1039, 4879, 1481, 2396, 4880, 4881, 799, 121, 473, 2689, 2690, 4882, 4883, 412, 4884, 4885, 1833, 862, 4886, 478, 4887, 4888, 254, 4889, 2, 462, 478, 2652, 1360, 4890, 113, 3, 445, 598, 4891, 1, 2421, 628, 598, 4892, 1382, 2612, 1452, 1141, 641, 670, 889, 445, 2691, 299, 4893, 208, 4894, 299, 2480, 98, 4895, 1343, 1154, 38, 28, 54, 4, 149, 14, 5, 207, 4, 81, 162, 150, 33, 235, 33, 4, 110, 19, 20, 6, 17, 24], [1, 2692, 2693, 229, 2694, 5, 4, 2213, 1, 2695, 330, 184, 48, 1260, 979, 2270, 754, 2695, 668, 4896, 3, 188, 15, 4897, 2696, 4898, 2277, 3, 4899, 1596, 4900, 99, 15, 4901, 4902, 4903, 46, 3, 4904, 99, 4905, 4906, 4907, 65, 200, 4908, 1890, 4909, 508, 111, 4910, 34, 683, 1021, 4911, 4912, 4913, 4914, 4915, 1298, 410, 2322, 1188, 4916, 410, 824, 4917, 610, 52, 66, 1496, 16, 4918, 4919, 243, 822, 4920, 98, 4921, 1309, 413, 4922, 410, 824, 2697, 661, 4923, 1341, 245, 2698, 1342, 508, 373, 4924, 1788, 1, 41, 61, 972, 91, 507, 1700, 38, 1, 2692, 2693, 229, 2694, 4925, 2470, 2378, 2674, 1594, 4926, 4927, 963, 520, 46, 938, 1517, 2699, 3, 2660, 1, 9, 4928, 609, 4929, 4930, 508, 4931, 1549, 2700, 405, 46, 2701, 115, 4932, 1442, 9, 243, 112, 4933, 4934, 952, 550, 77, 4935, 1410, 52, 1065, 767, 826, 532, 2, 4936, 2111, 4937, 2702, 2, 1, 767, 37, 4938, 2099, 16, 48, 4939, 2703, 4940, 78, 4941, 508, 550, 2374, 14, 2704, 262, 4942, 4943, 15, 861, 4944, 4945, 2495, 2, 4946, 52, 297, 224, 752, 1568, 2088, 1673, 52, 4947, 351, 939, 4948, 4949, 92, 4950, 2563, 208, 4951, 4952, 4953, 4954, 413, 1259, 524, 2704, 2158, 4955, 83, 4956, 2699, 1181, 1309, 508, 4957, 12, 1312, 1309, 4958, 1419, 160, 245, 2698, 51, 4959, 1, 4, 2363, 318, 119, 19, 20, 6, 17, 24], [596, 73, 37, 22, 89, 1832, 254, 166, 220, 71, 592, 5, 4, 596, 950, 1358, 612, 73, 13, 1822, 2705, 1834, 249, 2706, 430, 8, 857, 247, 862, 166, 220, 71, 1357, 18, 132, 3, 1821, 4960, 4961, 116, 2707, 86, 419, 649, 1356, 2, 326, 107, 37, 22, 89, 945, 1185, 515, 596, 950, 1358, 612, 73, 4962, 862, 166, 220, 71, 1821, 26, 22, 89, 243, 1832, 1, 425, 73, 13, 1822, 2705, 1834, 249, 2706, 430, 8, 857, 22, 89, 86, 419, 649, 1504, 1356, 609, 520, 32, 4963, 1190, 649, 1041, 4964, 220, 4965, 9, 1322, 30, 2653, 1322, 1, 49, 22, 89, 297, 254, 220, 71, 15, 1818, 4966, 182, 766, 16, 445, 2691, 1677, 4967, 4968, 4969, 1658, 4970, 4971, 2708, 4972, 22, 89, 32, 4973, 2, 425, 73, 49, 968, 22, 89, 9, 32, 410, 300, 22, 89, 2702, 160, 4974, 4975, 4976, 4977, 2707, 48, 2658, 1817, 37, 113, 1, 4978, 2519, 1463, 2697, 319, 4979, 2, 37, 166, 4980, 4981, 394, 425, 73, 4982, 4983, 95, 89, 1204, 330, 1293, 9, 4984, 99, 111, 1823, 297, 4985, 4986, 2128, 7, 4987, 326, 89, 1185, 2543, 2388, 9, 4988, 188, 136, 4989, 49, 425, 73, 254, 4990, 2662, 8, 89, 2661, 964, 89, 243, 2700, 729, 254, 325, 2, 89, 1824, 757, 473, 22, 89, 658, 1652, 1189, 4991, 1, 300, 3, 425, 73, 63, 32, 89, 4992, 860, 15, 2675, 22, 89, 860, 220, 2482, 22, 89, 860, 2664, 18, 1826, 723, 1, 111, 177, 297, 425, 73, 22, 89, 2424, 111, 1823, 297, 240, 1054, 2663, 840, 4993, 4994, 4995, 1, 758, 71, 1442, 1323, 4996, 297, 552, 419, 2665, 14, 1180, 49, 425, 73, 28, 206, 27, 2077, 491, 245, 317, 970, 364, 394, 1508, 92, 2249, 449, 1, 9, 190, 4997, 46, 1724, 4998, 247, 4999, 2667, 626, 526, 1816, 1828, 200, 5000, 239, 61, 3, 390, 49, 9, 1498, 14, 978, 85, 788, 1355, 2709, 1067, 1819, 2709, 390, 1, 2, 1814, 673, 2668, 1057, 506, 2516, 1066, 552, 14, 1220, 1066, 1499, 9, 1815, 796, 61, 3, 390, 1297, 49, 5001, 1353, 2682, 425, 73, 72, 862, 1243, 144, 484, 188, 5002, 9, 788, 132, 159, 2, 90, 115, 5003, 1040, 132, 10, 723, 5004, 92, 706, 2, 1040, 2666, 1, 63, 32, 649, 85, 5005, 796, 427, 1067, 942, 484, 5006, 1828, 547, 297, 32, 1, 49, 5007, 8, 739, 198, 740, 124, 230, 613, 882, 14, 739, 19, 20, 6, 17, 24], [32, 70, 861, 635, 1835, 863, 960, 5, 4, 13, 141, 32, 70, 2375, 13, 141, 5008, 8, 72, 101, 475, 45, 1269, 2, 475, 32, 1, 479, 14, 2673, 32, 70, 861, 2, 755, 11, 475, 2710, 1352, 1360, 592, 7, 52, 474, 32, 70, 101, 1361, 1835, 260, 1836, 380, 446, 380, 5009, 896, 380, 2178, 380, 5010, 674, 5011, 380, 5012, 5013, 2711, 5014, 380, 2450, 380, 5015, 674, 380, 5016, 2029, 2, 163, 5017, 53, 2, 5018, 5019, 5020, 1269, 5021, 101, 37, 112, 1836, 380, 74, 1083, 112, 283, 163, 5022, 546, 5023, 832, 244, 5024, 1360, 480, 972, 1, 70, 259, 5025, 5026, 41, 961, 5027, 59, 32, 70, 5028, 2, 13, 141, 863, 2712, 730, 318, 52, 56, 1361, 1070, 164, 2308, 59, 59, 59, 5029, 1070, 45, 5030, 1070, 164, 1183, 164, 280, 863, 2, 863, 112, 1836, 59, 380, 59, 112, 52, 101, 1361, 1835, 863, 2712, 353, 1, 751, 65, 1639, 37, 32, 70, 820, 318, 1, 1361, 5031, 362, 1932, 2, 3, 755, 11, 475, 2710, 1352, 1360, 5032, 1352, 2520, 1021, 540, 1070, 222, 5033, 200, 588, 71, 2262, 98, 37, 32, 39, 1, 863, 70, 1755, 70, 1161, 1504, 23, 2338, 1, 1070, 1161, 2, 32, 70, 45, 358, 1239, 5034, 2535, 2133, 353, 161, 1553, 5035, 132, 13, 141, 32, 70, 141, 1621, 1622, 1623, 30, 15, 141, 198, 124, 740, 191, 1000, 1624, 141, 1625, 19, 20, 17, 24], [1837, 66, 117, 1639, 120, 1974, 2713, 185, 2714, 2715, 31, 65, 135, 5, 4, 41, 2716, 129, 5036, 107, 1246, 2, 5037, 81, 2716, 274, 170, 5038, 515, 1837, 66, 117, 18, 473, 271, 5039, 5040, 120, 5041, 5042, 1151, 2, 120, 5043, 1484, 66, 117, 1362, 1838, 1756, 1712, 26, 31, 1712, 605, 5044, 1362, 1838, 135, 31, 2713, 185, 341, 341, 5045, 66, 117, 31, 135, 2, 7, 31, 70, 434, 499, 341, 341, 138, 1, 2717, 66, 117, 31, 456, 1692, 2718, 170, 608, 5046, 1, 499, 5047, 1447, 2057, 2718, 1, 665, 5048, 1362, 1838, 135, 31, 2714, 2715, 5049, 1837, 66, 117, 31, 135, 2, 7, 31, 70, 1908, 809, 5050, 5051, 5052, 1925, 7, 5053, 1, 5054, 5055, 1484, 3, 533, 1386, 120, 5056, 2719, 120, 71, 2720, 5057, 31, 97, 1321, 1, 259, 129, 26, 325, 192, 197, 1525, 179], [5058, 71, 172, 1071, 148, 864, 5059, 1363, 1503, 2167, 5, 4, 1071, 148, 906, 663, 864, 1839, 1, 168, 152, 2721, 167, 5060, 1505, 5061, 2680, 2722, 5062, 2, 667, 1773, 9, 5063, 2, 2013, 121, 210, 2040, 1834, 5064, 5065, 1071, 148, 2723, 2679, 752, 378, 5066, 1335, 5067, 1758, 159, 637, 2, 1071, 148, 5068, 1072, 864, 378, 2723, 5069, 2722, 5070, 182, 322, 1604, 201, 16, 550, 637, 2, 5071, 2724, 1072, 5072, 1045, 550, 955, 5073, 5074, 378, 1072, 824, 5075, 5076, 5077, 1826, 1126, 864, 1071, 148, 137, 5078, 270, 4, 1116, 5079, 468, 1766, 5080, 5081, 78, 2531, 1, 5082, 1462, 5083, 5084, 1072, 804, 477, 2268, 1, 671, 152, 3, 1072, 5085, 5086, 5087, 1, 967, 39, 1, 967, 152, 172, 66, 869, 2295, 864, 672, 137, 1, 840, 5088, 5089, 1839, 1, 168, 599, 152, 3, 107, 1246, 1840, 5090, 497, 982, 835, 1138, 1841, 801, 1363, 627, 578, 2, 2676, 1666, 172, 568, 864, 1842, 1, 1363, 31, 2725, 5091, 193, 1357, 5092, 1363, 70, 2726, 57, 215, 8, 57, 215, 30, 15, 301, 125, 57, 119, 142, 302, 19, 20, 6, 17, 24], [1073, 714, 118, 1, 29, 2230, 2571, 5, 4, 167, 4, 2727, 752, 8, 41, 21, 386, 5093, 386, 5094, 2728, 213, 2729, 714, 239, 467, 328, 25, 10, 1843, 53, 1073, 1073, 125, 5095, 103, 53, 386, 125, 29, 715, 245, 668, 155, 147, 3, 1347, 26, 386, 594, 5096, 936, 148, 5097, 125, 715, 253, 125, 29, 715, 245, 1073, 29, 5098, 125, 5099, 1799, 494, 1223, 605, 5100, 307, 582, 1970, 192, 396, 494, 386, 689, 5101, 2, 30, 102, 192, 98, 91, 1073, 617, 117, 25, 122, 5102, 1469, 1676, 386, 1710, 5103, 5104, 2646, 2647, 253, 163, 386, 718, 1492, 257, 25, 122, 118, 1843, 53, 257, 25, 5105, 193, 5106, 789, 1, 1359, 182, 253, 163, 1404, 5107, 1362, 2730, 193, 43, 102, 87, 715, 253, 1784, 2261, 102, 2635, 43, 25, 122, 386, 715, 315, 253, 209, 21, 386, 2538, 1843, 53, 268, 91, 2731, 1016, 617, 328, 25, 122, 5108, 5109, 5110, 87, 2459, 1746, 5111, 5112, 253, 209, 21, 133, 114, 155, 774, 2060, 896, 5113, 774, 315, 560, 43, 102, 25, 5114, 1364, 1446, 253, 386, 493, 714, 2728, 1676, 1359, 1347, 103, 286, 386, 493, 714, 2644, 1, 795, 1328, 147, 192, 5115, 160, 5116, 973, 26, 155, 1810, 16, 3, 167, 4, 826, 696, 149, 2727, 752, 8, 191, 2460, 1578, 369, 2429, 167, 246, 167, 4, 19, 20, 6, 17, 24], [101, 2721, 339, 232, 769, 1090, 115, 1688, 113, 1156, 5, 4, 35, 466, 763, 764, 29, 719, 1844, 381, 29, 719, 172, 790, 5117, 74, 667, 718, 5118, 5119, 1074, 789, 974, 339, 1283, 1462, 260, 29, 719, 1844, 29, 719, 381, 29, 719, 381, 101, 172, 45, 1786, 5120, 1075, 765, 731, 5121, 5122, 1075, 5123, 789, 1674, 5124, 172, 464, 345, 5125, 214, 719, 1844, 1270, 1, 9, 1987, 627, 61, 5126, 92, 5127, 2365, 1280, 5128, 157, 5129, 1840, 5130, 738, 1074, 693, 9, 5131, 974, 31, 2729, 26, 2732, 300, 186, 2725, 5132, 529, 5133, 933, 1990, 172, 1250, 5134, 1511, 1074, 152, 2412, 5135, 2182, 381, 2051, 2315, 27, 430, 2733, 1588, 2545, 2241, 430, 2733, 2734, 1258, 635, 766, 16, 29, 381, 2311, 133, 200, 5136, 220, 5137, 9, 1465, 632, 5138, 407, 147, 2331, 123, 257, 1465, 632, 5139, 815, 5140, 548, 447, 10, 5141, 5142, 180, 5143, 2734, 184, 1845, 5144, 329, 5145, 1042, 1003, 113, 750, 1845, 5146, 10, 1281, 2625, 13, 1668, 2049, 9, 5147, 388, 974, 5148, 1031, 5149, 1749, 1, 1658, 1511, 2409, 5150, 58, 1074, 152, 1176, 407, 7, 543, 13, 821, 5151, 1845, 5152, 329, 1042, 5153, 919, 53, 2, 29, 719, 381, 172, 1710, 5154, 162, 5155, 2334, 5156, 5157, 5158, 39, 1, 497, 281, 1, 974, 2281, 152, 11, 172, 232, 769, 5159, 5160, 5161, 227, 5162, 2273, 280, 11, 1721, 172, 122, 5163, 381, 444, 122, 16, 9, 276, 74, 45, 681, 5164, 55, 1193, 1154, 38, 35, 67, 124, 35, 534, 535, 35, 67, 536, 81, 139, 33, 537, 35], [5165, 5166, 2400, 31, 25, 1034, 5167, 23, 5168, 5, 4, 35, 466, 763, 764, 542, 1692, 170, 357, 5169, 350, 101, 357, 541, 45, 5170, 1807, 351, 48, 542, 357, 408, 323, 203, 23, 720, 1139, 1917, 51, 2735, 5171, 407, 2735, 51, 5172, 5173, 113, 1246, 628, 54, 357, 530, 962, 2, 5174, 73, 73, 502, 203, 23, 720, 1139, 1106, 866, 7, 496, 5175, 177, 875, 2736, 5176, 157, 357, 5177, 5178, 606, 1841, 93, 5179, 2737, 2737, 43, 2738, 5180, 170, 229, 2701, 1365, 1972, 88, 5181, 2, 2087, 43, 1683, 16, 542, 170, 357, 92, 2736, 2330, 5182, 5183, 31, 227, 1402, 7, 496, 357, 120, 204, 640, 2341, 120, 737, 2739, 2740, 2741, 640, 5184, 1369, 31, 387, 173, 87, 341, 1841, 2717, 2742, 1846, 1847, 387, 173, 5185, 2743, 173, 2744, 2745, 5186, 2743, 1076, 890, 796, 5187, 1076, 1035, 2, 176, 31, 2746, 16, 120, 733, 12, 737, 120, 5188, 5189, 5190, 403, 173, 1077, 1280, 523, 5191, 709, 5192, 2095, 1076, 92, 1751, 497, 2359, 899, 157, 88, 2747, 120, 2748, 925, 2749, 170, 733, 120, 5193, 2, 5194, 204, 2741, 176, 5195, 2744, 2745, 5196, 170, 120, 71, 1726, 2720, 620, 1846, 1847, 1574, 1746, 5197, 5198, 5199, 208, 1846, 1847, 1574, 5200, 2742, 2685, 357, 1709, 914, 1, 2026, 399, 1109, 70, 2642, 5201, 357, 5202, 5203, 227, 214, 51, 23, 720, 1139, 1106, 5204, 5205, 5206, 1, 31, 262, 5207, 5208, 274, 31, 2419, 70, 620, 38, 35, 67, 124, 35, 534, 535, 35, 67, 536, 81, 139, 33, 537, 35], [1364, 1135, 100, 755, 1364, 5209, 774, 66, 718, 2, 718, 5, 4, 718, 2527, 5210, 2750, 230, 624, 410, 5211, 114, 868, 626, 2628, 174, 5212, 5213, 708, 5214, 1364, 1135, 959, 5215, 262, 5216, 5217, 408, 463, 5218, 718, 588, 100, 213, 5219, 718, 5220, 5221, 895], [1366, 491, 5222, 172, 652, 80, 531, 10, 5223, 5224, 1078, 5, 4, 5225, 80, 94, 946, 642, 266, 5226, 58, 26, 525, 376, 281, 578, 2, 1839, 1, 2751, 5227, 2, 80, 1687, 2372, 531, 1296, 5228, 5229, 1, 550, 5230, 696, 77, 80, 577, 281, 1297, 899, 80, 168, 599, 2752, 1848, 545, 5231, 5232, 5233, 5234, 2753, 1112, 5235, 80, 1075, 80, 1849, 2754, 80, 58, 330, 1293, 46, 274, 577, 72, 799, 16, 5236, 824, 1911, 80, 691, 5237, 58, 2755, 80, 5238, 266, 1513, 1079, 44, 376, 1694, 5239, 80, 58, 1282, 97, 691, 1333, 61, 3, 2756, 58, 555, 405, 2432, 1641, 3, 2757, 1513, 5240, 2041, 71, 5241, 109, 80, 58, 1641, 1297, 274, 163, 5242, 120, 1079, 1017, 58, 177, 2755, 2758, 80, 58, 5243, 5244, 1223, 5245, 707, 23, 168, 599, 605, 1850, 80, 264, 1842, 5246, 2759, 1366, 68, 21, 5247, 1075, 1849, 1851, 1282, 97, 168, 1140, 558, 5248, 1, 577, 721, 2, 1328, 2166, 497, 1017, 58, 1851, 1017, 5249, 2754, 3, 555, 80, 172, 558, 3, 1581, 1581, 1279, 578, 2, 1078, 94, 44, 5250, 168, 2297, 2756, 172, 558, 10, 44, 295, 1, 270, 213, 859, 376, 324, 607, 2757, 1664, 1, 1015, 558, 5251, 2760, 1, 2760, 97, 66, 2761, 859, 31, 324, 578, 2, 1279, 1, 168, 558, 3, 1643, 5252, 5253, 31, 48, 192, 5254, 31, 2761, 859, 2762, 60, 2762, 213, 859, 324, 274, 163, 80, 1849, 1851, 1287, 1850, 1680, 58, 1850, 1852, 721, 113, 279, 578, 2, 1664, 1078, 568, 3, 1752, 285, 1852, 80, 80, 1075, 1663, 58, 1752, 285, 1852, 721, 5255, 1642, 1333, 61, 3, 5256, 2336, 58, 5257, 1842, 578, 2, 2708, 2636, 1078, 558, 3, 163, 5258, 58, 2747, 5259, 5260, 1848, 78, 721, 1282, 97, 5261, 1014, 568, 3, 80, 577, 2752, 1848, 545, 5262, 2703, 5263, 5264, 2763, 5265, 446, 1666, 2764, 1937, 5266, 683, 496, 21, 80, 2269, 555, 46, 465, 5267, 2696, 707, 58, 23, 274, 5268, 5269, 168, 599, 1, 1101, 465, 577, 168, 1140, 2394, 61, 3, 1548, 2190, 5270, 10, 1074, 5271, 721, 80, 577, 707, 58, 721, 607, 1366, 68, 168, 599, 80, 58, 410, 281, 1328, 578, 2, 2358, 172, 568, 3, 2758, 1079, 5272, 80, 58, 449, 943, 296, 1276, 2765, 1015, 2724, 1079, 2, 80, 58, 1840, 2340, 23, 1660, 5273, 1272, 648, 1017, 2750, 78, 721, 2751, 5274, 1078, 568, 3, 1366, 68, 2759, 663, 5275, 5276, 68, 5277, 168, 2030, 369, 193, 2764, 5278, 1630, 1798, 168, 599, 11, 51, 147, 7, 22, 839, 2526, 5279, 5280, 5281, 5282, 284, 463, 2766, 298, 101, 899, 168, 599, 477, 2683, 172, 5283, 5284, 1788, 2335, 172, 168, 599, 152, 766, 16, 3, 5285, 1064, 5286, 2627, 30, 15, 1064, 119, 142, 1, 1576, 1064, 19, 20, 6, 17, 24], [440, 441, 1367, 14, 1271, 58, 255, 175, 387, 173, 230, 5, 4, 2260, 2753, 2730, 193, 2767, 440, 441, 196, 1, 387, 173, 196, 2, 1036, 5287, 440, 441, 64, 5288, 524, 1367, 14, 1271, 77, 58, 255, 175, 387, 173, 152, 440, 441, 196, 288, 409, 337, 129, 5289, 2, 161, 1036, 230, 624, 3, 375, 440, 441, 58, 255, 175, 387, 173, 144, 1, 5290, 5291, 58, 2282, 683, 2768, 58, 98, 5292, 1853, 2769, 1368, 80, 2770, 720, 2771, 1854, 2772, 1855, 529, 2773, 66, 2774, 1317, 2775, 164, 387, 173, 837, 2, 295, 58, 255, 175, 1853, 1403, 2719, 120, 10, 5293, 2769, 1368, 542, 170, 1035, 2, 2739, 176, 1715, 80, 255, 175, 5294, 1, 120, 1663, 449, 5295, 2770, 1585, 170, 283, 2, 120, 1079, 1414, 1290, 23, 720, 255, 175, 2771, 5296, 170, 2740, 146, 1451, 120, 5297, 370, 255, 175, 1854, 5298, 5299, 120, 5300, 92, 2773, 66, 2774, 1076, 1035, 2, 99, 1855, 529, 255, 175, 5301, 2738, 981, 2746, 16, 120, 10, 737, 640, 1013, 5302, 5303, 2566, 1006, 2084, 120, 803, 2765, 1288, 2152, 387, 173, 1477, 2775, 5304, 170, 283, 2, 1317, 255, 175, 274, 440, 441, 1367, 14, 1295, 2711, 164, 440, 441, 58, 255, 175, 387, 173, 66, 5305, 1077, 836, 504, 5306, 896, 836, 837, 2, 5307, 5308, 5309, 5310, 1077, 836, 1853, 80, 720, 255, 1215, 2749, 1230, 5311, 405, 5312, 120, 5313, 504, 185, 1077, 836, 2772, 1855, 529, 1317, 255, 1215, 1751, 542, 170, 120, 691, 2748, 2, 1365, 5314, 170, 706, 746, 2767, 5315, 5316, 63, 2384, 58, 255, 175, 387, 173, 440, 441, 264, 1367, 14, 1271, 31, 551, 2, 683, 2768, 477, 58, 1978, 1, 322, 213, 542, 170, 706, 1077, 210, 542, 9, 602, 255, 175, 640, 395, 3, 532, 400, 440, 441, 31, 196, 430, 39, 1, 103, 53, 58, 255, 175, 387, 173, 283, 1, 43, 2455, 632, 5317, 253, 163, 5318, 5319, 1672, 582, 70, 5320, 3, 230, 31, 5321, 5322, 5323, 543, 78, 53, 2, 1321, 1, 240, 440, 441, 67, 129, 325, 3, 2726, 57, 215, 8, 57, 215, 30, 15, 301, 125, 57, 119, 142, 302, 19, 20, 6, 17, 24], [434, 388, 1856, 644, 1857, 1080, 830, 2776, 2777, 157, 5, 4, 434, 388, 1080, 830, 23, 1365, 1, 2778, 841, 1856, 644, 1857, 1368, 196, 38, 570, 374, 571, 41, 434, 388, 388, 1856, 644, 1857, 1368, 2244, 416, 1080, 830, 2776, 2777, 23, 1365, 157, 1, 665, 31, 5324, 2, 1080, 830, 1076, 1035, 2, 3, 1080, 830, 2512, 5325, 5326, 434, 388, 1143, 1144, 1854, 5327, 2763, 5328, 5329, 1749, 5330, 5331, 1600, 1025, 1698, 1722, 3, 271, 77, 1936, 5332, 1648, 5333, 5334, 601, 1614, 1722, 3, 7, 31, 7, 824, 2778, 323, 689, 1681, 2766, 689, 434, 388, 67, 689, 102, 3, 5335, 5336, 8, 191, 137, 572, 46, 140, 4, 19, 20, 6, 17, 24, 242, 46, 44, 177, 14, 139, 167, 573, 574], [35, 436, 2779, 371, 292, 442, 146, 31, 5337, 5, 4, 35, 5338, 442, 146, 323, 193, 292, 35, 436, 2779, 65, 193, 122, 1833, 5339, 366, 2780, 188, 95, 442, 5340, 2781, 5341, 292, 2781, 2533, 1, 957, 790, 647, 39, 1, 576, 944, 193, 196, 258, 2688, 230, 1766, 442, 193, 292, 31, 462, 244, 1016, 39, 1, 2142, 428, 67, 689, 1286, 239, 3, 9, 672, 2782, 146, 2783, 117, 979, 173, 198, 6, 2731, 533, 5342, 1500, 442, 173, 177, 300, 3, 39, 1, 442, 146, 31, 620, 292, 672, 2782, 146, 2783, 117, 979, 173, 5343, 2780, 146, 5344, 292, 1142, 2784, 442, 173, 816, 200, 442, 5345, 213, 442, 146, 31, 239, 3, 130, 292, 58, 2784, 58, 442, 173, 5346, 5347, 2327, 1375, 2, 220, 5348, 56, 5349, 2134, 58, 5350, 720, 198, 637, 2, 292, 18, 90, 530, 983, 3, 292, 812, 326, 1239, 71, 25, 5351, 1450, 95, 1112, 274, 31, 6, 40, 635, 25, 2732, 300, 257, 45, 1651, 2, 735, 47, 38, 35, 67, 124, 35, 534, 535, 35, 67, 536, 81, 139, 33, 537, 35]]\n"
     ]
    }
   ],
   "source": [
    "data = tokenizer.texts_to_sequences(texts)\n",
    "#각 단어 인덱스를 숫자 리스트로 변환해줌 \n",
    "print(data)\n",
    "#전체 데이터셋이 90개이고 각 기사마다 토큰화한것들에 대해 고유한 인덱스가 부여된것을 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...   15   22 1094]\n",
      " [ 456  451 1894 ...    6   17   24]\n",
      " [   0    0    0 ...   15   22 1094]\n",
      " ...\n",
      " [  16  120   10 ...    6   17   24]\n",
      " [   0    0    0 ...  167  573  574]\n",
      " [   0    0    0 ...   33  537   35]]\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(data, maxlen = maxlen)\n",
    "print(data)\n",
    "#한 리스트당 길이를 200개로 지정을 해줌 \n",
    "#지정을 하지 않으면 학습이 되지 않거나 좋지 못한 결과가 나옴 \n",
    "#만약 200개가 안된다면 앞에 0을 넣어주는 Padding을 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def to_one_hot(labels, dimension):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    #0행렬을 만듬 \n",
    "    for i,label in enumerate(labels):\n",
    "        results[i,label] = 1.\n",
    "    return results\n",
    "#위의 매개함수의 뜻은 label이 0이면 i가 label의 길이만큼 증가하고 이후 label이 1이 되는 과정을 계속 반복  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = to_one_hot(labels, dimension = class_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels\n",
    "#각 값마다 라벨값을 부여 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "[77 80 26 68 25 28 62 30 81 83  6 65 72 44 43 73 36 34 47 10 40 61 79 38\n",
      "  8 16 86 45 29 23  7  1  4 63 88 85 32 24 74 59 20 37  9  3 87 17 64 53\n",
      " 89 69 35 70 67 56 82 33 12 39 54  2 76 31 21 52 66 48 41 27 57 84 78 42\n",
      "  0 22 75 15 55 13 46 51 49 11 50 58 18 19 14 71 60  5]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "print(indices)\n",
    "np.random.shuffle(indices)\n",
    "print(indices)\n",
    "#랜덤하게 섞어줌 \n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "#이렇게 랜덤하게 섞어주어 모델의 신뢰성을 높임 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[validation_ratio:]\n",
    "y_train = labels[validation_ratio:]\n",
    "X_test = data[:validation_ratio]\n",
    "y_test = labels[:validation_ratio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Activation, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 50)           500000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 279       \n",
      "=================================================================\n",
      "Total params: 506,859\n",
      "Trainable params: 506,859\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(max_words, output_dim = embedding_dim,input_length = maxlen))\n",
    "#input node가 만개, output이 50개 각 노드의 길이는 200\n",
    "model.add(layers.SimpleRNN(50))\n",
    "model.add(layers.Dense(30, activation = 'relu'))\n",
    "model.add(layers.Dense(9, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63, 200)\n",
      "(63, 9)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63 samples\n",
      "Epoch 1/200\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 2.2130 - acc: 0.1111\n",
      "Epoch 2/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 1.9464 - acc: 0.6667\n",
      "Epoch 3/200\n",
      "63/63 [==============================] - 0s 976us/sample - loss: 1.8575 - acc: 0.6508\n",
      "Epoch 4/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 1.6991 - acc: 0.8730\n",
      "Epoch 5/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 1.5918 - acc: 0.8730\n",
      "Epoch 6/200\n",
      "63/63 [==============================] - 0s 988us/sample - loss: 1.4540 - acc: 0.9841\n",
      "Epoch 7/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 1.3328 - acc: 0.9841\n",
      "Epoch 8/200\n",
      "63/63 [==============================] - 0s 995us/sample - loss: 1.2407 - acc: 1.0000\n",
      "Epoch 9/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 1.1355 - acc: 1.0000\n",
      "Epoch 10/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 1.0495 - acc: 1.0000\n",
      "Epoch 11/200\n",
      "63/63 [==============================] - 0s 900us/sample - loss: 0.9718 - acc: 1.0000\n",
      "Epoch 12/200\n",
      "63/63 [==============================] - 0s 904us/sample - loss: 0.9122 - acc: 1.0000\n",
      "Epoch 13/200\n",
      "63/63 [==============================] - 0s 920us/sample - loss: 0.8694 - acc: 1.0000\n",
      "Epoch 14/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.7888 - acc: 1.0000\n",
      "Epoch 15/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.7363 - acc: 1.0000\n",
      "Epoch 16/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.6693 - acc: 1.0000\n",
      "Epoch 17/200\n",
      "63/63 [==============================] - 0s 951us/sample - loss: 0.6326 - acc: 1.0000\n",
      "Epoch 18/200\n",
      "63/63 [==============================] - 0s 902us/sample - loss: 0.5971 - acc: 1.0000\n",
      "Epoch 19/200\n",
      "63/63 [==============================] - 0s 881us/sample - loss: 0.5605 - acc: 1.0000\n",
      "Epoch 20/200\n",
      "63/63 [==============================] - 0s 843us/sample - loss: 0.5088 - acc: 1.0000\n",
      "Epoch 21/200\n",
      "63/63 [==============================] - 0s 928us/sample - loss: 0.4763 - acc: 1.0000\n",
      "Epoch 22/200\n",
      "63/63 [==============================] - 0s 911us/sample - loss: 0.4493 - acc: 1.0000\n",
      "Epoch 23/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.4320 - acc: 1.0000\n",
      "Epoch 24/200\n",
      "63/63 [==============================] - 0s 926us/sample - loss: 0.3945 - acc: 1.0000\n",
      "Epoch 25/200\n",
      "63/63 [==============================] - 0s 872us/sample - loss: 0.3665 - acc: 1.0000\n",
      "Epoch 26/200\n",
      "63/63 [==============================] - 0s 940us/sample - loss: 0.3456 - acc: 1.0000\n",
      "Epoch 27/200\n",
      "63/63 [==============================] - 0s 943us/sample - loss: 0.3250 - acc: 1.0000\n",
      "Epoch 28/200\n",
      "63/63 [==============================] - 0s 968us/sample - loss: 0.3100 - acc: 1.0000\n",
      "Epoch 29/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.2993 - acc: 1.0000\n",
      "Epoch 30/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.2762 - acc: 1.0000\n",
      "Epoch 31/200\n",
      "63/63 [==============================] - 0s 924us/sample - loss: 0.2574 - acc: 1.0000\n",
      "Epoch 32/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.2455 - acc: 1.0000\n",
      "Epoch 33/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.2317 - acc: 1.0000\n",
      "Epoch 34/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.2154 - acc: 1.0000\n",
      "Epoch 35/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.2063 - acc: 1.0000\n",
      "Epoch 36/200\n",
      "63/63 [==============================] - 0s 988us/sample - loss: 0.1985 - acc: 1.0000\n",
      "Epoch 37/200\n",
      "63/63 [==============================] - 0s 905us/sample - loss: 0.1892 - acc: 1.0000\n",
      "Epoch 38/200\n",
      "63/63 [==============================] - 0s 876us/sample - loss: 0.1853 - acc: 1.0000\n",
      "Epoch 39/200\n",
      "63/63 [==============================] - 0s 956us/sample - loss: 0.1691 - acc: 1.0000\n",
      "Epoch 40/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.1585 - acc: 1.0000\n",
      "Epoch 41/200\n",
      "63/63 [==============================] - 0s 938us/sample - loss: 0.1466 - acc: 1.0000\n",
      "Epoch 42/200\n",
      "63/63 [==============================] - 0s 864us/sample - loss: 0.1359 - acc: 1.0000\n",
      "Epoch 43/200\n",
      "63/63 [==============================] - 0s 931us/sample - loss: 0.1293 - acc: 1.0000\n",
      "Epoch 44/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.1256 - acc: 1.0000\n",
      "Epoch 45/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.1223 - acc: 1.0000\n",
      "Epoch 46/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.1128 - acc: 1.0000\n",
      "Epoch 47/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.1057 - acc: 1.0000\n",
      "Epoch 48/200\n",
      "63/63 [==============================] - 0s 956us/sample - loss: 0.1004 - acc: 1.0000\n",
      "Epoch 49/200\n",
      "63/63 [==============================] - 0s 892us/sample - loss: 0.0958 - acc: 1.0000\n",
      "Epoch 50/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.0915 - acc: 1.0000\n",
      "Epoch 51/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.0870 - acc: 1.0000\n",
      "Epoch 52/200\n",
      "63/63 [==============================] - 0s 957us/sample - loss: 0.0820 - acc: 1.0000\n",
      "Epoch 53/200\n",
      "63/63 [==============================] - 0s 953us/sample - loss: 0.0794 - acc: 1.0000\n",
      "Epoch 54/200\n",
      "63/63 [==============================] - 0s 958us/sample - loss: 0.0741 - acc: 1.0000\n",
      "Epoch 55/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.0704 - acc: 1.0000\n",
      "Epoch 56/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.0643 - acc: 1.0000\n",
      "Epoch 57/200\n",
      "63/63 [==============================] - 0s 976us/sample - loss: 0.0619 - acc: 1.0000\n",
      "Epoch 58/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.0595 - acc: 1.0000\n",
      "Epoch 59/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.0572 - acc: 1.0000\n",
      "Epoch 60/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.0527 - acc: 1.0000\n",
      "Epoch 61/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.0504 - acc: 1.0000\n",
      "Epoch 62/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.0487 - acc: 1.0000\n",
      "Epoch 63/200\n",
      "63/63 [==============================] - 0s 947us/sample - loss: 0.0487 - acc: 1.0000\n",
      "Epoch 64/200\n",
      "63/63 [==============================] - 0s 911us/sample - loss: 0.0461 - acc: 1.0000\n",
      "Epoch 65/200\n",
      "63/63 [==============================] - 0s 895us/sample - loss: 0.0428 - acc: 1.0000\n",
      "Epoch 66/200\n",
      "63/63 [==============================] - 0s 931us/sample - loss: 0.0410 - acc: 1.0000\n",
      "Epoch 67/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.0379 - acc: 1.0000\n",
      "Epoch 68/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.0342 - acc: 1.0000\n",
      "Epoch 69/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.0321 - acc: 1.0000\n",
      "Epoch 70/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.0311 - acc: 1.0000\n",
      "Epoch 71/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.0304 - acc: 1.0000\n",
      "Epoch 72/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.0295 - acc: 1.0000\n",
      "Epoch 73/200\n",
      "63/63 [==============================] - 0s 876us/sample - loss: 0.0281 - acc: 1.0000\n",
      "Epoch 74/200\n",
      "63/63 [==============================] - 0s 859us/sample - loss: 0.0257 - acc: 1.0000\n",
      "Epoch 75/200\n",
      "63/63 [==============================] - 0s 875us/sample - loss: 0.0241 - acc: 1.0000\n",
      "Epoch 76/200\n",
      "63/63 [==============================] - 0s 849us/sample - loss: 0.0227 - acc: 1.0000\n",
      "Epoch 77/200\n",
      "63/63 [==============================] - 0s 951us/sample - loss: 0.0221 - acc: 1.0000\n",
      "Epoch 78/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.0217 - acc: 1.0000\n",
      "Epoch 79/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.0208 - acc: 1.0000\n",
      "Epoch 80/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.0201 - acc: 1.0000\n",
      "Epoch 81/200\n",
      "63/63 [==============================] - 0s 991us/sample - loss: 0.0180 - acc: 1.0000\n",
      "Epoch 82/200\n",
      "63/63 [==============================] - 0s 953us/sample - loss: 0.0170 - acc: 1.0000\n",
      "Epoch 83/200\n",
      "63/63 [==============================] - 0s 897us/sample - loss: 0.0164 - acc: 1.0000\n",
      "Epoch 84/200\n",
      "63/63 [==============================] - 0s 921us/sample - loss: 0.0159 - acc: 1.0000\n",
      "Epoch 85/200\n",
      "63/63 [==============================] - 0s 942us/sample - loss: 0.0152 - acc: 1.0000\n",
      "Epoch 86/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.0149 - acc: 1.0000\n",
      "Epoch 87/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.0138 - acc: 1.0000\n",
      "Epoch 88/200\n",
      "63/63 [==============================] - 0s 976us/sample - loss: 0.0129 - acc: 1.0000\n",
      "Epoch 89/200\n",
      "63/63 [==============================] - 0s 932us/sample - loss: 0.0119 - acc: 1.0000\n",
      "Epoch 90/200\n",
      "63/63 [==============================] - 0s 871us/sample - loss: 0.0119 - acc: 1.0000\n",
      "Epoch 91/200\n",
      "63/63 [==============================] - 0s 942us/sample - loss: 0.0117 - acc: 1.0000\n",
      "Epoch 92/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.0112 - acc: 1.0000\n",
      "Epoch 93/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.0109 - acc: 1.0000\n",
      "Epoch 94/200\n",
      "63/63 [==============================] - 0s 990us/sample - loss: 0.0097 - acc: 1.0000\n",
      "Epoch 95/200\n",
      "63/63 [==============================] - 0s 916us/sample - loss: 0.0091 - acc: 1.0000\n",
      "Epoch 96/200\n",
      "63/63 [==============================] - 0s 857us/sample - loss: 0.0085 - acc: 1.0000\n",
      "Epoch 97/200\n",
      "63/63 [==============================] - 0s 803us/sample - loss: 0.0081 - acc: 1.0000\n",
      "Epoch 98/200\n",
      "63/63 [==============================] - 0s 868us/sample - loss: 0.0079 - acc: 1.0000\n",
      "Epoch 99/200\n",
      "63/63 [==============================] - 0s 903us/sample - loss: 0.0076 - acc: 1.0000\n",
      "Epoch 100/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.0072 - acc: 1.0000\n",
      "Epoch 101/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 102/200\n",
      "63/63 [==============================] - 0s 927us/sample - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 103/200\n",
      "63/63 [==============================] - 0s 850us/sample - loss: 0.0071 - acc: 1.0000\n",
      "Epoch 104/200\n",
      "63/63 [==============================] - 0s 918us/sample - loss: 0.0067 - acc: 1.0000\n",
      "Epoch 105/200\n",
      "63/63 [==============================] - 0s 856us/sample - loss: 0.0062 - acc: 1.0000\n",
      "Epoch 106/200\n",
      "63/63 [==============================] - 0s 859us/sample - loss: 0.0054 - acc: 1.0000\n",
      "Epoch 107/200\n",
      "63/63 [==============================] - 0s 895us/sample - loss: 0.0050 - acc: 1.0000\n",
      "Epoch 108/200\n",
      "63/63 [==============================] - 0s 896us/sample - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 109/200\n",
      "63/63 [==============================] - 0s 872us/sample - loss: 0.0045 - acc: 1.0000\n",
      "Epoch 110/200\n",
      "63/63 [==============================] - 0s 868us/sample - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 111/200\n",
      "63/63 [==============================] - 0s 881us/sample - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 112/200\n",
      "63/63 [==============================] - 0s 858us/sample - loss: 0.0039 - acc: 1.0000\n",
      "Epoch 113/200\n",
      "63/63 [==============================] - 0s 853us/sample - loss: 0.0038 - acc: 1.0000\n",
      "Epoch 114/200\n",
      "63/63 [==============================] - 0s 828us/sample - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 115/200\n",
      "63/63 [==============================] - 0s 884us/sample - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 116/200\n",
      "63/63 [==============================] - 0s 892us/sample - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 117/200\n",
      "63/63 [==============================] - 0s 855us/sample - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 118/200\n",
      "63/63 [==============================] - 0s 806us/sample - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 119/200\n",
      "63/63 [==============================] - 0s 880us/sample - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 120/200\n",
      "63/63 [==============================] - 0s 799us/sample - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 121/200\n",
      "63/63 [==============================] - 0s 825us/sample - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 122/200\n",
      "63/63 [==============================] - 0s 800us/sample - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 123/200\n",
      "63/63 [==============================] - 0s 814us/sample - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 124/200\n",
      "63/63 [==============================] - 0s 827us/sample - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 125/200\n",
      "63/63 [==============================] - 0s 863us/sample - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 126/200\n",
      "63/63 [==============================] - 0s 848us/sample - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 127/200\n",
      "63/63 [==============================] - 0s 870us/sample - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 128/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 129/200\n",
      "63/63 [==============================] - 0s 983us/sample - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 130/200\n",
      "63/63 [==============================] - 0s 819us/sample - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 131/200\n",
      "63/63 [==============================] - 0s 885us/sample - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 132/200\n",
      "63/63 [==============================] - 0s 886us/sample - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 133/200\n",
      "63/63 [==============================] - 0s 865us/sample - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 134/200\n",
      "63/63 [==============================] - 0s 878us/sample - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 135/200\n",
      "63/63 [==============================] - 0s 866us/sample - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 136/200\n",
      "63/63 [==============================] - 0s 829us/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 137/200\n",
      "63/63 [==============================] - 0s 827us/sample - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 138/200\n",
      "63/63 [==============================] - 0s 849us/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 139/200\n",
      "63/63 [==============================] - 0s 824us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 140/200\n",
      "63/63 [==============================] - 0s 859us/sample - loss: 9.7281e-04 - acc: 1.0000\n",
      "Epoch 141/200\n",
      "63/63 [==============================] - 0s 859us/sample - loss: 9.3188e-04 - acc: 1.0000\n",
      "Epoch 142/200\n",
      "63/63 [==============================] - 0s 963us/sample - loss: 8.8740e-04 - acc: 1.0000\n",
      "Epoch 143/200\n",
      "63/63 [==============================] - 0s 880us/sample - loss: 9.0429e-04 - acc: 1.0000\n",
      "Epoch 144/200\n",
      "63/63 [==============================] - 0s 855us/sample - loss: 8.7717e-04 - acc: 1.0000\n",
      "Epoch 145/200\n",
      "63/63 [==============================] - 0s 902us/sample - loss: 8.6989e-04 - acc: 1.0000\n",
      "Epoch 146/200\n",
      "63/63 [==============================] - 0s 886us/sample - loss: 8.1008e-04 - acc: 1.0000\n",
      "Epoch 147/200\n",
      "63/63 [==============================] - 0s 858us/sample - loss: 7.4599e-04 - acc: 1.0000\n",
      "Epoch 148/200\n",
      "63/63 [==============================] - 0s 882us/sample - loss: 6.9716e-04 - acc: 1.0000\n",
      "Epoch 149/200\n",
      "63/63 [==============================] - 0s 885us/sample - loss: 6.8247e-04 - acc: 1.0000\n",
      "Epoch 150/200\n",
      "63/63 [==============================] - 0s 885us/sample - loss: 6.6074e-04 - acc: 1.0000\n",
      "Epoch 151/200\n",
      "63/63 [==============================] - 0s 901us/sample - loss: 6.1679e-04 - acc: 1.0000\n",
      "Epoch 152/200\n",
      "63/63 [==============================] - 0s 881us/sample - loss: 5.7062e-04 - acc: 1.0000\n",
      "Epoch 153/200\n",
      "63/63 [==============================] - 0s 905us/sample - loss: 5.3993e-04 - acc: 1.0000\n",
      "Epoch 154/200\n",
      "63/63 [==============================] - 0s 850us/sample - loss: 5.5012e-04 - acc: 1.0000\n",
      "Epoch 155/200\n",
      "63/63 [==============================] - 0s 864us/sample - loss: 5.2945e-04 - acc: 1.0000\n",
      "Epoch 156/200\n",
      "63/63 [==============================] - 0s 880us/sample - loss: 4.9319e-04 - acc: 1.0000\n",
      "Epoch 157/200\n",
      "63/63 [==============================] - 0s 954us/sample - loss: 4.4503e-04 - acc: 1.0000\n",
      "Epoch 158/200\n",
      "63/63 [==============================] - 0s 839us/sample - loss: 4.2174e-04 - acc: 1.0000\n",
      "Epoch 159/200\n",
      "63/63 [==============================] - 0s 809us/sample - loss: 3.9398e-04 - acc: 1.0000\n",
      "Epoch 160/200\n",
      "63/63 [==============================] - 0s 869us/sample - loss: 3.7481e-04 - acc: 1.0000\n",
      "Epoch 161/200\n",
      "63/63 [==============================] - 0s 867us/sample - loss: 3.6254e-04 - acc: 1.0000\n",
      "Epoch 162/200\n",
      "63/63 [==============================] - 0s 894us/sample - loss: 3.5106e-04 - acc: 1.0000\n",
      "Epoch 163/200\n",
      "63/63 [==============================] - 0s 958us/sample - loss: 3.4272e-04 - acc: 1.0000\n",
      "Epoch 164/200\n",
      "63/63 [==============================] - 0s 862us/sample - loss: 3.4074e-04 - acc: 1.0000\n",
      "Epoch 165/200\n",
      "63/63 [==============================] - 0s 898us/sample - loss: 3.2253e-04 - acc: 1.0000\n",
      "Epoch 166/200\n",
      "63/63 [==============================] - 0s 887us/sample - loss: 3.0809e-04 - acc: 1.0000\n",
      "Epoch 167/200\n",
      "63/63 [==============================] - 0s 879us/sample - loss: 2.9753e-04 - acc: 1.0000\n",
      "Epoch 168/200\n",
      "63/63 [==============================] - 0s 871us/sample - loss: 2.9841e-04 - acc: 1.0000\n",
      "Epoch 169/200\n",
      "63/63 [==============================] - 0s 887us/sample - loss: 2.9211e-04 - acc: 1.0000\n",
      "Epoch 170/200\n",
      "63/63 [==============================] - 0s 855us/sample - loss: 2.5552e-04 - acc: 1.0000\n",
      "Epoch 171/200\n",
      "63/63 [==============================] - 0s 812us/sample - loss: 2.3713e-04 - acc: 1.0000\n",
      "Epoch 172/200\n",
      "63/63 [==============================] - 0s 828us/sample - loss: 2.1761e-04 - acc: 1.0000\n",
      "Epoch 173/200\n",
      "63/63 [==============================] - 0s 842us/sample - loss: 2.0866e-04 - acc: 1.0000\n",
      "Epoch 174/200\n",
      "63/63 [==============================] - 0s 803us/sample - loss: 1.9361e-04 - acc: 1.0000\n",
      "Epoch 175/200\n",
      "63/63 [==============================] - 0s 765us/sample - loss: 1.8198e-04 - acc: 1.0000\n",
      "Epoch 176/200\n",
      "63/63 [==============================] - 0s 751us/sample - loss: 1.7123e-04 - acc: 1.0000\n",
      "Epoch 177/200\n",
      "63/63 [==============================] - 0s 797us/sample - loss: 1.6147e-04 - acc: 1.0000\n",
      "Epoch 178/200\n",
      "63/63 [==============================] - 0s 803us/sample - loss: 1.5507e-04 - acc: 1.0000\n",
      "Epoch 179/200\n",
      "63/63 [==============================] - 0s 821us/sample - loss: 1.4820e-04 - acc: 1.0000\n",
      "Epoch 180/200\n",
      "63/63 [==============================] - 0s 739us/sample - loss: 1.4314e-04 - acc: 1.0000\n",
      "Epoch 181/200\n",
      "63/63 [==============================] - 0s 842us/sample - loss: 1.3914e-04 - acc: 1.0000\n",
      "Epoch 182/200\n",
      "63/63 [==============================] - 0s 915us/sample - loss: 1.3180e-04 - acc: 1.0000\n",
      "Epoch 183/200\n",
      "63/63 [==============================] - 0s 866us/sample - loss: 1.2530e-04 - acc: 1.0000\n",
      "Epoch 184/200\n",
      "63/63 [==============================] - 0s 795us/sample - loss: 1.1886e-04 - acc: 1.0000\n",
      "Epoch 185/200\n",
      "63/63 [==============================] - 0s 799us/sample - loss: 1.1707e-04 - acc: 1.0000\n",
      "Epoch 186/200\n",
      "63/63 [==============================] - 0s 859us/sample - loss: 1.1625e-04 - acc: 1.0000\n",
      "Epoch 187/200\n",
      "63/63 [==============================] - 0s 881us/sample - loss: 1.1491e-04 - acc: 1.0000\n",
      "Epoch 188/200\n",
      "63/63 [==============================] - 0s 880us/sample - loss: 1.0891e-04 - acc: 1.0000\n",
      "Epoch 189/200\n",
      "63/63 [==============================] - 0s 881us/sample - loss: 1.0491e-04 - acc: 1.0000\n",
      "Epoch 190/200\n",
      "63/63 [==============================] - 0s 859us/sample - loss: 1.0193e-04 - acc: 1.0000\n",
      "Epoch 191/200\n",
      "63/63 [==============================] - 0s 819us/sample - loss: 1.0007e-04 - acc: 1.0000\n",
      "Epoch 192/200\n",
      "63/63 [==============================] - 0s 828us/sample - loss: 1.7454e-04 - acc: 1.0000\n",
      "Epoch 193/200\n",
      "63/63 [==============================] - 0s 808us/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 194/200\n",
      "63/63 [==============================] - 0s 855us/sample - loss: 0.0465 - acc: 0.9841\n",
      "Epoch 195/200\n",
      "63/63 [==============================] - 0s 834us/sample - loss: 0.0213 - acc: 1.0000\n",
      "Epoch 196/200\n",
      "63/63 [==============================] - 0s 832us/sample - loss: 0.5160 - acc: 0.8730\n",
      "Epoch 197/200\n",
      "63/63 [==============================] - 0s 822us/sample - loss: 1.1339 - acc: 0.6984\n",
      "Epoch 198/200\n",
      "63/63 [==============================] - 0s 802us/sample - loss: 2.5404 - acc: 0.4603\n",
      "Epoch 199/200\n",
      "63/63 [==============================] - 0s 841us/sample - loss: 0.2893 - acc: 0.9206\n",
      "Epoch 200/200\n",
      "63/63 [==============================] - 0s 879us/sample - loss: 0.0512 - acc: 0.9841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff980340b10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,  epochs = 200, batch_size = 64)\n",
    "#거의 과적합이고 데이터의 개수가 부족하여 validation을 나누지 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 200, 50)           500000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 9)                 279       \n",
      "=================================================================\n",
      "Total params: 522,009\n",
      "Trainable params: 522,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(max_words, output_dim = embedding_dim,input_length = maxlen))\n",
    "#input node가 만개, output이 50개 각 노드의 길이는 200\n",
    "model.add(layers.LSTM(50))\n",
    "model.add(layers.Dense(30, activation = 'relu'))\n",
    "model.add(layers.Dense(9, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63 samples\n",
      "Epoch 1/200\n",
      "63/63 [==============================] - 2s 35ms/sample - loss: 2.1968 - acc: 0.0794\n",
      "Epoch 2/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1901 - acc: 0.1270\n",
      "Epoch 3/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1817 - acc: 0.1746\n",
      "Epoch 4/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1722 - acc: 0.1746\n",
      "Epoch 5/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1601 - acc: 0.1905\n",
      "Epoch 6/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1419 - acc: 0.2222\n",
      "Epoch 7/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1108 - acc: 0.2222\n",
      "Epoch 8/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0056 - acc: 0.2063\n",
      "Epoch 9/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0639 - acc: 0.1429\n",
      "Epoch 10/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0133 - acc: 0.5079\n",
      "Epoch 11/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8837 - acc: 0.3810\n",
      "Epoch 12/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8437 - acc: 0.4444\n",
      "Epoch 13/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8210 - acc: 0.3175\n",
      "Epoch 14/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8102 - acc: 0.5238\n",
      "Epoch 15/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7404 - acc: 0.4286\n",
      "Epoch 16/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7342 - acc: 0.3651\n",
      "Epoch 17/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7342 - acc: 0.4286\n",
      "Epoch 18/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6671 - acc: 0.5873\n",
      "Epoch 19/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6376 - acc: 0.5079\n",
      "Epoch 20/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6257 - acc: 0.5714\n",
      "Epoch 21/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6478 - acc: 0.3492\n",
      "Epoch 22/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6324 - acc: 0.3968\n",
      "Epoch 23/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.5558 - acc: 0.6032\n",
      "Epoch 24/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.5388 - acc: 0.4921\n",
      "Epoch 25/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.5665 - acc: 0.6508\n",
      "Epoch 26/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.5444 - acc: 0.6667\n",
      "Epoch 27/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4511 - acc: 0.6190\n",
      "Epoch 28/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4899 - acc: 0.5079\n",
      "Epoch 29/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3771 - acc: 0.6667\n",
      "Epoch 30/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3339 - acc: 0.6984\n",
      "Epoch 31/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3088 - acc: 0.7143\n",
      "Epoch 32/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4028 - acc: 0.5238\n",
      "Epoch 33/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3046 - acc: 0.7619\n",
      "Epoch 34/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2345 - acc: 0.6984\n",
      "Epoch 35/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2056 - acc: 0.7778\n",
      "Epoch 36/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2698 - acc: 0.5873\n",
      "Epoch 37/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2031 - acc: 0.7778\n",
      "Epoch 38/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1986 - acc: 0.5873\n",
      "Epoch 39/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1184 - acc: 0.7778\n",
      "Epoch 40/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0719 - acc: 0.7302\n",
      "Epoch 41/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0403 - acc: 0.8413\n",
      "Epoch 42/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0723 - acc: 0.7460\n",
      "Epoch 43/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1755 - acc: 0.7937\n",
      "Epoch 44/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2476 - acc: 0.6508\n",
      "Epoch 45/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1433 - acc: 0.8095\n",
      "Epoch 46/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0467 - acc: 0.8730\n",
      "Epoch 47/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9799 - acc: 0.8254\n",
      "Epoch 48/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9486 - acc: 0.7937\n",
      "Epoch 49/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9274 - acc: 0.7937\n",
      "Epoch 50/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9170 - acc: 0.8571\n",
      "Epoch 51/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8851 - acc: 0.8571\n",
      "Epoch 52/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8843 - acc: 0.8571\n",
      "Epoch 53/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9055 - acc: 0.8889\n",
      "Epoch 54/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8433 - acc: 0.8571\n",
      "Epoch 55/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8252 - acc: 0.8889\n",
      "Epoch 56/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8167 - acc: 0.8571\n",
      "Epoch 57/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8020 - acc: 0.8889\n",
      "Epoch 58/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7863 - acc: 0.8413\n",
      "Epoch 59/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7793 - acc: 0.8889\n",
      "Epoch 60/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7598 - acc: 0.8730\n",
      "Epoch 61/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7580 - acc: 0.8889\n",
      "Epoch 62/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7311 - acc: 0.8889\n",
      "Epoch 63/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7388 - acc: 0.8889\n",
      "Epoch 64/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7003 - acc: 0.9048\n",
      "Epoch 65/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6962 - acc: 0.8889\n",
      "Epoch 66/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6736 - acc: 0.9048\n",
      "Epoch 67/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6830 - acc: 0.8889\n",
      "Epoch 68/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6458 - acc: 0.8889\n",
      "Epoch 69/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6313 - acc: 0.8889\n",
      "Epoch 70/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6101 - acc: 0.9048\n",
      "Epoch 71/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5985 - acc: 0.8889\n",
      "Epoch 72/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5840 - acc: 0.9048\n",
      "Epoch 73/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5951 - acc: 0.8889\n",
      "Epoch 74/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5735 - acc: 0.9048\n",
      "Epoch 75/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5736 - acc: 0.8889\n",
      "Epoch 76/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6021 - acc: 0.8730\n",
      "Epoch 77/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6024 - acc: 0.8730\n",
      "Epoch 78/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5675 - acc: 0.8571\n",
      "Epoch 79/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0986 - acc: 0.6825\n",
      "Epoch 80/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6483 - acc: 0.8413\n",
      "Epoch 81/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6091 - acc: 0.8571\n",
      "Epoch 82/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6385 - acc: 0.8095\n",
      "Epoch 83/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5882 - acc: 0.8254\n",
      "Epoch 84/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4833 - acc: 0.8889\n",
      "Epoch 85/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4718 - acc: 0.8889\n",
      "Epoch 86/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4607 - acc: 0.9048\n",
      "Epoch 87/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4498 - acc: 0.9048\n",
      "Epoch 88/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4390 - acc: 0.9048\n",
      "Epoch 89/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4281 - acc: 0.9524\n",
      "Epoch 90/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4169 - acc: 0.9524\n",
      "Epoch 91/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5316 - acc: 0.9524\n",
      "Epoch 92/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4346 - acc: 0.9841\n",
      "Epoch 93/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4000 - acc: 0.9841\n",
      "Epoch 94/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4201 - acc: 0.9841\n",
      "Epoch 95/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4027 - acc: 0.9841\n",
      "Epoch 96/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4765 - acc: 0.8413\n",
      "Epoch 97/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.3683 - acc: 0.9683\n",
      "Epoch 98/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.3509 - acc: 0.9683\n",
      "Epoch 99/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.3298 - acc: 1.0000\n",
      "Epoch 100/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.3187 - acc: 1.0000\n",
      "Epoch 101/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4034 - acc: 0.9683\n",
      "Epoch 102/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.3664 - acc: 0.9048\n",
      "Epoch 103/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.3612 - acc: 0.9841\n",
      "Epoch 104/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8219 - acc: 0.7302\n",
      "Epoch 105/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4356 - acc: 0.8889\n",
      "Epoch 106/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2959 - acc: 0.9841\n",
      "Epoch 107/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2761 - acc: 1.0000\n",
      "Epoch 108/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2596 - acc: 1.0000\n",
      "Epoch 109/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2466 - acc: 1.0000\n",
      "Epoch 110/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2351 - acc: 1.0000\n",
      "Epoch 111/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2253 - acc: 1.0000\n",
      "Epoch 112/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2167 - acc: 1.0000\n",
      "Epoch 113/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2060 - acc: 1.0000\n",
      "Epoch 114/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1966 - acc: 1.0000\n",
      "Epoch 115/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1878 - acc: 1.0000\n",
      "Epoch 116/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1797 - acc: 1.0000\n",
      "Epoch 117/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1793 - acc: 1.0000\n",
      "Epoch 118/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1946 - acc: 1.0000\n",
      "Epoch 119/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.3373 - acc: 0.9524\n",
      "Epoch 120/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5974 - acc: 0.8889\n",
      "Epoch 121/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2360 - acc: 0.9683\n",
      "Epoch 122/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1726 - acc: 1.0000\n",
      "Epoch 123/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1643 - acc: 1.0000\n",
      "Epoch 124/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1579 - acc: 1.0000\n",
      "Epoch 125/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1520 - acc: 1.0000\n",
      "Epoch 126/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1465 - acc: 1.0000\n",
      "Epoch 127/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1413 - acc: 1.0000\n",
      "Epoch 128/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1363 - acc: 1.0000\n",
      "Epoch 129/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1314 - acc: 1.0000\n",
      "Epoch 130/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1267 - acc: 1.0000\n",
      "Epoch 131/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1221 - acc: 1.0000\n",
      "Epoch 132/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1176 - acc: 1.0000\n",
      "Epoch 133/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1134 - acc: 1.0000\n",
      "Epoch 134/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1093 - acc: 1.0000\n",
      "Epoch 135/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1055 - acc: 1.0000\n",
      "Epoch 136/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1021 - acc: 1.0000\n",
      "Epoch 137/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0998 - acc: 1.0000\n",
      "Epoch 138/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0987 - acc: 1.0000\n",
      "Epoch 139/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0968 - acc: 1.0000\n",
      "Epoch 140/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0911 - acc: 1.0000\n",
      "Epoch 141/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0863 - acc: 1.0000\n",
      "Epoch 142/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0814 - acc: 1.0000\n",
      "Epoch 143/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0783 - acc: 1.0000\n",
      "Epoch 144/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0748 - acc: 1.0000\n",
      "Epoch 145/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0720 - acc: 1.0000\n",
      "Epoch 146/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0690 - acc: 1.0000\n",
      "Epoch 147/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0669 - acc: 1.0000\n",
      "Epoch 148/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0664 - acc: 1.0000\n",
      "Epoch 149/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0656 - acc: 1.0000\n",
      "Epoch 150/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0616 - acc: 1.0000\n",
      "Epoch 151/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0596 - acc: 1.0000\n",
      "Epoch 152/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0569 - acc: 1.0000\n",
      "Epoch 153/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0538 - acc: 1.0000\n",
      "Epoch 154/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0520 - acc: 1.0000\n",
      "Epoch 155/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0482 - acc: 1.0000\n",
      "Epoch 156/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0459 - acc: 1.0000\n",
      "Epoch 157/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0434 - acc: 1.0000\n",
      "Epoch 158/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0414 - acc: 1.0000\n",
      "Epoch 159/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0399 - acc: 1.0000\n",
      "Epoch 160/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0382 - acc: 1.0000\n",
      "Epoch 161/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0366 - acc: 1.0000\n",
      "Epoch 162/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0356 - acc: 1.0000\n",
      "Epoch 163/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0349 - acc: 1.0000\n",
      "Epoch 164/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0372 - acc: 1.0000\n",
      "Epoch 165/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5752 - acc: 0.8571\n",
      "Epoch 166/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6103 - acc: 0.5556\n",
      "Epoch 167/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2158 - acc: 0.9683\n",
      "Epoch 168/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0510 - acc: 1.0000\n",
      "Epoch 169/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0427 - acc: 1.0000\n",
      "Epoch 170/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0400 - acc: 1.0000\n",
      "Epoch 171/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0387 - acc: 1.0000\n",
      "Epoch 172/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0375 - acc: 1.0000\n",
      "Epoch 173/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0365 - acc: 1.0000\n",
      "Epoch 174/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0355 - acc: 1.0000\n",
      "Epoch 175/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0346 - acc: 1.0000\n",
      "Epoch 176/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0338 - acc: 1.0000\n",
      "Epoch 177/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0330 - acc: 1.0000\n",
      "Epoch 178/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0322 - acc: 1.0000\n",
      "Epoch 179/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0314 - acc: 1.0000\n",
      "Epoch 180/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0307 - acc: 1.0000\n",
      "Epoch 181/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0299 - acc: 1.0000\n",
      "Epoch 182/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0292 - acc: 1.0000\n",
      "Epoch 183/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0285 - acc: 1.0000\n",
      "Epoch 184/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0277 - acc: 1.0000\n",
      "Epoch 185/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0270 - acc: 1.0000\n",
      "Epoch 186/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0262 - acc: 1.0000\n",
      "Epoch 187/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0256 - acc: 1.0000\n",
      "Epoch 188/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0249 - acc: 1.0000\n",
      "Epoch 189/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0242 - acc: 1.0000\n",
      "Epoch 190/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0235 - acc: 1.0000\n",
      "Epoch 191/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0229 - acc: 1.0000\n",
      "Epoch 192/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0222 - acc: 1.0000\n",
      "Epoch 193/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0216 - acc: 1.0000\n",
      "Epoch 194/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0210 - acc: 1.0000\n",
      "Epoch 195/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0205 - acc: 1.0000\n",
      "Epoch 196/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0200 - acc: 1.0000\n",
      "Epoch 197/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0195 - acc: 1.0000\n",
      "Epoch 198/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0190 - acc: 1.0000\n",
      "Epoch 199/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0184 - acc: 1.0000\n",
      "Epoch 200/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0176 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff9839b78d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop', metrics = ['acc'])\n",
    "model.fit(X_train, y_train,  epochs = 200, batch_size = 64)\n",
    "#데이터의 개수가 부족하여 validation을 나누지 못함 \n",
    "#신기한게 과거의 정보를 학습해서 그런지 모르겠지만 점점 acc가 높아짐 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 200, 50)           500000    \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 50)                15300     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 9)                 279       \n",
      "=================================================================\n",
      "Total params: 517,109\n",
      "Trainable params: 517,109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(max_words, output_dim = embedding_dim,input_length = maxlen))\n",
    "#input node가 만개, output이 50개 각 노드의 길이는 200\n",
    "model.add(layers.GRU(50))\n",
    "model.add(layers.Dense(30, activation = 'relu'))\n",
    "model.add(layers.Dense(9, activation = 'softmax'))\n",
    "model.summary()\n",
    "#다음에 노드를 엄청 복잡하게 쌓아봐야겠음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63 samples\n",
      "Epoch 1/200\n",
      "63/63 [==============================] - 2s 33ms/sample - loss: 2.1977 - acc: 0.1111\n",
      "Epoch 2/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1869 - acc: 0.1746\n",
      "Epoch 3/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1766 - acc: 0.1746\n",
      "Epoch 4/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1666 - acc: 0.2381\n",
      "Epoch 5/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1563 - acc: 0.3175\n",
      "Epoch 6/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1453 - acc: 0.3175\n",
      "Epoch 7/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1334 - acc: 0.3651\n",
      "Epoch 8/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1205 - acc: 0.3333\n",
      "Epoch 9/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1071 - acc: 0.3651\n",
      "Epoch 10/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0917 - acc: 0.3651\n",
      "Epoch 11/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0754 - acc: 0.3651\n",
      "Epoch 12/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0587 - acc: 0.3651\n",
      "Epoch 13/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0394 - acc: 0.3651\n",
      "Epoch 14/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0193 - acc: 0.3651\n",
      "Epoch 15/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9966 - acc: 0.3651\n",
      "Epoch 16/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9719 - acc: 0.3651\n",
      "Epoch 17/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9445 - acc: 0.3651\n",
      "Epoch 18/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9132 - acc: 0.3651\n",
      "Epoch 19/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8733 - acc: 0.3651\n",
      "Epoch 20/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8223 - acc: 0.3651\n",
      "Epoch 21/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7257 - acc: 0.3810\n",
      "Epoch 22/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1524 - acc: 0.3016\n",
      "Epoch 23/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7398 - acc: 0.3968\n",
      "Epoch 24/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6971 - acc: 0.3810\n",
      "Epoch 25/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6656 - acc: 0.3810\n",
      "Epoch 26/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6409 - acc: 0.3810\n",
      "Epoch 27/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6202 - acc: 0.3968\n",
      "Epoch 28/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6014 - acc: 0.3968\n",
      "Epoch 29/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.5835 - acc: 0.3968\n",
      "Epoch 30/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.5662 - acc: 0.3968\n",
      "Epoch 31/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.5495 - acc: 0.4127\n",
      "Epoch 32/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.5334 - acc: 0.4127\n",
      "Epoch 33/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.5184 - acc: 0.4286\n",
      "Epoch 34/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.5034 - acc: 0.4286\n",
      "Epoch 35/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4888 - acc: 0.4286\n",
      "Epoch 36/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4774 - acc: 0.4286\n",
      "Epoch 37/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4662 - acc: 0.4603\n",
      "Epoch 38/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 1.4490 - acc: 0.4286\n",
      "Epoch 39/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4346 - acc: 0.4762\n",
      "Epoch 40/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4263 - acc: 0.4444\n",
      "Epoch 41/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4193 - acc: 0.5079\n",
      "Epoch 42/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3982 - acc: 0.4762\n",
      "Epoch 43/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3884 - acc: 0.5397\n",
      "Epoch 44/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3733 - acc: 0.5556\n",
      "Epoch 45/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3584 - acc: 0.5079\n",
      "Epoch 46/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3483 - acc: 0.5556\n",
      "Epoch 47/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3299 - acc: 0.5079\n",
      "Epoch 48/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3196 - acc: 0.5397\n",
      "Epoch 49/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3341 - acc: 0.5079\n",
      "Epoch 50/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3540 - acc: 0.5397\n",
      "Epoch 51/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2945 - acc: 0.5556\n",
      "Epoch 52/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2741 - acc: 0.5556\n",
      "Epoch 53/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2549 - acc: 0.5556\n",
      "Epoch 54/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2355 - acc: 0.5556\n",
      "Epoch 55/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2179 - acc: 0.5714\n",
      "Epoch 56/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2026 - acc: 0.5556\n",
      "Epoch 57/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2150 - acc: 0.5238\n",
      "Epoch 58/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2337 - acc: 0.5397\n",
      "Epoch 59/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1792 - acc: 0.5556\n",
      "Epoch 60/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1542 - acc: 0.6032\n",
      "Epoch 61/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1442 - acc: 0.5397\n",
      "Epoch 62/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1562 - acc: 0.5556\n",
      "Epoch 63/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1181 - acc: 0.5556\n",
      "Epoch 64/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0918 - acc: 0.6032\n",
      "Epoch 65/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0746 - acc: 0.5556\n",
      "Epoch 66/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0715 - acc: 0.6032\n",
      "Epoch 67/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1041 - acc: 0.5238\n",
      "Epoch 68/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1237 - acc: 0.5556\n",
      "Epoch 69/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0497 - acc: 0.6032\n",
      "Epoch 70/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0225 - acc: 0.6032\n",
      "Epoch 71/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0014 - acc: 0.6032\n",
      "Epoch 72/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9803 - acc: 0.6032\n",
      "Epoch 73/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9646 - acc: 0.6032\n",
      "Epoch 74/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9702 - acc: 0.6032\n",
      "Epoch 75/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0413 - acc: 0.5238\n",
      "Epoch 76/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0382 - acc: 0.5556\n",
      "Epoch 77/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9662 - acc: 0.6032\n",
      "Epoch 78/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9323 - acc: 0.6032\n",
      "Epoch 79/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9124 - acc: 0.6032\n",
      "Epoch 80/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8948 - acc: 0.6032\n",
      "Epoch 81/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8781 - acc: 0.6032\n",
      "Epoch 82/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8651 - acc: 0.6032\n",
      "Epoch 83/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8853 - acc: 0.6825\n",
      "Epoch 84/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9648 - acc: 0.5238\n",
      "Epoch 85/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9360 - acc: 0.6190\n",
      "Epoch 86/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8470 - acc: 0.6825\n",
      "Epoch 87/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8242 - acc: 0.6667\n",
      "Epoch 88/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8053 - acc: 0.6667\n",
      "Epoch 89/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7912 - acc: 0.6349\n",
      "Epoch 90/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7801 - acc: 0.6984\n",
      "Epoch 91/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7849 - acc: 0.6190\n",
      "Epoch 92/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8546 - acc: 0.6825\n",
      "Epoch 93/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9584 - acc: 0.5079\n",
      "Epoch 94/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8156 - acc: 0.7778\n",
      "Epoch 95/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7699 - acc: 0.7302\n",
      "Epoch 96/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7500 - acc: 0.7302\n",
      "Epoch 97/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7328 - acc: 0.7302\n",
      "Epoch 98/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7175 - acc: 0.7143\n",
      "Epoch 99/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7034 - acc: 0.7143\n",
      "Epoch 100/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6903 - acc: 0.7143\n",
      "Epoch 101/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6984 - acc: 0.7619\n",
      "Epoch 102/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7030 - acc: 0.8730\n",
      "Epoch 103/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7045 - acc: 0.7778\n",
      "Epoch 104/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7139 - acc: 0.7778\n",
      "Epoch 105/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7949 - acc: 0.7302\n",
      "Epoch 106/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7023 - acc: 0.8413\n",
      "Epoch 107/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6535 - acc: 0.7778\n",
      "Epoch 108/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6350 - acc: 0.8730\n",
      "Epoch 109/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6311 - acc: 0.7460\n",
      "Epoch 110/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6602 - acc: 0.8254\n",
      "Epoch 111/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7316 - acc: 0.7619\n",
      "Epoch 112/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6417 - acc: 0.8889\n",
      "Epoch 113/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6019 - acc: 0.8413\n",
      "Epoch 114/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5783 - acc: 0.9206\n",
      "Epoch 115/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5642 - acc: 0.7937\n",
      "Epoch 116/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5599 - acc: 0.9206\n",
      "Epoch 117/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6096 - acc: 0.7778\n",
      "Epoch 118/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7285 - acc: 0.7619\n",
      "Epoch 119/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7409 - acc: 0.7302\n",
      "Epoch 120/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5715 - acc: 0.8730\n",
      "Epoch 121/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5406 - acc: 0.9048\n",
      "Epoch 122/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5179 - acc: 0.9365\n",
      "Epoch 123/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4995 - acc: 0.9365\n",
      "Epoch 124/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4839 - acc: 0.9524\n",
      "Epoch 125/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4699 - acc: 0.9524\n",
      "Epoch 126/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4567 - acc: 0.9841\n",
      "Epoch 127/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4453 - acc: 0.9683\n",
      "Epoch 128/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4522 - acc: 0.9524\n",
      "Epoch 129/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5801 - acc: 0.8095\n",
      "Epoch 130/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5564 - acc: 0.8730\n",
      "Epoch 131/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5346 - acc: 0.8571\n",
      "Epoch 132/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4258 - acc: 0.9683\n",
      "Epoch 133/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4065 - acc: 0.9683\n",
      "Epoch 134/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4081 - acc: 0.9524\n",
      "Epoch 135/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4015 - acc: 0.9841\n",
      "Epoch 136/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.3806 - acc: 0.9683\n",
      "Epoch 137/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.3725 - acc: 0.9683\n",
      "Epoch 138/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4035 - acc: 0.9683\n",
      "Epoch 139/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5034 - acc: 0.8254\n",
      "Epoch 140/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4533 - acc: 0.9524\n",
      "Epoch 141/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4199 - acc: 0.9365\n",
      "Epoch 142/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.3549 - acc: 0.9524\n",
      "Epoch 143/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.3325 - acc: 0.9683\n",
      "Epoch 144/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.3178 - acc: 0.9683\n",
      "Epoch 145/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.3200 - acc: 0.9683\n",
      "Epoch 146/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.3263 - acc: 0.9683\n",
      "Epoch 147/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.3678 - acc: 0.9524\n",
      "Epoch 148/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.3129 - acc: 0.9683\n",
      "Epoch 149/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.3183 - acc: 0.9683\n",
      "Epoch 150/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2942 - acc: 0.9683\n",
      "Epoch 151/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2965 - acc: 0.9841\n",
      "Epoch 152/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2784 - acc: 0.9683\n",
      "Epoch 153/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2892 - acc: 0.9841\n",
      "Epoch 154/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2850 - acc: 0.9683\n",
      "Epoch 155/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.3062 - acc: 0.9841\n",
      "Epoch 156/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2846 - acc: 0.9683\n",
      "Epoch 157/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2908 - acc: 0.9683\n",
      "Epoch 158/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2466 - acc: 0.9683\n",
      "Epoch 159/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2452 - acc: 0.9841\n",
      "Epoch 160/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2230 - acc: 0.9841\n",
      "Epoch 161/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2210 - acc: 1.0000\n",
      "Epoch 162/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2059 - acc: 1.0000\n",
      "Epoch 163/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2060 - acc: 1.0000\n",
      "Epoch 164/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2006 - acc: 0.9841\n",
      "Epoch 165/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2127 - acc: 0.9841\n",
      "Epoch 166/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2093 - acc: 0.9841\n",
      "Epoch 167/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2400 - acc: 0.9841\n",
      "Epoch 168/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2140 - acc: 0.9841\n",
      "Epoch 169/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2331 - acc: 0.9841\n",
      "Epoch 170/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2134 - acc: 0.9841\n",
      "Epoch 171/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2332 - acc: 0.9841\n",
      "Epoch 172/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1901 - acc: 1.0000\n",
      "Epoch 173/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1611 - acc: 1.0000\n",
      "Epoch 174/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1538 - acc: 1.0000\n",
      "Epoch 175/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1476 - acc: 1.0000\n",
      "Epoch 176/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1425 - acc: 1.0000\n",
      "Epoch 177/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1420 - acc: 1.0000\n",
      "Epoch 178/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1411 - acc: 1.0000\n",
      "Epoch 179/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1643 - acc: 1.0000\n",
      "Epoch 180/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1757 - acc: 0.9841\n",
      "Epoch 181/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0217 - acc: 0.6984\n",
      "Epoch 182/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.3254 - acc: 0.9206\n",
      "Epoch 183/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.2287 - acc: 0.9841\n",
      "Epoch 184/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1787 - acc: 0.9841\n",
      "Epoch 185/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1428 - acc: 1.0000\n",
      "Epoch 186/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1332 - acc: 1.0000\n",
      "Epoch 187/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1275 - acc: 1.0000\n",
      "Epoch 188/200\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 0.1231 - acc: 1.0000\n",
      "Epoch 189/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1196 - acc: 1.0000\n",
      "Epoch 190/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1164 - acc: 1.0000\n",
      "Epoch 191/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1137 - acc: 1.0000\n",
      "Epoch 192/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1111 - acc: 1.0000\n",
      "Epoch 193/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1088 - acc: 1.0000\n",
      "Epoch 194/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1065 - acc: 1.0000\n",
      "Epoch 195/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1044 - acc: 1.0000\n",
      "Epoch 196/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1023 - acc: 1.0000\n",
      "Epoch 197/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.1003 - acc: 1.0000\n",
      "Epoch 198/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0987 - acc: 1.0000\n",
      "Epoch 199/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0982 - acc: 1.0000\n",
      "Epoch 200/200\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.0968 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff9878718d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop', metrics = ['acc'])\n",
    "model.fit(X_train, y_train,  epochs = 200, batch_size = 64)\n",
    "#데이터의 개수가 부족하여 validation을 나누지 못함 \n",
    "#LSTM에 비해 확실히 연산속도가 빠르고 acc가 꾸준하게 올라감 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 규제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 200, 50)           500000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 9)                 279       \n",
      "=================================================================\n",
      "Total params: 522,009\n",
      "Trainable params: 522,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(max_words, output_dim = embedding_dim,input_length = maxlen))\n",
    "model.add(layers.LSTM(50))\n",
    "model.add(layers.Dense(30, kernel_regularizer = regularizers.l1(0.001),activation = 'relu'))\n",
    "model.add(layers.Dense(9, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63 samples\n",
      "Epoch 1/100\n",
      "63/63 [==============================] - 2s 33ms/sample - loss: 2.4034 - acc: 0.0794\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.3906 - acc: 0.1587\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.3795 - acc: 0.2063\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.3668 - acc: 0.1587\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.3482 - acc: 0.2540\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.3090 - acc: 0.1746\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1544 - acc: 0.3333\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0685 - acc: 0.3175\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0972 - acc: 0.3175\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.7213 - acc: 0.2540\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.3616 - acc: 0.2857\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0959 - acc: 0.3651\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1495 - acc: 0.3968\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0384 - acc: 0.3968\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9998 - acc: 0.4127\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0367 - acc: 0.3651\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0004 - acc: 0.3810\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9599 - acc: 0.4127\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9395 - acc: 0.3968\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9192 - acc: 0.3810\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9696 - acc: 0.3968\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9540 - acc: 0.3968\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8948 - acc: 0.4127\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8748 - acc: 0.3968\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8581 - acc: 0.3810\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8421 - acc: 0.3810\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8293 - acc: 0.3968\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8153 - acc: 0.3810\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8002 - acc: 0.4127\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7879 - acc: 0.3651\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8112 - acc: 0.3968\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7823 - acc: 0.3492\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8279 - acc: 0.3492\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7680 - acc: 0.4444\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7383 - acc: 0.4127\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7183 - acc: 0.4127\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6997 - acc: 0.3968\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7184 - acc: 0.3651\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6835 - acc: 0.3651\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7001 - acc: 0.3810\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6572 - acc: 0.3810\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6690 - acc: 0.3810\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6356 - acc: 0.4921\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6003 - acc: 0.5556\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6142 - acc: 0.3175\n",
      "Epoch 46/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6052 - acc: 0.5397\n",
      "Epoch 47/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.5419 - acc: 0.4762\n",
      "Epoch 48/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.5217 - acc: 0.5556\n",
      "Epoch 49/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.5204 - acc: 0.5079\n",
      "Epoch 50/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6269 - acc: 0.4603\n",
      "Epoch 51/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6859 - acc: 0.6190\n",
      "Epoch 52/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4953 - acc: 0.5873\n",
      "Epoch 53/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4502 - acc: 0.5397\n",
      "Epoch 54/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4328 - acc: 0.6508\n",
      "Epoch 55/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4199 - acc: 0.5714\n",
      "Epoch 56/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4442 - acc: 0.6032\n",
      "Epoch 57/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3726 - acc: 0.6825\n",
      "Epoch 58/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3630 - acc: 0.6825\n",
      "Epoch 59/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3559 - acc: 0.6667\n",
      "Epoch 60/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3535 - acc: 0.6984\n",
      "Epoch 61/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3125 - acc: 0.6667\n",
      "Epoch 62/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3088 - acc: 0.6190\n",
      "Epoch 63/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3259 - acc: 0.6508\n",
      "Epoch 64/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2339 - acc: 0.7619\n",
      "Epoch 65/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2082 - acc: 0.7302\n",
      "Epoch 66/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2026 - acc: 0.7937\n",
      "Epoch 67/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2450 - acc: 0.6349\n",
      "Epoch 68/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2139 - acc: 0.8413\n",
      "Epoch 69/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1329 - acc: 0.7778\n",
      "Epoch 70/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1104 - acc: 0.7302\n",
      "Epoch 71/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1256 - acc: 0.7778\n",
      "Epoch 72/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1421 - acc: 0.6825\n",
      "Epoch 73/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1335 - acc: 0.7460\n",
      "Epoch 74/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0206 - acc: 0.8571\n",
      "Epoch 75/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9913 - acc: 0.8413\n",
      "Epoch 76/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9638 - acc: 0.8730\n",
      "Epoch 77/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9404 - acc: 0.8889\n",
      "Epoch 78/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9317 - acc: 0.9048\n",
      "Epoch 79/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0095 - acc: 0.8095\n",
      "Epoch 80/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1414 - acc: 0.7619\n",
      "Epoch 81/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9085 - acc: 0.8730\n",
      "Epoch 82/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8788 - acc: 0.8254\n",
      "Epoch 83/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8728 - acc: 0.9048\n",
      "Epoch 84/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8369 - acc: 0.8571\n",
      "Epoch 85/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8308 - acc: 0.9048\n",
      "Epoch 86/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8002 - acc: 0.8889\n",
      "Epoch 87/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7811 - acc: 0.9524\n",
      "Epoch 88/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7542 - acc: 0.9365\n",
      "Epoch 89/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7444 - acc: 0.9365\n",
      "Epoch 90/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7253 - acc: 0.9365\n",
      "Epoch 91/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7384 - acc: 0.9048\n",
      "Epoch 92/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7275 - acc: 0.9365\n",
      "Epoch 93/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6893 - acc: 0.9524\n",
      "Epoch 94/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6700 - acc: 0.9524\n",
      "Epoch 95/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6579 - acc: 0.9524\n",
      "Epoch 96/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6779 - acc: 0.9206\n",
      "Epoch 97/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7448 - acc: 0.8254\n",
      "Epoch 98/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6379 - acc: 0.9524\n",
      "Epoch 99/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7154 - acc: 0.8413\n",
      "Epoch 100/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6648 - acc: 0.9524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff9732e2b90>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop', metrics = ['acc'])\n",
    "model.fit(X_train, y_train,  epochs = 100, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 200, 50)           500000    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 9)                 279       \n",
      "=================================================================\n",
      "Total params: 522,009\n",
      "Trainable params: 522,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(max_words, output_dim = embedding_dim,input_length = maxlen))\n",
    "#input node가 만개, output이 50개 각 노드의 길이는 200\n",
    "model.add(layers.LSTM(50))\n",
    "model.add(layers.Dense(30, kernel_regularizer = regularizers.l2(0.01),activation = 'relu'))\n",
    "model.add(layers.Dense(9, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63 samples\n",
      "Epoch 1/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 2.5628 - acc: 0.1270\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.5409 - acc: 0.0794\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.5229 - acc: 0.3492\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.5058 - acc: 0.2698\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.4870 - acc: 0.2698\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.4642 - acc: 0.2698\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.4248 - acc: 0.2381\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.5253 - acc: 0.1587\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.4001 - acc: 0.4127\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.3748 - acc: 0.3492\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.3476 - acc: 0.3175\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.3158 - acc: 0.2857\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.2763 - acc: 0.2857\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.2291 - acc: 0.3016\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1821 - acc: 0.3016\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1457 - acc: 0.3810\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1174 - acc: 0.3810\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0928 - acc: 0.4286\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0720 - acc: 0.4127\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0560 - acc: 0.3968\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0349 - acc: 0.4127\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0175 - acc: 0.4127\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0034 - acc: 0.4286\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9850 - acc: 0.4286\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9758 - acc: 0.3968\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9819 - acc: 0.5556\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9412 - acc: 0.3968\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9136 - acc: 0.4603\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8859 - acc: 0.5714\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8940 - acc: 0.4921\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8953 - acc: 0.6190\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8721 - acc: 0.5397\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8503 - acc: 0.4921\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8156 - acc: 0.5873\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8761 - acc: 0.5238\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8374 - acc: 0.5397\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8785 - acc: 0.6667\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7212 - acc: 0.5873\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7415 - acc: 0.4762\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7770 - acc: 0.5873\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6945 - acc: 0.6349\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6445 - acc: 0.6508\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6089 - acc: 0.6508\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7345 - acc: 0.5556\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6511 - acc: 0.5556\n",
      "Epoch 46/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.5703 - acc: 0.6667\n",
      "Epoch 47/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.5175 - acc: 0.6508\n",
      "Epoch 48/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4774 - acc: 0.6825\n",
      "Epoch 49/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4414 - acc: 0.6825\n",
      "Epoch 50/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4099 - acc: 0.6825\n",
      "Epoch 51/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3811 - acc: 0.6825\n",
      "Epoch 52/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3564 - acc: 0.6825\n",
      "Epoch 53/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3318 - acc: 0.7143\n",
      "Epoch 54/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3159 - acc: 0.6984\n",
      "Epoch 55/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2967 - acc: 0.7143\n",
      "Epoch 56/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0675 - acc: 0.3333\n",
      "Epoch 57/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4275 - acc: 0.6825\n",
      "Epoch 58/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2833 - acc: 0.7143\n",
      "Epoch 59/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2569 - acc: 0.7460\n",
      "Epoch 60/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2358 - acc: 0.7778\n",
      "Epoch 61/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2285 - acc: 0.7778\n",
      "Epoch 62/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2007 - acc: 0.8095\n",
      "Epoch 63/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1799 - acc: 0.8095\n",
      "Epoch 64/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1601 - acc: 0.8095\n",
      "Epoch 65/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1401 - acc: 0.8095\n",
      "Epoch 66/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1203 - acc: 0.8095\n",
      "Epoch 67/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1009 - acc: 0.8095\n",
      "Epoch 68/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0809 - acc: 0.8095\n",
      "Epoch 69/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0674 - acc: 0.8095\n",
      "Epoch 70/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0774 - acc: 0.7778\n",
      "Epoch 71/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1839 - acc: 0.7460\n",
      "Epoch 72/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3122 - acc: 0.6667\n",
      "Epoch 73/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1503 - acc: 0.6984\n",
      "Epoch 74/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0715 - acc: 0.8095\n",
      "Epoch 75/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0195 - acc: 0.8095\n",
      "Epoch 76/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9964 - acc: 0.8413\n",
      "Epoch 77/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9760 - acc: 0.8730\n",
      "Epoch 78/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9572 - acc: 0.8730\n",
      "Epoch 79/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9386 - acc: 0.8730\n",
      "Epoch 80/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9195 - acc: 0.8571\n",
      "Epoch 81/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8983 - acc: 0.8730\n",
      "Epoch 82/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8776 - acc: 0.8889\n",
      "Epoch 83/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8602 - acc: 0.8889\n",
      "Epoch 84/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8772 - acc: 0.8095\n",
      "Epoch 85/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8650 - acc: 0.8889\n",
      "Epoch 86/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8644 - acc: 0.8889\n",
      "Epoch 87/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8472 - acc: 0.9048\n",
      "Epoch 88/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8177 - acc: 0.9365\n",
      "Epoch 89/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8956 - acc: 0.8730\n",
      "Epoch 90/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8439 - acc: 0.9365\n",
      "Epoch 91/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7839 - acc: 0.9524\n",
      "Epoch 92/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7804 - acc: 0.9206\n",
      "Epoch 93/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7232 - acc: 0.9524\n",
      "Epoch 94/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7060 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6937 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7305 - acc: 0.9365\n",
      "Epoch 97/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7954 - acc: 0.9206\n",
      "Epoch 98/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6559 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6260 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6043 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff9776e1650>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop', metrics = ['acc'])\n",
    "model.fit(X_train, y_train,  epochs = 100, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_31 (Embedding)     (None, 200, 50)           500000    \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 9)                 279       \n",
      "=================================================================\n",
      "Total params: 522,009\n",
      "Trainable params: 522,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(max_words, output_dim = embedding_dim,input_length = maxlen))\n",
    "#input node가 만개, output이 50개 각 노드의 길이는 200\n",
    "model.add(layers.LSTM(50))\n",
    "model.add(layers.Dense(30, kernel_regularizer = regularizers.l1_l2(l1 = 0.01, l2 = 0.001),activation = 'relu'))\n",
    "model.add(layers.Dense(9, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63 samples\n",
      "Epoch 1/100\n",
      "63/63 [==============================] - 2s 29ms/sample - loss: 4.3122 - acc: 0.1746\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 4.2542 - acc: 0.3175\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 4.2110 - acc: 0.2222\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 4.1732 - acc: 0.2698\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 4.1369 - acc: 0.2540\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 4.1010 - acc: 0.2698\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 4.0611 - acc: 0.2698\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 4.0126 - acc: 0.3492\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.9309 - acc: 0.2381\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.9283 - acc: 0.1587\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.8872 - acc: 0.2857\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.8408 - acc: 0.3175\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.7949 - acc: 0.2540\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.7470 - acc: 0.2381\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.6968 - acc: 0.2540\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.6448 - acc: 0.3333\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.5925 - acc: 0.2857\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.5439 - acc: 0.3333\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.5006 - acc: 0.3333\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.4973 - acc: 0.2222\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.4945 - acc: 0.2381\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.4156 - acc: 0.3651\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.3793 - acc: 0.4603\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.3581 - acc: 0.2540\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.4124 - acc: 0.3333\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.2996 - acc: 0.3016\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.2542 - acc: 0.4762\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.2218 - acc: 0.3492\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.2041 - acc: 0.4286\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.1550 - acc: 0.4762\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.1352 - acc: 0.4762\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.0755 - acc: 0.5238\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.0585 - acc: 0.4762\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.0350 - acc: 0.5079\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.0073 - acc: 0.4762\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.9553 - acc: 0.4762\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.9053 - acc: 0.5397\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.8703 - acc: 0.5556\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.8408 - acc: 0.5079\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.9951 - acc: 0.4286\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 3.0057 - acc: 0.3810\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.8763 - acc: 0.5238\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.7730 - acc: 0.5238\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.7501 - acc: 0.5714\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.7126 - acc: 0.5397\n",
      "Epoch 46/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.6696 - acc: 0.5873\n",
      "Epoch 47/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.6393 - acc: 0.5397\n",
      "Epoch 48/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.6259 - acc: 0.5873\n",
      "Epoch 49/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.6849 - acc: 0.4286\n",
      "Epoch 50/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.6813 - acc: 0.5556\n",
      "Epoch 51/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.6073 - acc: 0.5714\n",
      "Epoch 52/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.5875 - acc: 0.5397\n",
      "Epoch 53/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.6526 - acc: 0.4603\n",
      "Epoch 54/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.5967 - acc: 0.4603\n",
      "Epoch 55/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.4917 - acc: 0.6190\n",
      "Epoch 56/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.4733 - acc: 0.6032\n",
      "Epoch 57/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.4651 - acc: 0.6508\n",
      "Epoch 58/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.4065 - acc: 0.6349\n",
      "Epoch 59/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.3764 - acc: 0.6190\n",
      "Epoch 60/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.3536 - acc: 0.6508\n",
      "Epoch 61/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.3547 - acc: 0.6508\n",
      "Epoch 62/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.3082 - acc: 0.6825\n",
      "Epoch 63/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.2882 - acc: 0.6508\n",
      "Epoch 64/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.2722 - acc: 0.6825\n",
      "Epoch 65/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.2660 - acc: 0.6508\n",
      "Epoch 66/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.2387 - acc: 0.7302\n",
      "Epoch 67/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.2209 - acc: 0.6667\n",
      "Epoch 68/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.2274 - acc: 0.6667\n",
      "Epoch 69/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1746 - acc: 0.7302\n",
      "Epoch 70/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1724 - acc: 0.7460\n",
      "Epoch 71/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.3692 - acc: 0.5079\n",
      "Epoch 72/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.2418 - acc: 0.8254\n",
      "Epoch 73/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.2413 - acc: 0.7143\n",
      "Epoch 74/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.2417 - acc: 0.6349\n",
      "Epoch 75/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1040 - acc: 0.7937\n",
      "Epoch 76/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0803 - acc: 0.7143\n",
      "Epoch 77/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0555 - acc: 0.7619\n",
      "Epoch 78/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0344 - acc: 0.8413\n",
      "Epoch 79/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0376 - acc: 0.7302\n",
      "Epoch 80/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0073 - acc: 0.7302\n",
      "Epoch 81/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9736 - acc: 0.7778\n",
      "Epoch 82/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9529 - acc: 0.7778\n",
      "Epoch 83/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9345 - acc: 0.8571\n",
      "Epoch 84/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9166 - acc: 0.8730\n",
      "Epoch 85/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9011 - acc: 0.8730\n",
      "Epoch 86/100\n",
      "63/63 [==============================] - 0s 1ms/sample - loss: 1.8888 - acc: 0.8730\n",
      "Epoch 87/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8723 - acc: 0.8889\n",
      "Epoch 88/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9269 - acc: 0.9048\n",
      "Epoch 89/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.2887 - acc: 0.6667\n",
      "Epoch 90/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0265 - acc: 0.6825\n",
      "Epoch 91/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1339 - acc: 0.7143\n",
      "Epoch 92/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1163 - acc: 0.7460\n",
      "Epoch 93/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9679 - acc: 0.7937\n",
      "Epoch 94/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8526 - acc: 0.9206\n",
      "Epoch 95/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8196 - acc: 0.9206\n",
      "Epoch 96/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8015 - acc: 0.9365\n",
      "Epoch 97/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7860 - acc: 0.9365\n",
      "Epoch 98/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7723 - acc: 0.9365\n",
      "Epoch 99/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7598 - acc: 0.9206\n",
      "Epoch 100/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7493 - acc: 0.9365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff947303d10>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop', metrics = ['acc'])\n",
    "model.fit(X_train, y_train,  epochs = 100, batch_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 200, 50)           500000    \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 9)                 279       \n",
      "=================================================================\n",
      "Total params: 522,009\n",
      "Trainable params: 522,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(max_words, output_dim = embedding_dim,input_length = maxlen))\n",
    "model.add(layers.LSTM(50, dropout= 0.3))\n",
    "model.add(layers.Dense(30, activation = 'relu'))\n",
    "model.add(layers.Dense(9, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63 samples\n",
      "Epoch 1/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 2.1953 - acc: 0.1429\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1842 - acc: 0.2381\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1743 - acc: 0.2540\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1627 - acc: 0.2857\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1471 - acc: 0.3175\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1303 - acc: 0.3333\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1043 - acc: 0.3175\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0681 - acc: 0.3651\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0038 - acc: 0.3810\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8982 - acc: 0.3968\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8224 - acc: 0.3651\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1026 - acc: 0.2540\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8019 - acc: 0.3651\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7270 - acc: 0.4444\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6399 - acc: 0.4444\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.5834 - acc: 0.4286\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.5777 - acc: 0.4444\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6705 - acc: 0.3016\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.5638 - acc: 0.4603\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4627 - acc: 0.5397\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4093 - acc: 0.5714\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3670 - acc: 0.6032\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3254 - acc: 0.6349\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3267 - acc: 0.5873\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8524 - acc: 0.3333\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.5894 - acc: 0.2222\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.3236 - acc: 0.2540\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9640 - acc: 0.3492\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9182 - acc: 0.3175\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9470 - acc: 0.3333\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8632 - acc: 0.3651\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8841 - acc: 0.3651\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0019 - acc: 0.3016\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8503 - acc: 0.3651\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8238 - acc: 0.3492\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7980 - acc: 0.3492\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7924 - acc: 0.3492\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9370 - acc: 0.3175\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7167 - acc: 0.3968\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6921 - acc: 0.3968\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6685 - acc: 0.4444\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6473 - acc: 0.4127\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6252 - acc: 0.4603\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.5940 - acc: 0.4127\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.5372 - acc: 0.4444\n",
      "Epoch 46/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3951 - acc: 0.4921\n",
      "Epoch 47/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6653 - acc: 0.3810\n",
      "Epoch 48/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6989 - acc: 0.3651\n",
      "Epoch 49/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4216 - acc: 0.4603\n",
      "Epoch 50/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3701 - acc: 0.5397\n",
      "Epoch 51/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3410 - acc: 0.5397\n",
      "Epoch 52/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3193 - acc: 0.5714\n",
      "Epoch 53/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2951 - acc: 0.5556\n",
      "Epoch 54/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2576 - acc: 0.5556\n",
      "Epoch 55/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2622 - acc: 0.6032\n",
      "Epoch 56/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2975 - acc: 0.5079\n",
      "Epoch 57/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2402 - acc: 0.6349\n",
      "Epoch 58/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1542 - acc: 0.5873\n",
      "Epoch 59/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0874 - acc: 0.6508\n",
      "Epoch 60/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0437 - acc: 0.6825\n",
      "Epoch 61/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0007 - acc: 0.7302\n",
      "Epoch 62/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9716 - acc: 0.8095\n",
      "Epoch 63/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9512 - acc: 0.7143\n",
      "Epoch 64/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9338 - acc: 0.8254\n",
      "Epoch 65/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9290 - acc: 0.7619\n",
      "Epoch 66/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9136 - acc: 0.7937\n",
      "Epoch 67/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9168 - acc: 0.8571\n",
      "Epoch 68/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4565 - acc: 0.4762\n",
      "Epoch 69/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2062 - acc: 0.5556\n",
      "Epoch 70/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9362 - acc: 0.7460\n",
      "Epoch 71/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8422 - acc: 0.6825\n",
      "Epoch 72/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8034 - acc: 0.8571\n",
      "Epoch 73/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7743 - acc: 0.9524\n",
      "Epoch 74/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7578 - acc: 0.9365\n",
      "Epoch 75/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7461 - acc: 0.9048\n",
      "Epoch 76/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7390 - acc: 0.9365\n",
      "Epoch 77/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7219 - acc: 0.9365\n",
      "Epoch 78/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6907 - acc: 0.9524\n",
      "Epoch 79/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6747 - acc: 0.9524\n",
      "Epoch 80/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6574 - acc: 0.9048\n",
      "Epoch 81/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6517 - acc: 0.9365\n",
      "Epoch 82/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6266 - acc: 0.9524\n",
      "Epoch 83/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5979 - acc: 0.9524\n",
      "Epoch 84/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5814 - acc: 0.9841\n",
      "Epoch 85/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5642 - acc: 0.9365\n",
      "Epoch 86/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5435 - acc: 0.9683\n",
      "Epoch 87/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5260 - acc: 0.9683\n",
      "Epoch 88/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5166 - acc: 0.9524\n",
      "Epoch 89/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5088 - acc: 0.9683\n",
      "Epoch 90/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5018 - acc: 0.9683\n",
      "Epoch 91/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5086 - acc: 0.9524\n",
      "Epoch 92/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5629 - acc: 0.9048\n",
      "Epoch 93/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.6915 - acc: 0.7937\n",
      "Epoch 94/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2771 - acc: 0.4762\n",
      "Epoch 95/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5580 - acc: 0.9524\n",
      "Epoch 96/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.5444 - acc: 0.9048\n",
      "Epoch 97/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4820 - acc: 0.9365\n",
      "Epoch 98/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4362 - acc: 0.9683\n",
      "Epoch 99/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4081 - acc: 0.9683\n",
      "Epoch 100/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.4035 - acc: 0.9841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff97e72ac10>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop', metrics = ['acc'])\n",
    "model.fit(X_train, y_train,  epochs = 100, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_21 (Embedding)     (None, 200, 50)           500000    \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 9)                 279       \n",
      "=================================================================\n",
      "Total params: 522,009\n",
      "Trainable params: 522,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(max_words, output_dim = embedding_dim,input_length = maxlen))\n",
    "model.add(layers.LSTM(50))\n",
    "model.add(layers.Dense(30, activation = 'relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(9, activation = 'softmax'))\n",
    "model.summary()\n",
    "#dropout layer를 추가하고 성능이 좋지못함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63 samples\n",
      "Epoch 1/100\n",
      "63/63 [==============================] - 2s 36ms/sample - loss: 2.2003 - acc: 0.0159\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1923 - acc: 0.1587\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1916 - acc: 0.2063\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1859 - acc: 0.2063\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1846 - acc: 0.2222\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1752 - acc: 0.2540\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1647 - acc: 0.3016\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1549 - acc: 0.3016\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.1446 - acc: 0.2857\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0913 - acc: 0.1746\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0568 - acc: 0.2063\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 2.0790 - acc: 0.4603\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9873 - acc: 0.3651\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9139 - acc: 0.4603\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9415 - acc: 0.3492\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9292 - acc: 0.3810\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8698 - acc: 0.2857\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9296 - acc: 0.2698\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.9449 - acc: 0.2063\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8940 - acc: 0.3175\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8499 - acc: 0.3968\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7848 - acc: 0.4286\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.8303 - acc: 0.2222\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7819 - acc: 0.4444\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6693 - acc: 0.3810\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7203 - acc: 0.3968\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6668 - acc: 0.3016\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6487 - acc: 0.3810\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6637 - acc: 0.3016\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7123 - acc: 0.2381\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6215 - acc: 0.4127\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.5021 - acc: 0.5079\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.5746 - acc: 0.4127\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.7187 - acc: 0.3810\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6551 - acc: 0.3016\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.5317 - acc: 0.4921\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.5723 - acc: 0.3492\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4621 - acc: 0.4127\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4142 - acc: 0.4921\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4168 - acc: 0.4444\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4287 - acc: 0.4603\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4187 - acc: 0.3651\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3687 - acc: 0.5079\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2719 - acc: 0.4603\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2996 - acc: 0.5238\n",
      "Epoch 46/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4150 - acc: 0.4127\n",
      "Epoch 47/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.6549 - acc: 0.4286\n",
      "Epoch 48/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4343 - acc: 0.4444\n",
      "Epoch 49/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4303 - acc: 0.4444\n",
      "Epoch 50/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.4102 - acc: 0.3175\n",
      "Epoch 51/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2609 - acc: 0.5556\n",
      "Epoch 52/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2155 - acc: 0.5556\n",
      "Epoch 53/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2276 - acc: 0.5714\n",
      "Epoch 54/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2697 - acc: 0.5079\n",
      "Epoch 55/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3905 - acc: 0.4603\n",
      "Epoch 56/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2211 - acc: 0.4921\n",
      "Epoch 57/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2666 - acc: 0.4762\n",
      "Epoch 58/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.3030 - acc: 0.4921\n",
      "Epoch 59/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1658 - acc: 0.5714\n",
      "Epoch 60/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1510 - acc: 0.5556\n",
      "Epoch 61/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0878 - acc: 0.5873\n",
      "Epoch 62/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1198 - acc: 0.5079\n",
      "Epoch 63/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.2052 - acc: 0.5079\n",
      "Epoch 64/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1669 - acc: 0.5397\n",
      "Epoch 65/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1021 - acc: 0.5714\n",
      "Epoch 66/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1901 - acc: 0.4921\n",
      "Epoch 67/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0509 - acc: 0.6825\n",
      "Epoch 68/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0924 - acc: 0.5873\n",
      "Epoch 69/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0722 - acc: 0.6190\n",
      "Epoch 70/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9825 - acc: 0.6825\n",
      "Epoch 71/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0471 - acc: 0.6190\n",
      "Epoch 72/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1205 - acc: 0.5556\n",
      "Epoch 73/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0664 - acc: 0.5397\n",
      "Epoch 74/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0748 - acc: 0.5873\n",
      "Epoch 75/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.1181 - acc: 0.5238\n",
      "Epoch 76/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0473 - acc: 0.6032\n",
      "Epoch 77/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9966 - acc: 0.6508\n",
      "Epoch 78/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0083 - acc: 0.5873\n",
      "Epoch 79/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9133 - acc: 0.5873\n",
      "Epoch 80/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9691 - acc: 0.5714\n",
      "Epoch 81/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8150 - acc: 0.7460\n",
      "Epoch 82/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9840 - acc: 0.6032\n",
      "Epoch 83/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9492 - acc: 0.5714\n",
      "Epoch 84/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9524 - acc: 0.6667\n",
      "Epoch 85/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9463 - acc: 0.6984\n",
      "Epoch 86/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9667 - acc: 0.6190\n",
      "Epoch 87/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9003 - acc: 0.6667\n",
      "Epoch 88/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9841 - acc: 0.6667\n",
      "Epoch 89/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8751 - acc: 0.6032\n",
      "Epoch 90/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9598 - acc: 0.6825\n",
      "Epoch 91/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.9995 - acc: 0.6349\n",
      "Epoch 92/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 1.0715 - acc: 0.6190\n",
      "Epoch 93/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8000 - acc: 0.6984\n",
      "Epoch 94/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7470 - acc: 0.7619\n",
      "Epoch 95/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7398 - acc: 0.8095\n",
      "Epoch 96/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7004 - acc: 0.8254\n",
      "Epoch 97/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.7195 - acc: 0.8254\n",
      "Epoch 98/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8845 - acc: 0.6667\n",
      "Epoch 99/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8164 - acc: 0.7460\n",
      "Epoch 100/100\n",
      "63/63 [==============================] - 0s 2ms/sample - loss: 0.8057 - acc: 0.7619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff96142a410>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop', metrics = ['acc'])\n",
    "model.fit(X_train, y_train,  epochs = 100, batch_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순환신경망 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (None, 200, 50)           500000    \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 200, 50)           20200     \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 36)                12528     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 30)                1110      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 9)                 279       \n",
      "=================================================================\n",
      "Total params: 534,117\n",
      "Trainable params: 534,117\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(max_words, output_dim = embedding_dim,input_length = maxlen))\n",
    "model.add(layers.LSTM(50, return_sequences = True))\n",
    "model.add(layers.LSTM(36,dropout = 0.3))\n",
    "model.add(layers.Dense(30, activation = 'relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(9, activation = 'softmax'))\n",
    "model.summary()\n",
    "#dropout layer를 추가하고 성능이 좋지못함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63 samples\n",
      "Epoch 1/100\n",
      "63/63 [==============================] - 4s 59ms/sample - loss: 2.1961 - acc: 0.0794\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 2.1948 - acc: 0.1270\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 2.1807 - acc: 0.1746\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 2.1628 - acc: 0.2222\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 2.1316 - acc: 0.2222\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 2.0603 - acc: 0.2381\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 2.0683 - acc: 0.2063\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 2.0354 - acc: 0.2381\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.9370 - acc: 0.1905\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.8764 - acc: 0.2698\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.9002 - acc: 0.2381\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.8040 - acc: 0.2857\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.7791 - acc: 0.2540\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.7889 - acc: 0.2540\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.7940 - acc: 0.3492\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.6803 - acc: 0.3492\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.7676 - acc: 0.3492\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.6244 - acc: 0.3651\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.5480 - acc: 0.4286\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.4945 - acc: 0.4127\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.4292 - acc: 0.4444\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.4402 - acc: 0.4603\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.4292 - acc: 0.4286\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.3836 - acc: 0.5079\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 1.4533 - acc: 0.4921\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.2564 - acc: 0.5238\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 1.2094 - acc: 0.5238\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 1.1984 - acc: 0.5238\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.2324 - acc: 0.6032\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.1567 - acc: 0.6825\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.1713 - acc: 0.6349\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.1481 - acc: 0.6825\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.1900 - acc: 0.6032\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.0150 - acc: 0.7302\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.9689 - acc: 0.6984\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.8822 - acc: 0.7460\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.0922 - acc: 0.6508\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.0360 - acc: 0.6667\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 0.8415 - acc: 0.7460\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.8114 - acc: 0.7619\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.8448 - acc: 0.7302\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.8713 - acc: 0.7778\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.8113 - acc: 0.7302\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.8436 - acc: 0.7619\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.7321 - acc: 0.8413\n",
      "Epoch 46/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.7616 - acc: 0.8095\n",
      "Epoch 47/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 0.6118 - acc: 0.9048\n",
      "Epoch 48/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.6422 - acc: 0.8095\n",
      "Epoch 49/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 0.6448 - acc: 0.8413\n",
      "Epoch 50/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.5483 - acc: 0.8730\n",
      "Epoch 51/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 0.5711 - acc: 0.8571\n",
      "Epoch 52/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.7719 - acc: 0.7302\n",
      "Epoch 53/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.7689 - acc: 0.7619\n",
      "Epoch 54/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.5998 - acc: 0.8413\n",
      "Epoch 55/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 0.4268 - acc: 0.9206\n",
      "Epoch 56/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 0.5165 - acc: 0.8254\n",
      "Epoch 57/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.5846 - acc: 0.8413\n",
      "Epoch 58/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.4790 - acc: 0.9048\n",
      "Epoch 59/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.5526 - acc: 0.8571\n",
      "Epoch 60/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.4801 - acc: 0.9048\n",
      "Epoch 61/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.4043 - acc: 0.8889\n",
      "Epoch 62/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.5170 - acc: 0.8413\n",
      "Epoch 63/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.4443 - acc: 0.8730\n",
      "Epoch 64/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.3135 - acc: 0.9841\n",
      "Epoch 65/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.4702 - acc: 0.8413\n",
      "Epoch 66/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.4122 - acc: 0.8730\n",
      "Epoch 67/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.3550 - acc: 0.9206\n",
      "Epoch 68/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.7860 - acc: 0.8254\n",
      "Epoch 69/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.5194 - acc: 0.8254\n",
      "Epoch 70/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.3883 - acc: 0.8730\n",
      "Epoch 71/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.3070 - acc: 0.9365\n",
      "Epoch 72/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.2921 - acc: 0.9365\n",
      "Epoch 73/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.4126 - acc: 0.8571\n",
      "Epoch 74/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.3887 - acc: 0.9365\n",
      "Epoch 75/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.2974 - acc: 0.9206\n",
      "Epoch 76/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.3580 - acc: 0.9206\n",
      "Epoch 77/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.3834 - acc: 0.9048\n",
      "Epoch 78/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.2526 - acc: 0.9683\n",
      "Epoch 79/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.2444 - acc: 0.9524\n",
      "Epoch 80/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.3824 - acc: 0.9048\n",
      "Epoch 81/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 1.0045 - acc: 0.6667\n",
      "Epoch 82/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 0.4416 - acc: 0.9206\n",
      "Epoch 83/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 0.2937 - acc: 0.9206\n",
      "Epoch 84/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 0.3174 - acc: 0.9206\n",
      "Epoch 85/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 0.3609 - acc: 0.8730\n",
      "Epoch 86/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 0.2105 - acc: 0.9683\n",
      "Epoch 87/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.2286 - acc: 0.9365\n",
      "Epoch 88/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.2729 - acc: 0.9365\n",
      "Epoch 89/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 0.2745 - acc: 0.9524\n",
      "Epoch 90/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 0.3412 - acc: 0.9206\n",
      "Epoch 91/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 0.2232 - acc: 0.9524\n",
      "Epoch 92/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 0.2485 - acc: 0.9524\n",
      "Epoch 93/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.1793 - acc: 0.9683\n",
      "Epoch 94/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 0.3019 - acc: 0.8889\n",
      "Epoch 95/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 0.1775 - acc: 0.9365\n",
      "Epoch 96/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 0.1840 - acc: 0.9683\n",
      "Epoch 97/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 0.1937 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "63/63 [==============================] - 0s 6ms/sample - loss: 0.2140 - acc: 0.9683\n",
      "Epoch 99/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.1841 - acc: 0.9683\n",
      "Epoch 100/100\n",
      "63/63 [==============================] - 0s 5ms/sample - loss: 0.1310 - acc: 0.9841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff955566f10>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop', metrics = ['acc'])\n",
    "model.fit(X_train, y_train,  epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 양방향 순환신경망 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_29 (Embedding)     (None, 200, 50)           500000    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 100)          40400     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 72)                39456     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 30)                2190      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 9)                 279       \n",
      "=================================================================\n",
      "Total params: 582,325\n",
      "Trainable params: 582,325\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(max_words, output_dim = embedding_dim,input_length = maxlen))\n",
    "model.add(layers.Bidirectional(layers.LSTM(50, return_sequences = True)))\n",
    "model.add(layers.Bidirectional(layers.LSTM(36, dropout = 0.3)))\n",
    "model.add(layers.Dense(30, activation = 'relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(9, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63 samples\n",
      "Epoch 1/100\n",
      "63/63 [==============================] - 7s 111ms/sample - loss: 2.1975 - acc: 0.0635\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 2.1933 - acc: 0.1270\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 2.1755 - acc: 0.1905\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 2.1706 - acc: 0.1587\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 2.1473 - acc: 0.1905\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 2.1269 - acc: 0.1111\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 2.0467 - acc: 0.2063\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 1.8859 - acc: 0.3333\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 1.8820 - acc: 0.2540\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 1.7923 - acc: 0.3492\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 1.7942 - acc: 0.3175\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 1.7065 - acc: 0.2857\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 1.7560 - acc: 0.3016\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 1.6214 - acc: 0.4127\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 1.5446 - acc: 0.4762\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 1s 9ms/sample - loss: 1.5231 - acc: 0.4286\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 1.4645 - acc: 0.4762\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 1.3938 - acc: 0.4921\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 1.4201 - acc: 0.4286\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 1.2308 - acc: 0.5397\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 1.3223 - acc: 0.4762\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 1.3462 - acc: 0.4286\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 1.2932 - acc: 0.4127\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 1.4020 - acc: 0.4286\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 1.1855 - acc: 0.5238\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 1.5037 - acc: 0.4762\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 1.1570 - acc: 0.5556\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 1.2423 - acc: 0.4603\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 0.9544 - acc: 0.6190\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 1.0983 - acc: 0.6190\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 1.3348 - acc: 0.4762\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 0.9872 - acc: 0.6825\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 1.0102 - acc: 0.5556\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 0.9621 - acc: 0.5714\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 0.9215 - acc: 0.6825\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 0.8552 - acc: 0.7302\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 0.7595 - acc: 0.7460\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 0.7748 - acc: 0.7778\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 0.8032 - acc: 0.6984\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 0.8062 - acc: 0.7937\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 1.0066 - acc: 0.6984\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 0.9805 - acc: 0.6190\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 0.6847 - acc: 0.7937\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 0.7143 - acc: 0.7143\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 0.8012 - acc: 0.8254\n",
      "Epoch 46/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 0.7811 - acc: 0.7302\n",
      "Epoch 47/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 0.7015 - acc: 0.8254\n",
      "Epoch 48/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 0.6186 - acc: 0.8571\n",
      "Epoch 49/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 0.5441 - acc: 0.8413\n",
      "Epoch 50/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 0.6344 - acc: 0.7619\n",
      "Epoch 51/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 0.5675 - acc: 0.8571\n",
      "Epoch 52/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 0.5484 - acc: 0.8571\n",
      "Epoch 53/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 0.6712 - acc: 0.7778\n",
      "Epoch 54/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 0.4558 - acc: 0.8730\n",
      "Epoch 55/100\n",
      "63/63 [==============================] - 1s 9ms/sample - loss: 0.4654 - acc: 0.8730\n",
      "Epoch 56/100\n",
      "63/63 [==============================] - 1s 9ms/sample - loss: 0.5157 - acc: 0.8095\n",
      "Epoch 57/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 0.7657 - acc: 0.8095\n",
      "Epoch 58/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 0.4995 - acc: 0.8571\n",
      "Epoch 59/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 0.4568 - acc: 0.8889\n",
      "Epoch 60/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 0.4839 - acc: 0.8413\n",
      "Epoch 61/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 0.4651 - acc: 0.8571\n",
      "Epoch 62/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 0.4494 - acc: 0.8413\n",
      "Epoch 63/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 0.3206 - acc: 0.9365\n",
      "Epoch 64/100\n",
      "63/63 [==============================] - 1s 9ms/sample - loss: 0.3980 - acc: 0.9048\n",
      "Epoch 65/100\n",
      "63/63 [==============================] - 1s 9ms/sample - loss: 0.3908 - acc: 0.8571\n",
      "Epoch 66/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 0.2964 - acc: 0.9683\n",
      "Epoch 67/100\n",
      "63/63 [==============================] - 1s 9ms/sample - loss: 0.3427 - acc: 0.9365\n",
      "Epoch 68/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 0.7573 - acc: 0.7460\n",
      "Epoch 69/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 0.5301 - acc: 0.8413\n",
      "Epoch 70/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 0.4018 - acc: 0.9524\n",
      "Epoch 71/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 0.3681 - acc: 0.8889\n",
      "Epoch 72/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 0.3293 - acc: 0.9206\n",
      "Epoch 73/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 0.3761 - acc: 0.8889\n",
      "Epoch 74/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 0.3948 - acc: 0.8730\n",
      "Epoch 75/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 0.3384 - acc: 0.8889\n",
      "Epoch 76/100\n",
      "63/63 [==============================] - 0s 8ms/sample - loss: 0.2937 - acc: 0.9206\n",
      "Epoch 77/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 0.3078 - acc: 0.9206\n",
      "Epoch 78/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 0.3329 - acc: 0.8730\n",
      "Epoch 79/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 0.4519 - acc: 0.8730\n",
      "Epoch 80/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 0.4150 - acc: 0.8571\n",
      "Epoch 81/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 0.5119 - acc: 0.8413\n",
      "Epoch 82/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 0.3953 - acc: 0.9206\n",
      "Epoch 83/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 0.3406 - acc: 0.9524\n",
      "Epoch 84/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 0.2577 - acc: 0.9365\n",
      "Epoch 85/100\n",
      "63/63 [==============================] - 1s 8ms/sample - loss: 0.2548 - acc: 0.9524\n",
      "Epoch 86/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 0.3577 - acc: 0.9365\n",
      "Epoch 87/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 0.3603 - acc: 0.8413\n",
      "Epoch 88/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 0.4223 - acc: 0.8571\n",
      "Epoch 89/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 0.2632 - acc: 0.9206\n",
      "Epoch 90/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 0.4646 - acc: 0.8413\n",
      "Epoch 91/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 0.4777 - acc: 0.8889\n",
      "Epoch 92/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 0.3934 - acc: 0.9206\n",
      "Epoch 93/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 0.4049 - acc: 0.9048\n",
      "Epoch 94/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 0.3389 - acc: 0.9206\n",
      "Epoch 95/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 0.3292 - acc: 0.9048\n",
      "Epoch 96/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 0.3162 - acc: 0.9365\n",
      "Epoch 97/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 0.2110 - acc: 0.9365\n",
      "Epoch 98/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 0.2493 - acc: 0.9365\n",
      "Epoch 99/100\n",
      "63/63 [==============================] - 0s 7ms/sample - loss: 0.3275 - acc: 0.9048\n",
      "Epoch 100/100\n",
      "63/63 [==============================] - 1s 9ms/sample - loss: 0.2865 - acc: 0.9683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff95db36b90>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop', metrics = ['acc'])\n",
    "model.fit(X_train, y_train,  epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
