{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dot Product & Multi Head Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNF6UWLOgYB/eyWAO35bZrH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY-aFuJLNqRJ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import copy\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz1hHgBeNq_E"
      },
      "source": [
        "%%bash\n",
        "apt-get update\n",
        "apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
        "pip3 install JPype1\n",
        "pip3 install konlpy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_h2WlX7NwYq",
        "outputId": "f49c05da-26c3-4cff-f32f-7102be595588"
      },
      "source": [
        "%env JAVA_HOME \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbIzW2bjNypA",
        "outputId": "e7709593-5c12-4799-b994-a4f855d23320"
      },
      "source": [
        "%%bash\n",
        "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
        "pip3 install /tmp/mecab-python-0.996"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mecab-ko is already installed\n",
            "mecab-ko-dic is already installed\n",
            "mecab-python is already installed\n",
            "Done.\n",
            "Processing /tmp/mecab-python-0.996\n",
            "Building wheels for collected packages: mecab-python\n",
            "  Building wheel for mecab-python (setup.py): started\n",
            "  Building wheel for mecab-python (setup.py): finished with status 'done'\n",
            "  Created wheel for mecab-python: filename=mecab_python-0.996_ko_0.9.2-cp37-cp37m-linux_x86_64.whl size=141813 sha256=42cb3f77503a704636fd85caecacf154fea5d66a99a05f46f8cb0ca4da386351\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/7b/9f/2922869bef86c3354ae7034f7a3647c573ee1997c2dad0290a\n",
            "Failed to build mecab-python\n",
            "Installing collected packages: mecab-python\n",
            "  Attempting uninstall: mecab-python\n",
            "    Found existing installation: mecab-python 0.996-ko-0.9.2\n",
            "    Uninstalling mecab-python-0.996-ko-0.9.2:\n",
            "      Successfully uninstalled mecab-python-0.996-ko-0.9.2\n",
            "    Running setup.py install for mecab-python: started\n",
            "    Running setup.py install for mecab-python: finished with status 'done'\n",
            "Successfully installed mecab-python-0.996-ko-0.9.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\n",
            "  WARNING: Built wheel for mecab-python is invalid: Metadata 1.2 mandates PEP 440 version, but '0.996-ko-0.9.2' is not\n",
            "  DEPRECATION: mecab-python was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_GE0BsnNz7F",
        "outputId": "535b3266-8a63-41c2-9ae9-e13b8f9d601e"
      },
      "source": [
        "! pip install konlpy\n",
        "from konlpy.tag import Mecab"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 31.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "L2ns0kr3OLH5",
        "outputId": "02146c25-9d7e-43e1-908b-e0514bf3ce4f"
      },
      "source": [
        "np.set_printoptions(suppress = True)\n",
        "temp_k = torch.tensor([[10,0,0],\n",
        "                       [0, 10, 0],\n",
        "                       [0, 0, 10],\n",
        "                       [0, 0, 10]], dtype = torch.float32)\n",
        "# 예시이지만 key는 임베딩 벡터 값\n",
        "\n",
        "temp_v = torch.tensor([[10,0, 0],\n",
        "                       [0,10, 0],\n",
        "                       [0, 0, 10],\n",
        "                       [0,0, 10]], dtype = torch.float32)\n",
        "# 예시이지만 value는 임베딩 벡터 값 \n",
        "\n",
        "temp_q = torch.tensor([[0,10,0]], dtype = torch.float32)\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n- 여기서는 안써있지만 각 단어에 대해 임베딩 벡터값을 그대로 가져오는게 아님 \\n- 각 단어를 표시하는 차원을 head개수로 나눠 변환하여 Query, Key, Value를 만들어냄 \\n'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFegLFitQrs-",
        "outputId": "4a1adb6f-2103-4d9c-e64f-1b270e7ed259"
      },
      "source": [
        "trans_k = torch.transpose(temp_k, -2, -1)\n",
        "# query와 연관성이 있는 단어를 찾기위해 각 단어에 대해서 matmul을 수행하여 연관성 값 도출 \n",
        "trans_k"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10.,  0.,  0.,  0.],\n",
              "        [ 0., 10.,  0.,  0.],\n",
              "        [ 0.,  0., 10., 10.]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-QYZjw7SqkC",
        "outputId": "98be4255-2c21-4bc5-afb3-a71c0c7cea98"
      },
      "source": [
        "temp_k.size()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpXYEKkfVFgf",
        "outputId": "40e71dd3-be6c-4fc6-911e-b2d4b9a2416b"
      },
      "source": [
        "at_s = torch.matmul(temp_q, trans_k)\n",
        "print(at_s)\n",
        "scaled = at_s / math.sqrt(temp_k.size()[-1])\n",
        "print(scaled)\n",
        "at_w = torch.nn.functional.softmax(scaled, dim = -1)\n",
        "print(at_w)\n",
        "out = torch.matmul(at_w, temp_v)\n",
        "print(out)\n",
        "# "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  0., 100.,   0.,   0.]])\n",
            "tensor([[ 0.0000, 57.7350,  0.0000,  0.0000]])\n",
            "tensor([[8.4333e-26, 1.0000e+00, 8.4333e-26, 8.4333e-26]])\n",
            "tensor([[8.4333e-25, 1.0000e+01, 1.6867e-24]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygoPifsTN1Ee"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  trans_k = torch.transpose(k, -2, -1)\n",
        "  # query와 연관성이 있는 단어를 찾기위해 각 단어에 대해서 matmul을 수행하여 연관성 값 도출 \n",
        "\n",
        "  matmul_qk = torch.matmul(q, trans_k)\n",
        "\n",
        "  dk = k.size()[-1]\n",
        "  # 각 단어의 벡터를 알기위해서 사용 \n",
        "\n",
        "  scaled_attention_logits = matmul_qk / math.sqrt(dk)\n",
        "  # scaling, 너무 큰값을 가지면 모델에 좋지않기 때문에 사용하는듯? \n",
        "  # 이게 attention score임 \n",
        "\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)\n",
        "    # mask가 있다면 mask tensor를 추가 \n",
        "\n",
        "  attention_weights = torch.nn.functional.softmax(scaled_attention_logits , dim = -1)\n",
        "  # attention score의 분포를 softmax함수를 통해서 확률로써 도출 \n",
        "  # 이렇게되면 해당 query에 대해 key들의 영향을 확률로써 확인가능 \n",
        "\n",
        "  output = torch.matmul(attention_weights, v)\n",
        "  # 도출된 확률을 바탕으로 실제 value vector와 matmul\n",
        "  # 근데 이렇게 곱하면 각 단어에 대한 value와 attention score를 곱하는게 아니지 않나?...\n",
        "\n",
        "  return output, attention_weights\n",
        "  "
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ui08ARUjRaY"
      },
      "source": [
        "x = torch.rand(1, 60, 512)\n",
        "# x = [batch_size, max_len(encoder), embedding_size]\n",
        "m_h = MultiHeadAttention(d_model = 512, num_heads = 8)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKVlimB8oRvP",
        "outputId": "c1775120-33c1-4a33-be17-19776e39dcf9"
      },
      "source": [
        "s_c = x.transpose(1,2).contiguous().view(1, -1, num_heads * depth)\n",
        "s_c.shape"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 60, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpUqfZTJmcde",
        "outputId": "f72595f3-ef8c-4e1a-c569-240b0cbe890b"
      },
      "source": [
        "d_model, num_heads = 512, 8\n",
        "depth = d_model // num_heads\n",
        "\n",
        "# query 계산 과정 \n",
        "wq = nn.Linear(d_model, d_model)\n",
        "q = wq(x)\n",
        "print(q.shape)\n",
        "q = q.view(1, -1, 8, depth).transpose(1,2)\n",
        "print(q.shape)\n",
        "# 각 헤드에 들어가는 query vertor, value와 key도 똑같이 적용 \n",
        "\n",
        "\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 60, 512])\n",
            "torch.Size([1, 8, 60, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNKhE6bwjRL1"
      },
      "source": [
        "out, at_w = m_h(v = x, k = x, q = x, mask = None)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BC5kuT5lrSr",
        "outputId": "3a25ea89-3dda-4571-9679-3884fb5344f6"
      },
      "source": [
        "print(out.shape, at_w.shape)\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 60, 512]) torch.Size([1, 8, 60, 60])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfsUQCRqVqhJ"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0 \n",
        "    # 딱 나누어 떨어지도록 d_model과 num_heads를 잘설계해야함 \n",
        "    # assert : 뒤의 조건이 TRUE가 아니면 error 발생 시킴 \n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = nn.Linear(d_model, d_model)\n",
        "    self.wk = nn.Linear(d_model, d_model)\n",
        "    self.wv = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.wo = nn.Linear(d_model, d_model)\n",
        "\n",
        "  def forward(self, v, k ,q, mask):\n",
        "    batch_size = q.size()[0]\n",
        "    # 배치사이즈에 맞에 query가 들어갈 것임으로 \n",
        "\n",
        "    q = self.wq(q).view(batch_size, -1, self.num_heads, self.depth).transpose(1,2)\n",
        "    # q : [batch_size, num_head, max_len, depth]\n",
        "    # 단어 임베딩을 사용하는게 아니라 기존의 단어 임베딩 벡터로부터 헤더 개수를 나눈 것을 벡터의 차원으로 사용하여 단어를표현 \n",
        "    \n",
        "    k = self.wk(k).view(batch_size, -1, self.num_heads, self.depth).transpose(1,2)\n",
        "    v = self.wv(v).view(batch_size, -1, self.num_heads, self.depth).transpose(1,2)\n",
        "    # q와 똑같으므로 생략 \n",
        "\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(q,k,v,mask)\n",
        "\n",
        "    scaled_attention = scaled_attention.transpose(1,2).contiguous().view(batch_size, -1, self.num_heads * self.depth)\n",
        "    # 여기서 차원 헷갈림 ...\n",
        "    # 여기서 최종적으로 sacled_attention [batch_size, max_len, embedding_size]로 차원을 맞춰줌 \n",
        "\n",
        "    output = self.wo(scaled_attention)\n",
        "    # 한번 더 선형회귀를 거쳐서 최종 output도출 \n",
        "\n",
        "    return output, attention_weights\n",
        "    # output : [batch_size, max_len, embedding_size]\n",
        "    # attention_weights : [batch_size, num_heads, max_len, embedding_size]\n",
        "    # attnetion_weights는 헤더의 개수마다 softmax값임으로 차원이 하나 늘어난것 같음 \n",
        "\n",
        "'''\n",
        "- 여기서는 안써있지만 각 단어에 대해 임베딩 벡터값을 그대로 가져오는게 아님 \n",
        "- 각 단어를 표시하는 차원을 head개수로 나눠 변환하여 Query, Key, Value vector를 만들어냄 \n",
        "''' "
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FqTkQEvhtn_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
